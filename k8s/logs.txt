
==> Audit <==
|-----------|--------------------------------|----------|-----------------------|---------|---------------------|---------------------|
|  Command  |              Args              | Profile  |         User          | Version |     Start Time      |      End Time       |
|-----------|--------------------------------|----------|-----------------------|---------|---------------------|---------------------|
| start     |                                | minikube | DESKTOP-PGQU8E8\admin | v1.33.1 | 02 Aug 24 15:12 +07 |                     |
| start     |                                | minikube | DESKTOP-PGQU8E8\admin | v1.33.1 | 02 Aug 24 15:16 +07 | 02 Aug 24 15:19 +07 |
| dashboard |                                | minikube | DESKTOP-PGQU8E8\admin | v1.33.1 | 02 Aug 24 15:29 +07 |                     |
| start     |                                | minikube | DESKTOP-PGQU8E8\admin | v1.33.1 | 03 Aug 24 09:28 +07 | 03 Aug 24 09:30 +07 |
| start     |                                | minikube | DESKTOP-PGQU8E8\admin | v1.33.1 | 04 Aug 24 09:54 +07 | 04 Aug 24 09:55 +07 |
| start     |                                | minikube | DESKTOP-PGQU8E8\admin | v1.33.1 | 09 Aug 24 21:03 +07 |                     |
| service   | zipkin --url                   | minikube | DESKTOP-PGQU8E8\admin | v1.33.1 | 12 Aug 24 22:53 +07 |                     |
| service   | zipkin -n job-app --url        | minikube | DESKTOP-PGQU8E8\admin | v1.33.1 | 12 Aug 24 22:54 +07 | 12 Aug 24 22:56 +07 |
| service   | job-service --url              | minikube | DESKTOP-PGQU8E8\admin | v1.33.1 | 13 Aug 24 10:53 +07 |                     |
| service   | job-service --url -n job-app   | minikube | DESKTOP-PGQU8E8\admin | v1.33.1 | 13 Aug 24 10:53 +07 | 13 Aug 24 10:58 +07 |
| service   | job-service --url -n job-app   | minikube | DESKTOP-PGQU8E8\admin | v1.33.1 | 13 Aug 24 10:58 +07 | 13 Aug 24 11:19 +07 |
| service   | review-service --url -n        | minikube | DESKTOP-PGQU8E8\admin | v1.33.1 | 13 Aug 24 11:17 +07 | 13 Aug 24 15:49 +07 |
|           | job-app                        |          |                       |         |                     |                     |
| service   | company-service --url -n       | minikube | DESKTOP-PGQU8E8\admin | v1.33.1 | 13 Aug 24 11:17 +07 | 13 Aug 24 16:33 +07 |
|           | job-app                        |          |                       |         |                     |                     |
| service   | job-service --url -n job-app   | minikube | DESKTOP-PGQU8E8\admin | v1.33.1 | 13 Aug 24 11:26 +07 | 13 Aug 24 11:28 +07 |
| service   | review-service -n job-app      | minikube | DESKTOP-PGQU8E8\admin | v1.33.1 | 13 Aug 24 15:47 +07 |                     |
| service   | review-service --url -n        | minikube | DESKTOP-PGQU8E8\admin | v1.33.1 | 13 Aug 24 15:49 +07 |                     |
|           | job-app                        |          |                       |         |                     |                     |
| service   | review-service --url -n        | minikube | DESKTOP-PGQU8E8\admin | v1.33.1 | 13 Aug 24 15:49 +07 |                     |
|           | job-app                        |          |                       |         |                     |                     |
| service   | company-service --url -n       | minikube | DESKTOP-PGQU8E8\admin | v1.33.1 | 13 Aug 24 16:00 +07 | 13 Aug 24 16:32 +07 |
|           | job-app                        |          |                       |         |                     |                     |
| service   | company-service --url -n       | minikube | DESKTOP-PGQU8E8\admin | v1.33.1 | 13 Aug 24 16:08 +07 | 13 Aug 24 16:08 +07 |
|           | job-app                        |          |                       |         |                     |                     |
| service   | company-service --url -n       | minikube | DESKTOP-PGQU8E8\admin | v1.33.1 | 13 Aug 24 16:31 +07 | 13 Aug 24 16:31 +07 |
|           | job-app                        |          |                       |         |                     |                     |
| service   | job-service --url -n job-app   | minikube | DESKTOP-PGQU8E8\admin | v1.33.1 | 13 Aug 24 16:33 +07 |                     |
| service   | review-service --url -n        | minikube | DESKTOP-PGQU8E8\admin | v1.33.1 | 13 Aug 24 16:33 +07 |                     |
|           | job-app                        |          |                       |         |                     |                     |
| service   | company-service --url -n       | minikube | DESKTOP-PGQU8E8\admin | v1.33.1 | 13 Aug 24 16:35 +07 | 13 Aug 24 16:43 +07 |
|           | job-app                        |          |                       |         |                     |                     |
| service   | company-service --url -n       | minikube | DESKTOP-PGQU8E8\admin | v1.33.1 | 13 Aug 24 16:42 +07 | 13 Aug 24 16:44 +07 |
|           | job-app                        |          |                       |         |                     |                     |
| service   | review-service --url -n        | minikube | DESKTOP-PGQU8E8\admin | v1.33.1 | 13 Aug 24 16:43 +07 |                     |
|           | job-app                        |          |                       |         |                     |                     |
| service   | review-service --url -n        | minikube | DESKTOP-PGQU8E8\admin | v1.33.1 | 13 Aug 24 16:43 +07 |                     |
|           | job-app                        |          |                       |         |                     |                     |
| service   | review-service -n job-app      | minikube | DESKTOP-PGQU8E8\admin | v1.33.1 | 13 Aug 24 16:59 +07 |                     |
| service   | review-service -n job-app      | minikube | DESKTOP-PGQU8E8\admin | v1.33.1 | 13 Aug 24 16:59 +07 |                     |
| service   | review-service -n job-app      | minikube | DESKTOP-PGQU8E8\admin | v1.33.1 | 13 Aug 24 17:00 +07 |                     |
| service   | company-service -n job-app     | minikube | DESKTOP-PGQU8E8\admin | v1.33.1 | 13 Aug 24 17:01 +07 | 13 Aug 24 17:01 +07 |
| service   | zipkin --url                   | minikube | DESKTOP-PGQU8E8\admin | v1.33.1 | 13 Aug 24 20:52 +07 |                     |
| service   | zipkin --url -n job-app        | minikube | DESKTOP-PGQU8E8\admin | v1.33.1 | 13 Aug 24 20:53 +07 | 13 Aug 24 20:57 +07 |
| service   | zipkin --url                   | minikube | DESKTOP-PGQU8E8\admin | v1.33.1 | 13 Aug 24 21:10 +07 |                     |
| service   | zipkin --url -n job-app        | minikube | DESKTOP-PGQU8E8\admin | v1.33.1 | 13 Aug 24 21:10 +07 | 13 Aug 24 21:28 +07 |
| service   | company-service --url -n       | minikube | DESKTOP-PGQU8E8\admin | v1.33.1 | 13 Aug 24 21:14 +07 | 13 Aug 24 21:14 +07 |
|           | job-app                        |          |                       |         |                     |                     |
| service   | company-service --url -n       | minikube | DESKTOP-PGQU8E8\admin | v1.33.1 | 13 Aug 24 21:15 +07 |                     |
|           | job-app                        |          |                       |         |                     |                     |
| service   | zipkin --url                   | minikube | DESKTOP-PGQU8E8\admin | v1.33.1 | 13 Aug 24 21:28 +07 |                     |
| service   | zipkin --url -n job-app        | minikube | DESKTOP-PGQU8E8\admin | v1.33.1 | 13 Aug 24 21:29 +07 | 13 Aug 24 21:32 +07 |
| service   | zipkin --url                   | minikube | DESKTOP-PGQU8E8\admin | v1.33.1 | 13 Aug 24 21:35 +07 |                     |
| service   | zipkin --url                   | minikube | DESKTOP-PGQU8E8\admin | v1.33.1 | 13 Aug 24 21:36 +07 |                     |
| service   | zipkin --url                   | minikube | DESKTOP-PGQU8E8\admin | v1.33.1 | 13 Aug 24 21:36 +07 |                     |
|-----------|--------------------------------|----------|-----------------------|---------|---------------------|---------------------|


==> Last Start <==
Log file created at: 2024/08/09 21:03:34
Running on machine: DESKTOP-PGQU8E8
Binary: Built with gc go1.22.1 for windows/amd64
Log line format: [IWEF]mmdd hh:mm:ss.uuuuuu threadid file:line] msg
I0809 21:03:34.337342     952 out.go:291] Setting OutFile to fd 84 ...
I0809 21:03:34.338182     952 out.go:338] TERM=,COLORTERM=, which probably does not support color
I0809 21:03:34.338182     952 out.go:304] Setting ErrFile to fd 88...
I0809 21:03:34.338182     952 out.go:338] TERM=,COLORTERM=, which probably does not support color
W0809 21:03:34.358857     952 root.go:314] Error reading config file at C:\Users\admin\.minikube\config\config.json: open C:\Users\admin\.minikube\config\config.json: The system cannot find the file specified.
I0809 21:03:34.398299     952 out.go:298] Setting JSON to false
I0809 21:03:34.404656     952 start.go:129] hostinfo: {"hostname":"DESKTOP-PGQU8E8","uptime":1079563,"bootTime":1722132651,"procs":284,"os":"windows","platform":"Microsoft Windows 10 Home","platformFamily":"Standalone Workstation","platformVersion":"10.0.19045.4651 Build 19045.4651","kernelVersion":"10.0.19045.4651 Build 19045.4651","kernelArch":"x86_64","virtualizationSystem":"","virtualizationRole":"","hostId":"700b1eae-6867-43cd-ad33-268fee13ea5b"}
W0809 21:03:34.404656     952 start.go:137] gopshost.Virtualization returned error: not implemented yet
I0809 21:03:34.406693     952 out.go:177] * minikube v1.33.1 on Microsoft Windows 10 Home 10.0.19045.4651 Build 19045.4651
I0809 21:03:34.407983     952 notify.go:220] Checking for updates...
I0809 21:03:34.428618     952 config.go:182] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.30.0
I0809 21:03:34.429342     952 driver.go:392] Setting default libvirt URI to qemu:///system
I0809 21:03:34.747279     952 docker.go:122] docker version: linux-26.0.0:Docker Desktop 4.29.0 (145265)
I0809 21:03:34.770839     952 cli_runner.go:164] Run: docker system info --format "{{json .}}"
I0809 21:03:37.794788     952 cli_runner.go:217] Completed: docker system info --format "{{json .}}": (3.0239331s)
I0809 21:03:37.796592     952 info.go:266] docker info: {ID:39e94cff-37c8-4ec0-93e8-813897ab0ac2 Containers:12 ContainersRunning:1 ContainersPaused:0 ContainersStopped:11 Images:32 Driver:overlay2 DriverStatus:[[Backing Filesystem extfs] [Supports d_type true] [Using metacopy false] [Native Overlay Diff true] [userxattr false]] SystemStatus:<nil> Plugins:{Volume:[local] Network:[bridge host ipvlan macvlan null overlay] Authorization:<nil> Log:[awslogs fluentd gcplogs gelf journald json-file local splunk syslog]} MemoryLimit:true SwapLimit:true KernelMemory:false KernelMemoryTCP:true CPUCfsPeriod:true CPUCfsQuota:true CPUShares:true CPUSet:true PidsLimit:true IPv4Forwarding:true BridgeNfIptables:true BridgeNfIP6Tables:true Debug:false NFd:65 OomKillDisable:true NGoroutines:77 SystemTime:2024-08-09 14:03:37.72802592 +0000 UTC LoggingDriver:json-file CgroupDriver:cgroupfs NEventsListener:14 KernelVersion:5.15.146.1-microsoft-standard-WSL2 OperatingSystem:Docker Desktop OSType:linux Architecture:x86_64 IndexServerAddress:https://index.docker.io/v1/ RegistryConfig:{AllowNondistributableArtifactsCIDRs:[] AllowNondistributableArtifactsHostnames:[] InsecureRegistryCIDRs:[127.0.0.0/8] IndexConfigs:{DockerIo:{Name:docker.io Mirrors:[] Secure:true Official:true}} Mirrors:[]} NCPU:8 MemTotal:7879073792 GenericResources:<nil> DockerRootDir:/var/lib/docker HTTPProxy:http.docker.internal:3128 HTTPSProxy:http.docker.internal:3128 NoProxy:hubproxy.docker.internal Name:docker-desktop Labels:[com.docker.desktop.address=npipe://\\.\pipe\docker_cli] ExperimentalBuild:false ServerVersion:26.0.0 ClusterStore: ClusterAdvertise: Runtimes:{Runc:{Path:runc}} DefaultRuntime:runc Swarm:{NodeID: NodeAddr: LocalNodeState:inactive ControlAvailable:false Error: RemoteManagers:<nil>} LiveRestoreEnabled:false Isolation: InitBinary:docker-init ContainerdCommit:{ID:ae07eda36dd25f8a1b98dfbf587313b99c0190bb Expected:ae07eda36dd25f8a1b98dfbf587313b99c0190bb} RuncCommit:{ID:v1.1.12-0-g51d5e94 Expected:v1.1.12-0-g51d5e94} InitCommit:{ID:de40ad0 Expected:de40ad0} SecurityOptions:[name=seccomp,profile=unconfined] ProductLicense: Warnings:[WARNING: No blkio throttle.read_bps_device support WARNING: No blkio throttle.write_bps_device support WARNING: No blkio throttle.read_iops_device support WARNING: No blkio throttle.write_iops_device support WARNING: daemon is not using the default seccomp profile] ServerErrors:[] ClientInfo:{Debug:false Plugins:[map[Name:buildx Path:C:\Program Files\Docker\cli-plugins\docker-buildx.exe SchemaVersion:0.1.0 ShortDescription:Docker Buildx Vendor:Docker Inc. Version:v0.13.1-desktop.1] map[Name:compose Path:C:\Program Files\Docker\cli-plugins\docker-compose.exe SchemaVersion:0.1.0 ShortDescription:Docker Compose Vendor:Docker Inc. Version:v2.26.1-desktop.1] map[Name:debug Path:C:\Program Files\Docker\cli-plugins\docker-debug.exe SchemaVersion:0.1.0 ShortDescription:Get a shell into any image or container. Vendor:Docker Inc. Version:0.0.27] map[Name:dev Path:C:\Program Files\Docker\cli-plugins\docker-dev.exe SchemaVersion:0.1.0 ShortDescription:Docker Dev Environments Vendor:Docker Inc. Version:v0.1.2] map[Name:extension Path:C:\Program Files\Docker\cli-plugins\docker-extension.exe SchemaVersion:0.1.0 ShortDescription:Manages Docker extensions Vendor:Docker Inc. Version:v0.2.23] map[Name:feedback Path:C:\Program Files\Docker\cli-plugins\docker-feedback.exe SchemaVersion:0.1.0 ShortDescription:Provide feedback, right in your terminal! Vendor:Docker Inc. Version:v1.0.4] map[Name:init Path:C:\Program Files\Docker\cli-plugins\docker-init.exe SchemaVersion:0.1.0 ShortDescription:Creates Docker-related starter files for your project Vendor:Docker Inc. Version:v1.1.0] map[Name:sbom Path:C:\Program Files\Docker\cli-plugins\docker-sbom.exe SchemaVersion:0.1.0 ShortDescription:View the packaged-based Software Bill Of Materials (SBOM) for an image URL:https://github.com/docker/sbom-cli-plugin Vendor:Anchore Inc. Version:0.6.0] map[Name:scout Path:C:\Program Files\Docker\cli-plugins\docker-scout.exe SchemaVersion:0.1.0 ShortDescription:Docker Scout Vendor:Docker Inc. Version:v1.6.3]] Warnings:<nil>}}
I0809 21:03:37.798129     952 out.go:177] * Using the docker driver based on existing profile
I0809 21:03:37.798707     952 start.go:297] selected driver: docker
I0809 21:03:37.798707     952 start.go:901] validating driver "docker" against &{Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.44@sha256:eb04641328b06c5c4a14f4348470e1046bbcf9c2cbc551486e343d3a49db557e Memory:3800 CPUs:2 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.30.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.30.0 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[dashboard:true default-storageclass:true storage-provisioner:true] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:C:\Users\admin:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s}
I0809 21:03:37.799273     952 start.go:912] status for docker: {Installed:true Healthy:true Running:false NeedsImprovement:false Error:<nil> Reason: Fix: Doc: Version:}
I0809 21:03:37.858707     952 cli_runner.go:164] Run: docker system info --format "{{json .}}"
I0809 21:03:38.263327     952 info.go:266] docker info: {ID:39e94cff-37c8-4ec0-93e8-813897ab0ac2 Containers:12 ContainersRunning:1 ContainersPaused:0 ContainersStopped:11 Images:32 Driver:overlay2 DriverStatus:[[Backing Filesystem extfs] [Supports d_type true] [Using metacopy false] [Native Overlay Diff true] [userxattr false]] SystemStatus:<nil> Plugins:{Volume:[local] Network:[bridge host ipvlan macvlan null overlay] Authorization:<nil> Log:[awslogs fluentd gcplogs gelf journald json-file local splunk syslog]} MemoryLimit:true SwapLimit:true KernelMemory:false KernelMemoryTCP:true CPUCfsPeriod:true CPUCfsQuota:true CPUShares:true CPUSet:true PidsLimit:true IPv4Forwarding:true BridgeNfIptables:true BridgeNfIP6Tables:true Debug:false NFd:65 OomKillDisable:true NGoroutines:77 SystemTime:2024-08-09 14:03:38.234243928 +0000 UTC LoggingDriver:json-file CgroupDriver:cgroupfs NEventsListener:14 KernelVersion:5.15.146.1-microsoft-standard-WSL2 OperatingSystem:Docker Desktop OSType:linux Architecture:x86_64 IndexServerAddress:https://index.docker.io/v1/ RegistryConfig:{AllowNondistributableArtifactsCIDRs:[] AllowNondistributableArtifactsHostnames:[] InsecureRegistryCIDRs:[127.0.0.0/8] IndexConfigs:{DockerIo:{Name:docker.io Mirrors:[] Secure:true Official:true}} Mirrors:[]} NCPU:8 MemTotal:7879073792 GenericResources:<nil> DockerRootDir:/var/lib/docker HTTPProxy:http.docker.internal:3128 HTTPSProxy:http.docker.internal:3128 NoProxy:hubproxy.docker.internal Name:docker-desktop Labels:[com.docker.desktop.address=npipe://\\.\pipe\docker_cli] ExperimentalBuild:false ServerVersion:26.0.0 ClusterStore: ClusterAdvertise: Runtimes:{Runc:{Path:runc}} DefaultRuntime:runc Swarm:{NodeID: NodeAddr: LocalNodeState:inactive ControlAvailable:false Error: RemoteManagers:<nil>} LiveRestoreEnabled:false Isolation: InitBinary:docker-init ContainerdCommit:{ID:ae07eda36dd25f8a1b98dfbf587313b99c0190bb Expected:ae07eda36dd25f8a1b98dfbf587313b99c0190bb} RuncCommit:{ID:v1.1.12-0-g51d5e94 Expected:v1.1.12-0-g51d5e94} InitCommit:{ID:de40ad0 Expected:de40ad0} SecurityOptions:[name=seccomp,profile=unconfined] ProductLicense: Warnings:[WARNING: No blkio throttle.read_bps_device support WARNING: No blkio throttle.write_bps_device support WARNING: No blkio throttle.read_iops_device support WARNING: No blkio throttle.write_iops_device support WARNING: daemon is not using the default seccomp profile] ServerErrors:[] ClientInfo:{Debug:false Plugins:[map[Name:buildx Path:C:\Program Files\Docker\cli-plugins\docker-buildx.exe SchemaVersion:0.1.0 ShortDescription:Docker Buildx Vendor:Docker Inc. Version:v0.13.1-desktop.1] map[Name:compose Path:C:\Program Files\Docker\cli-plugins\docker-compose.exe SchemaVersion:0.1.0 ShortDescription:Docker Compose Vendor:Docker Inc. Version:v2.26.1-desktop.1] map[Name:debug Path:C:\Program Files\Docker\cli-plugins\docker-debug.exe SchemaVersion:0.1.0 ShortDescription:Get a shell into any image or container. Vendor:Docker Inc. Version:0.0.27] map[Name:dev Path:C:\Program Files\Docker\cli-plugins\docker-dev.exe SchemaVersion:0.1.0 ShortDescription:Docker Dev Environments Vendor:Docker Inc. Version:v0.1.2] map[Name:extension Path:C:\Program Files\Docker\cli-plugins\docker-extension.exe SchemaVersion:0.1.0 ShortDescription:Manages Docker extensions Vendor:Docker Inc. Version:v0.2.23] map[Name:feedback Path:C:\Program Files\Docker\cli-plugins\docker-feedback.exe SchemaVersion:0.1.0 ShortDescription:Provide feedback, right in your terminal! Vendor:Docker Inc. Version:v1.0.4] map[Name:init Path:C:\Program Files\Docker\cli-plugins\docker-init.exe SchemaVersion:0.1.0 ShortDescription:Creates Docker-related starter files for your project Vendor:Docker Inc. Version:v1.1.0] map[Name:sbom Path:C:\Program Files\Docker\cli-plugins\docker-sbom.exe SchemaVersion:0.1.0 ShortDescription:View the packaged-based Software Bill Of Materials (SBOM) for an image URL:https://github.com/docker/sbom-cli-plugin Vendor:Anchore Inc. Version:0.6.0] map[Name:scout Path:C:\Program Files\Docker\cli-plugins\docker-scout.exe SchemaVersion:0.1.0 ShortDescription:Docker Scout Vendor:Docker Inc. Version:v1.6.3]] Warnings:<nil>}}
I0809 21:03:38.447557     952 cni.go:84] Creating CNI manager for ""
I0809 21:03:38.447557     952 cni.go:158] "docker" driver + "docker" container runtime found on kubernetes v1.24+, recommending bridge
I0809 21:03:38.448083     952 start.go:340] cluster config:
{Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.44@sha256:eb04641328b06c5c4a14f4348470e1046bbcf9c2cbc551486e343d3a49db557e Memory:3800 CPUs:2 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.30.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.30.0 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[dashboard:true default-storageclass:true storage-provisioner:true] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:C:\Users\admin:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s}
I0809 21:03:38.449779     952 out.go:177] * Starting "minikube" primary control-plane node in "minikube" cluster
I0809 21:03:38.450801     952 cache.go:121] Beginning downloading kic base image for docker with docker
I0809 21:03:38.451408     952 out.go:177] * Pulling base image v0.0.44 ...
I0809 21:03:38.452651     952 preload.go:132] Checking if preload exists for k8s version v1.30.0 and runtime docker
I0809 21:03:38.452651     952 image.go:79] Checking for gcr.io/k8s-minikube/kicbase:v0.0.44@sha256:eb04641328b06c5c4a14f4348470e1046bbcf9c2cbc551486e343d3a49db557e in local docker daemon
I0809 21:03:38.453851     952 preload.go:147] Found local preload: C:\Users\admin\.minikube\cache\preloaded-tarball\preloaded-images-k8s-v18-v1.30.0-docker-overlay2-amd64.tar.lz4
I0809 21:03:38.454381     952 cache.go:56] Caching tarball of preloaded images
I0809 21:03:38.455034     952 preload.go:173] Found C:\Users\admin\.minikube\cache\preloaded-tarball\preloaded-images-k8s-v18-v1.30.0-docker-overlay2-amd64.tar.lz4 in cache, skipping download
I0809 21:03:38.455087     952 cache.go:59] Finished verifying existence of preloaded tar for v1.30.0 on docker
I0809 21:03:38.455087     952 profile.go:143] Saving config to C:\Users\admin\.minikube\profiles\minikube\config.json ...
I0809 21:03:38.711583     952 image.go:83] Found gcr.io/k8s-minikube/kicbase:v0.0.44@sha256:eb04641328b06c5c4a14f4348470e1046bbcf9c2cbc551486e343d3a49db557e in local docker daemon, skipping pull
I0809 21:03:38.711583     952 cache.go:144] gcr.io/k8s-minikube/kicbase:v0.0.44@sha256:eb04641328b06c5c4a14f4348470e1046bbcf9c2cbc551486e343d3a49db557e exists in daemon, skipping load
I0809 21:03:38.711583     952 cache.go:194] Successfully downloaded all kic artifacts
I0809 21:03:38.712126     952 start.go:360] acquireMachinesLock for minikube: {Name:mk643f7e9da83601d587a510a4be0c611c47f2bc Clock:{} Delay:500ms Timeout:10m0s Cancel:<nil>}
I0809 21:03:38.712126     952 start.go:364] duration metric: took 0s to acquireMachinesLock for "minikube"
I0809 21:03:38.712126     952 start.go:96] Skipping create...Using existing machine configuration
I0809 21:03:38.712126     952 fix.go:54] fixHost starting: 
I0809 21:03:38.740611     952 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0809 21:03:38.982006     952 fix.go:112] recreateIfNeeded on minikube: state=Running err=<nil>
W0809 21:03:38.982006     952 fix.go:138] unexpected machine state, will restart: <nil>
I0809 21:03:38.984214     952 out.go:177] * Updating the running docker "minikube" container ...
I0809 21:03:38.985548     952 machine.go:94] provisionDockerMachine start ...
I0809 21:03:39.024570     952 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0809 21:03:39.391861     952 main.go:141] libmachine: Using SSH client type: native
I0809 21:03:39.420297     952 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0xc9a3c0] 0xc9cfa0 <nil>  [] 0s} 127.0.0.1 50136 <nil> <nil>}
I0809 21:03:39.420297     952 main.go:141] libmachine: About to run SSH command:
hostname
I0809 21:03:39.853334     952 main.go:141] libmachine: SSH cmd err, output: <nil>: minikube

I0809 21:03:39.853334     952 ubuntu.go:169] provisioning hostname "minikube"
I0809 21:03:39.909318     952 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0809 21:03:40.172431     952 main.go:141] libmachine: Using SSH client type: native
I0809 21:03:40.172964     952 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0xc9a3c0] 0xc9cfa0 <nil>  [] 0s} 127.0.0.1 50136 <nil> <nil>}
I0809 21:03:40.172964     952 main.go:141] libmachine: About to run SSH command:
sudo hostname minikube && echo "minikube" | sudo tee /etc/hostname
I0809 21:03:40.436493     952 main.go:141] libmachine: SSH cmd err, output: <nil>: minikube

I0809 21:03:40.487249     952 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0809 21:03:40.909466     952 main.go:141] libmachine: Using SSH client type: native
I0809 21:03:40.910411     952 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0xc9a3c0] 0xc9cfa0 <nil>  [] 0s} 127.0.0.1 50136 <nil> <nil>}
I0809 21:03:40.910509     952 main.go:141] libmachine: About to run SSH command:

		if ! grep -xq '.*\sminikube' /etc/hosts; then
			if grep -xq '127.0.1.1\s.*' /etc/hosts; then
				sudo sed -i 's/^127.0.1.1\s.*/127.0.1.1 minikube/g' /etc/hosts;
			else 
				echo '127.0.1.1 minikube' | sudo tee -a /etc/hosts; 
			fi
		fi
I0809 21:03:41.109234     952 main.go:141] libmachine: SSH cmd err, output: <nil>: 
I0809 21:03:41.109795     952 ubuntu.go:175] set auth options {CertDir:C:\Users\admin\.minikube CaCertPath:C:\Users\admin\.minikube\certs\ca.pem CaPrivateKeyPath:C:\Users\admin\.minikube\certs\ca-key.pem CaCertRemotePath:/etc/docker/ca.pem ServerCertPath:C:\Users\admin\.minikube\machines\server.pem ServerKeyPath:C:\Users\admin\.minikube\machines\server-key.pem ClientKeyPath:C:\Users\admin\.minikube\certs\key.pem ServerCertRemotePath:/etc/docker/server.pem ServerKeyRemotePath:/etc/docker/server-key.pem ClientCertPath:C:\Users\admin\.minikube\certs\cert.pem ServerCertSANs:[] StorePath:C:\Users\admin\.minikube}
I0809 21:03:41.109835     952 ubuntu.go:177] setting up certificates
I0809 21:03:41.109850     952 provision.go:84] configureAuth start
I0809 21:03:41.141033     952 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I0809 21:03:41.437487     952 provision.go:143] copyHostCerts
I0809 21:03:41.451877     952 exec_runner.go:144] found C:\Users\admin\.minikube/ca.pem, removing ...
I0809 21:03:41.451877     952 exec_runner.go:203] rm: C:\Users\admin\.minikube\ca.pem
I0809 21:03:41.452601     952 exec_runner.go:151] cp: C:\Users\admin\.minikube\certs\ca.pem --> C:\Users\admin\.minikube/ca.pem (1074 bytes)
I0809 21:03:41.473678     952 exec_runner.go:144] found C:\Users\admin\.minikube/cert.pem, removing ...
I0809 21:03:41.473678     952 exec_runner.go:203] rm: C:\Users\admin\.minikube\cert.pem
I0809 21:03:41.474444     952 exec_runner.go:151] cp: C:\Users\admin\.minikube\certs\cert.pem --> C:\Users\admin\.minikube/cert.pem (1119 bytes)
I0809 21:03:41.490799     952 exec_runner.go:144] found C:\Users\admin\.minikube/key.pem, removing ...
I0809 21:03:41.490799     952 exec_runner.go:203] rm: C:\Users\admin\.minikube\key.pem
I0809 21:03:41.491169     952 exec_runner.go:151] cp: C:\Users\admin\.minikube\certs\key.pem --> C:\Users\admin\.minikube/key.pem (1675 bytes)
I0809 21:03:41.492211     952 provision.go:117] generating server cert: C:\Users\admin\.minikube\machines\server.pem ca-key=C:\Users\admin\.minikube\certs\ca.pem private-key=C:\Users\admin\.minikube\certs\ca-key.pem org=admin.minikube san=[127.0.0.1 192.168.49.2 localhost minikube]
I0809 21:03:42.285545     952 provision.go:177] copyRemoteCerts
I0809 21:03:42.415727     952 ssh_runner.go:195] Run: sudo mkdir -p /etc/docker /etc/docker /etc/docker
I0809 21:03:42.444613     952 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0809 21:03:42.751563     952 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:50136 SSHKeyPath:C:\Users\admin\.minikube\machines\minikube\id_rsa Username:docker}
I0809 21:03:42.910959     952 ssh_runner.go:362] scp C:\Users\admin\.minikube\certs\ca.pem --> /etc/docker/ca.pem (1074 bytes)
I0809 21:03:42.966288     952 ssh_runner.go:362] scp C:\Users\admin\.minikube\machines\server.pem --> /etc/docker/server.pem (1176 bytes)
I0809 21:03:43.017988     952 ssh_runner.go:362] scp C:\Users\admin\.minikube\machines\server-key.pem --> /etc/docker/server-key.pem (1679 bytes)
I0809 21:03:43.073699     952 provision.go:87] duration metric: took 1.9638484s to configureAuth
I0809 21:03:43.073699     952 ubuntu.go:193] setting minikube options for container-runtime
I0809 21:03:43.074780     952 config.go:182] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.30.0
I0809 21:03:43.096241     952 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0809 21:03:43.328201     952 main.go:141] libmachine: Using SSH client type: native
I0809 21:03:43.328201     952 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0xc9a3c0] 0xc9cfa0 <nil>  [] 0s} 127.0.0.1 50136 <nil> <nil>}
I0809 21:03:43.328710     952 main.go:141] libmachine: About to run SSH command:
df --output=fstype / | tail -n 1
I0809 21:03:43.532919     952 main.go:141] libmachine: SSH cmd err, output: <nil>: overlay

I0809 21:03:43.532989     952 ubuntu.go:71] root file system type: overlay
I0809 21:03:43.532989     952 provision.go:314] Updating docker unit: /lib/systemd/system/docker.service ...
I0809 21:03:43.576598     952 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0809 21:03:44.002089     952 main.go:141] libmachine: Using SSH client type: native
I0809 21:03:44.002682     952 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0xc9a3c0] 0xc9cfa0 <nil>  [] 0s} 127.0.0.1 50136 <nil> <nil>}
I0809 21:03:44.003240     952 main.go:141] libmachine: About to run SSH command:
sudo mkdir -p /lib/systemd/system && printf %!s(MISSING) "[Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
BindsTo=containerd.service
After=network-online.target firewalld.service containerd.service
Wants=network-online.target
Requires=docker.socket
StartLimitBurst=3
StartLimitIntervalSec=60

[Service]
Type=notify
Restart=on-failure



# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
# The base configuration already specifies an 'ExecStart=...' command. The first directive
# here is to clear out that command inherited from the base configuration. Without this,
# the command from the base configuration and the command specified here are treated as
# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
# will catch this invalid input and refuse to start the service with an error like:
#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.

# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
ExecStart=
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
ExecReload=/bin/kill -s HUP \$MAINPID

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity

# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
TimeoutStartSec=0

# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes

# kill only the docker process, not all processes in the cgroup
KillMode=process

[Install]
WantedBy=multi-user.target
" | sudo tee /lib/systemd/system/docker.service.new
I0809 21:03:44.324285     952 main.go:141] libmachine: SSH cmd err, output: <nil>: [Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
BindsTo=containerd.service
After=network-online.target firewalld.service containerd.service
Wants=network-online.target
Requires=docker.socket
StartLimitBurst=3
StartLimitIntervalSec=60

[Service]
Type=notify
Restart=on-failure



# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
# The base configuration already specifies an 'ExecStart=...' command. The first directive
# here is to clear out that command inherited from the base configuration. Without this,
# the command from the base configuration and the command specified here are treated as
# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
# will catch this invalid input and refuse to start the service with an error like:
#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.

# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
ExecStart=
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
ExecReload=/bin/kill -s HUP $MAINPID

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity

# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
TimeoutStartSec=0

# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes

# kill only the docker process, not all processes in the cgroup
KillMode=process

[Install]
WantedBy=multi-user.target

I0809 21:03:44.347561     952 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0809 21:03:44.572671     952 main.go:141] libmachine: Using SSH client type: native
I0809 21:03:44.572988     952 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0xc9a3c0] 0xc9cfa0 <nil>  [] 0s} 127.0.0.1 50136 <nil> <nil>}
I0809 21:03:44.572988     952 main.go:141] libmachine: About to run SSH command:
sudo diff -u /lib/systemd/system/docker.service /lib/systemd/system/docker.service.new || { sudo mv /lib/systemd/system/docker.service.new /lib/systemd/system/docker.service; sudo systemctl -f daemon-reload && sudo systemctl -f enable docker && sudo systemctl -f restart docker; }
I0809 21:03:44.821878     952 main.go:141] libmachine: SSH cmd err, output: <nil>: 
I0809 21:03:44.821878     952 machine.go:97] duration metric: took 5.8363306s to provisionDockerMachine
I0809 21:03:44.822034     952 start.go:293] postStartSetup for "minikube" (driver="docker")
I0809 21:03:44.822034     952 start.go:322] creating required directories: [/etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs]
I0809 21:03:44.869839     952 ssh_runner.go:195] Run: sudo mkdir -p /etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs
I0809 21:03:44.889064     952 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0809 21:03:45.096903     952 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:50136 SSHKeyPath:C:\Users\admin\.minikube\machines\minikube\id_rsa Username:docker}
I0809 21:03:45.306400     952 ssh_runner.go:195] Run: cat /etc/os-release
I0809 21:03:45.316913     952 main.go:141] libmachine: Couldn't set key VERSION_CODENAME, no corresponding struct field found
I0809 21:03:45.317481     952 main.go:141] libmachine: Couldn't set key PRIVACY_POLICY_URL, no corresponding struct field found
I0809 21:03:45.317517     952 main.go:141] libmachine: Couldn't set key UBUNTU_CODENAME, no corresponding struct field found
I0809 21:03:45.317533     952 info.go:137] Remote host: Ubuntu 22.04.4 LTS
I0809 21:03:45.317558     952 filesync.go:126] Scanning C:\Users\admin\.minikube\addons for local assets ...
I0809 21:03:45.318196     952 filesync.go:126] Scanning C:\Users\admin\.minikube\files for local assets ...
I0809 21:03:45.318735     952 start.go:296] duration metric: took 496.7004ms for postStartSetup
I0809 21:03:45.353983     952 ssh_runner.go:195] Run: sh -c "df -h /var | awk 'NR==2{print $5}'"
I0809 21:03:45.376715     952 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0809 21:03:45.617740     952 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:50136 SSHKeyPath:C:\Users\admin\.minikube\machines\minikube\id_rsa Username:docker}
I0809 21:03:45.822525     952 ssh_runner.go:195] Run: sh -c "df -BG /var | awk 'NR==2{print $4}'"
I0809 21:03:45.838703     952 fix.go:56] duration metric: took 7.1265554s for fixHost
I0809 21:03:45.838784     952 start.go:83] releasing machines lock for "minikube", held for 7.1266583s
I0809 21:03:45.871652     952 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I0809 21:03:46.179496     952 ssh_runner.go:195] Run: curl -sS -m 2 https://registry.k8s.io/
I0809 21:03:46.226029     952 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0809 21:03:46.231447     952 ssh_runner.go:195] Run: cat /version.json
I0809 21:03:46.259748     952 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0809 21:03:46.433788     952 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:50136 SSHKeyPath:C:\Users\admin\.minikube\machines\minikube\id_rsa Username:docker}
I0809 21:03:46.501258     952 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:50136 SSHKeyPath:C:\Users\admin\.minikube\machines\minikube\id_rsa Username:docker}
I0809 21:03:47.101483     952 ssh_runner.go:195] Run: systemctl --version
I0809 21:03:47.152188     952 ssh_runner.go:195] Run: sh -c "stat /etc/cni/net.d/*loopback.conf*"
I0809 21:03:47.196058     952 ssh_runner.go:195] Run: sudo find \etc\cni\net.d -maxdepth 1 -type f -name *loopback.conf* -not -name *.mk_disabled -exec sh -c "grep -q loopback {} && ( grep -q name {} || sudo sed -i '/"type": "loopback"/i \ \ \ \ "name": "loopback",' {} ) && sudo sed -i 's|"cniVersion": ".*"|"cniVersion": "1.0.0"|g' {}" ;
W0809 21:03:47.233588     952 start.go:438] unable to name loopback interface in configureRuntimes: unable to patch loopback cni config "/etc/cni/net.d/*loopback.conf*": sudo find \etc\cni\net.d -maxdepth 1 -type f -name *loopback.conf* -not -name *.mk_disabled -exec sh -c "grep -q loopback {} && ( grep -q name {} || sudo sed -i '/"type": "loopback"/i \ \ \ \ "name": "loopback",' {} ) && sudo sed -i 's|"cniVersion": ".*"|"cniVersion": "1.0.0"|g' {}" ;: Process exited with status 1
stdout:

stderr:
find: '\\etc\\cni\\net.d': No such file or directory
I0809 21:03:47.265609     952 ssh_runner.go:195] Run: sudo find /etc/cni/net.d -maxdepth 1 -type f ( ( -name *bridge* -or -name *podman* ) -and -not -name *.mk_disabled ) -printf "%!p(MISSING), " -exec sh -c "sudo mv {} {}.mk_disabled" ;
I0809 21:03:47.287940     952 cni.go:259] no active bridge cni configs found in "/etc/cni/net.d" - nothing to disable
I0809 21:03:47.287940     952 start.go:494] detecting cgroup driver to use...
I0809 21:03:47.287940     952 detect.go:196] detected "cgroupfs" cgroup driver on host os
I0809 21:03:47.288946     952 ssh_runner.go:195] Run: /bin/bash -c "sudo mkdir -p /etc && printf %!s(MISSING) "runtime-endpoint: unix:///run/containerd/containerd.sock
" | sudo tee /etc/crictl.yaml"
I0809 21:03:47.342186     952 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)sandbox_image = .*$|\1sandbox_image = "registry.k8s.io/pause:3.9"|' /etc/containerd/config.toml"
I0809 21:03:47.386349     952 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)restrict_oom_score_adj = .*$|\1restrict_oom_score_adj = false|' /etc/containerd/config.toml"
I0809 21:03:47.406112     952 containerd.go:146] configuring containerd to use "cgroupfs" as cgroup driver...
I0809 21:03:47.431030     952 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)SystemdCgroup = .*$|\1SystemdCgroup = false|g' /etc/containerd/config.toml"
I0809 21:03:47.500062     952 ssh_runner.go:195] Run: sh -c "sudo sed -i 's|"io.containerd.runtime.v1.linux"|"io.containerd.runc.v2"|g' /etc/containerd/config.toml"
I0809 21:03:47.544003     952 ssh_runner.go:195] Run: sh -c "sudo sed -i '/systemd_cgroup/d' /etc/containerd/config.toml"
I0809 21:03:47.589805     952 ssh_runner.go:195] Run: sh -c "sudo sed -i 's|"io.containerd.runc.v1"|"io.containerd.runc.v2"|g' /etc/containerd/config.toml"
I0809 21:03:47.633773     952 ssh_runner.go:195] Run: sh -c "sudo rm -rf /etc/cni/net.mk"
I0809 21:03:47.682012     952 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)conf_dir = .*$|\1conf_dir = "/etc/cni/net.d"|g' /etc/containerd/config.toml"
I0809 21:03:47.754262     952 ssh_runner.go:195] Run: sh -c "sudo sed -i '/^ *enable_unprivileged_ports = .*/d' /etc/containerd/config.toml"
I0809 21:03:47.806294     952 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)\[plugins."io.containerd.grpc.v1.cri"\]|&\n\1  enable_unprivileged_ports = true|' /etc/containerd/config.toml"
I0809 21:03:47.862182     952 ssh_runner.go:195] Run: sudo sysctl net.bridge.bridge-nf-call-iptables
I0809 21:03:47.916267     952 ssh_runner.go:195] Run: sudo sh -c "echo 1 > /proc/sys/net/ipv4/ip_forward"
I0809 21:03:48.011261     952 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0809 21:03:48.252546     952 ssh_runner.go:195] Run: sudo systemctl restart containerd
I0809 21:03:48.517862     952 start.go:494] detecting cgroup driver to use...
I0809 21:03:48.517862     952 detect.go:196] detected "cgroupfs" cgroup driver on host os
I0809 21:03:48.580322     952 ssh_runner.go:195] Run: sudo systemctl cat docker.service
I0809 21:03:48.618010     952 cruntime.go:279] skipping containerd shutdown because we are bound to it
I0809 21:03:48.651715     952 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service crio
I0809 21:03:48.680464     952 ssh_runner.go:195] Run: /bin/bash -c "sudo mkdir -p /etc && printf %!s(MISSING) "runtime-endpoint: unix:///var/run/cri-dockerd.sock
" | sudo tee /etc/crictl.yaml"
I0809 21:03:48.779443     952 ssh_runner.go:195] Run: which cri-dockerd
I0809 21:03:48.835015     952 ssh_runner.go:195] Run: sudo mkdir -p /etc/systemd/system/cri-docker.service.d
I0809 21:03:48.854684     952 ssh_runner.go:362] scp memory --> /etc/systemd/system/cri-docker.service.d/10-cni.conf (189 bytes)
I0809 21:03:48.920820     952 ssh_runner.go:195] Run: sudo systemctl unmask docker.service
I0809 21:03:49.213062     952 ssh_runner.go:195] Run: sudo systemctl enable docker.socket
I0809 21:03:49.417809     952 docker.go:574] configuring docker to use "cgroupfs" as cgroup driver...
I0809 21:03:49.417809     952 ssh_runner.go:362] scp memory --> /etc/docker/daemon.json (130 bytes)
I0809 21:03:49.489421     952 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0809 21:03:49.737685     952 ssh_runner.go:195] Run: sudo systemctl restart docker
I0809 21:03:51.217982     952 ssh_runner.go:235] Completed: sudo systemctl restart docker: (1.4802972s)
I0809 21:03:51.282330     952 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service cri-docker.socket
I0809 21:03:51.430271     952 ssh_runner.go:195] Run: sudo systemctl stop cri-docker.socket
I0809 21:03:51.533620     952 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service cri-docker.service
I0809 21:03:51.664135     952 ssh_runner.go:195] Run: sudo systemctl unmask cri-docker.socket
I0809 21:03:52.049163     952 ssh_runner.go:195] Run: sudo systemctl enable cri-docker.socket
I0809 21:03:52.433079     952 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0809 21:03:52.815179     952 ssh_runner.go:195] Run: sudo systemctl restart cri-docker.socket
I0809 21:03:52.984121     952 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service cri-docker.service
I0809 21:03:53.119826     952 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0809 21:03:53.384878     952 ssh_runner.go:195] Run: sudo systemctl restart cri-docker.service
I0809 21:03:54.114218     952 start.go:541] Will wait 60s for socket path /var/run/cri-dockerd.sock
I0809 21:03:54.181410     952 ssh_runner.go:195] Run: stat /var/run/cri-dockerd.sock
I0809 21:03:54.205796     952 start.go:562] Will wait 60s for crictl version
I0809 21:03:54.237331     952 ssh_runner.go:195] Run: which crictl
I0809 21:03:54.273564     952 ssh_runner.go:195] Run: sudo /usr/bin/crictl version
I0809 21:03:54.555457     952 start.go:578] Version:  0.1.0
RuntimeName:  docker
RuntimeVersion:  26.1.1
RuntimeApiVersion:  v1
I0809 21:03:54.597118     952 ssh_runner.go:195] Run: docker version --format {{.Server.Version}}
I0809 21:03:55.014997     952 ssh_runner.go:195] Run: docker version --format {{.Server.Version}}
I0809 21:03:55.061100     952 out.go:204] * Preparing Kubernetes v1.30.0 on Docker 26.1.1 ...
I0809 21:03:55.080062     952 cli_runner.go:164] Run: docker exec -t minikube dig +short host.docker.internal
I0809 21:03:55.466608     952 network.go:96] got host ip for mount in container by digging dns: 192.168.65.254
I0809 21:03:55.541907     952 ssh_runner.go:195] Run: grep 192.168.65.254	host.minikube.internal$ /etc/hosts
I0809 21:03:55.567446     952 ssh_runner.go:195] Run: /bin/bash -c "{ grep -v $'\thost.minikube.internal$' "/etc/hosts"; echo "192.168.65.254	host.minikube.internal"; } > /tmp/h.$$; sudo cp /tmp/h.$$ "/etc/hosts""
I0809 21:03:55.711405     952 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "8443/tcp") 0).HostPort}}'" minikube
I0809 21:03:56.049717     952 kubeadm.go:877] updating cluster {Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.44@sha256:eb04641328b06c5c4a14f4348470e1046bbcf9c2cbc551486e343d3a49db557e Memory:3800 CPUs:2 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.30.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.30.0 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[dashboard:true default-storageclass:true storage-provisioner:true] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:C:\Users\admin:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s} ...
I0809 21:03:56.049717     952 preload.go:132] Checking if preload exists for k8s version v1.30.0 and runtime docker
I0809 21:03:56.077154     952 ssh_runner.go:195] Run: docker images --format {{.Repository}}:{{.Tag}}
I0809 21:03:56.119759     952 docker.go:685] Got preloaded images: -- stdout --
registry.k8s.io/kube-apiserver:v1.30.0
registry.k8s.io/kube-controller-manager:v1.30.0
registry.k8s.io/kube-scheduler:v1.30.0
registry.k8s.io/kube-proxy:v1.30.0
registry.k8s.io/etcd:3.5.12-0
registry.k8s.io/coredns/coredns:v1.11.1
registry.k8s.io/pause:3.9
kubernetesui/dashboard:<none>
kubernetesui/metrics-scraper:<none>
redis:6.2.5
gcr.io/k8s-minikube/storage-provisioner:v5
nginx:1.14.2

-- /stdout --
I0809 21:03:56.119759     952 docker.go:615] Images already preloaded, skipping extraction
I0809 21:03:56.140391     952 ssh_runner.go:195] Run: docker images --format {{.Repository}}:{{.Tag}}
I0809 21:03:56.205710     952 docker.go:685] Got preloaded images: -- stdout --
registry.k8s.io/kube-apiserver:v1.30.0
registry.k8s.io/kube-scheduler:v1.30.0
registry.k8s.io/kube-controller-manager:v1.30.0
registry.k8s.io/kube-proxy:v1.30.0
registry.k8s.io/etcd:3.5.12-0
registry.k8s.io/coredns/coredns:v1.11.1
registry.k8s.io/pause:3.9
kubernetesui/dashboard:<none>
kubernetesui/metrics-scraper:<none>
redis:6.2.5
gcr.io/k8s-minikube/storage-provisioner:v5
nginx:1.14.2

-- /stdout --
I0809 21:03:56.205710     952 cache_images.go:84] Images are preloaded, skipping loading
I0809 21:03:56.205710     952 kubeadm.go:928] updating node { 192.168.49.2 8443 v1.30.0 docker true true} ...
I0809 21:03:56.206330     952 kubeadm.go:940] kubelet [Unit]
Wants=docker.socket

[Service]
ExecStart=
ExecStart=/var/lib/minikube/binaries/v1.30.0/kubelet --bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf --config=/var/lib/kubelet/config.yaml --hostname-override=minikube --kubeconfig=/etc/kubernetes/kubelet.conf --node-ip=192.168.49.2

[Install]
 config:
{KubernetesVersion:v1.30.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:}
I0809 21:03:56.233928     952 ssh_runner.go:195] Run: docker info --format {{.CgroupDriver}}
I0809 21:03:56.790440     952 cni.go:84] Creating CNI manager for ""
I0809 21:03:56.790440     952 cni.go:158] "docker" driver + "docker" container runtime found on kubernetes v1.24+, recommending bridge
I0809 21:03:56.790440     952 kubeadm.go:84] Using pod CIDR: 10.244.0.0/16
I0809 21:03:56.790440     952 kubeadm.go:181] kubeadm options: {CertDir:/var/lib/minikube/certs ServiceCIDR:10.96.0.0/12 PodSubnet:10.244.0.0/16 AdvertiseAddress:192.168.49.2 APIServerPort:8443 KubernetesVersion:v1.30.0 EtcdDataDir:/var/lib/minikube/etcd EtcdExtraArgs:map[] ClusterName:minikube NodeName:minikube DNSDomain:cluster.local CRISocket:/var/run/cri-dockerd.sock ImageRepository: ComponentOptions:[{Component:apiServer ExtraArgs:map[enable-admission-plugins:NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota] Pairs:map[certSANs:["127.0.0.1", "localhost", "192.168.49.2"]]} {Component:controllerManager ExtraArgs:map[allocate-node-cidrs:true leader-elect:false] Pairs:map[]} {Component:scheduler ExtraArgs:map[leader-elect:false] Pairs:map[]}] FeatureArgs:map[] NodeIP:192.168.49.2 CgroupDriver:cgroupfs ClientCAFile:/var/lib/minikube/certs/ca.crt StaticPodPath:/etc/kubernetes/manifests ControlPlaneAddress:control-plane.minikube.internal KubeProxyOptions:map[] ResolvConfSearchRegression:false KubeletConfigOpts:map[containerRuntimeEndpoint:unix:///var/run/cri-dockerd.sock hairpinMode:hairpin-veth runtimeRequestTimeout:15m] PrependCriSocketUnix:true}
I0809 21:03:56.791237     952 kubeadm.go:187] kubeadm config:
apiVersion: kubeadm.k8s.io/v1beta3
kind: InitConfiguration
localAPIEndpoint:
  advertiseAddress: 192.168.49.2
  bindPort: 8443
bootstrapTokens:
  - groups:
      - system:bootstrappers:kubeadm:default-node-token
    ttl: 24h0m0s
    usages:
      - signing
      - authentication
nodeRegistration:
  criSocket: unix:///var/run/cri-dockerd.sock
  name: "minikube"
  kubeletExtraArgs:
    node-ip: 192.168.49.2
  taints: []
---
apiVersion: kubeadm.k8s.io/v1beta3
kind: ClusterConfiguration
apiServer:
  certSANs: ["127.0.0.1", "localhost", "192.168.49.2"]
  extraArgs:
    enable-admission-plugins: "NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota"
controllerManager:
  extraArgs:
    allocate-node-cidrs: "true"
    leader-elect: "false"
scheduler:
  extraArgs:
    leader-elect: "false"
certificatesDir: /var/lib/minikube/certs
clusterName: mk
controlPlaneEndpoint: control-plane.minikube.internal:8443
etcd:
  local:
    dataDir: /var/lib/minikube/etcd
    extraArgs:
      proxy-refresh-interval: "70000"
kubernetesVersion: v1.30.0
networking:
  dnsDomain: cluster.local
  podSubnet: "10.244.0.0/16"
  serviceSubnet: 10.96.0.0/12
---
apiVersion: kubelet.config.k8s.io/v1beta1
kind: KubeletConfiguration
authentication:
  x509:
    clientCAFile: /var/lib/minikube/certs/ca.crt
cgroupDriver: cgroupfs
containerRuntimeEndpoint: unix:///var/run/cri-dockerd.sock
hairpinMode: hairpin-veth
runtimeRequestTimeout: 15m
clusterDomain: "cluster.local"
# disable disk resource management by default
imageGCHighThresholdPercent: 100
evictionHard:
  nodefs.available: "0%!"(MISSING)
  nodefs.inodesFree: "0%!"(MISSING)
  imagefs.available: "0%!"(MISSING)
failSwapOn: false
staticPodPath: /etc/kubernetes/manifests
---
apiVersion: kubeproxy.config.k8s.io/v1alpha1
kind: KubeProxyConfiguration
clusterCIDR: "10.244.0.0/16"
metricsBindAddress: 0.0.0.0:10249
conntrack:
  maxPerCore: 0
# Skip setting "net.netfilter.nf_conntrack_tcp_timeout_established"
  tcpEstablishedTimeout: 0s
# Skip setting "net.netfilter.nf_conntrack_tcp_timeout_close"
  tcpCloseWaitTimeout: 0s

I0809 21:03:56.851911     952 ssh_runner.go:195] Run: sudo ls /var/lib/minikube/binaries/v1.30.0
I0809 21:03:56.936336     952 binaries.go:44] Found k8s binaries, skipping transfer
I0809 21:03:56.991652     952 ssh_runner.go:195] Run: sudo mkdir -p /etc/systemd/system/kubelet.service.d /lib/systemd/system /var/tmp/minikube
I0809 21:03:57.036612     952 ssh_runner.go:362] scp memory --> /etc/systemd/system/kubelet.service.d/10-kubeadm.conf (307 bytes)
I0809 21:03:57.106327     952 ssh_runner.go:362] scp memory --> /lib/systemd/system/kubelet.service (352 bytes)
I0809 21:03:57.199354     952 ssh_runner.go:362] scp memory --> /var/tmp/minikube/kubeadm.yaml.new (2150 bytes)
I0809 21:03:57.378936     952 ssh_runner.go:195] Run: grep 192.168.49.2	control-plane.minikube.internal$ /etc/hosts
I0809 21:03:57.409778     952 ssh_runner.go:195] Run: /bin/bash -c "{ grep -v $'\tcontrol-plane.minikube.internal$' "/etc/hosts"; echo "192.168.49.2	control-plane.minikube.internal"; } > /tmp/h.$$; sudo cp /tmp/h.$$ "/etc/hosts""
I0809 21:03:57.574901     952 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0809 21:03:58.061207     952 ssh_runner.go:195] Run: sudo systemctl start kubelet
I0809 21:03:58.118788     952 certs.go:68] Setting up C:\Users\admin\.minikube\profiles\minikube for IP: 192.168.49.2
I0809 21:03:58.118788     952 certs.go:194] generating shared ca certs ...
I0809 21:03:58.119426     952 certs.go:226] acquiring lock for ca certs: {Name:mkfd7a320f78a704b321a6e757eb4483941c4372 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0809 21:03:58.173214     952 certs.go:235] skipping valid "minikubeCA" ca cert: C:\Users\admin\.minikube\ca.key
I0809 21:03:58.248606     952 certs.go:235] skipping valid "proxyClientCA" ca cert: C:\Users\admin\.minikube\proxy-client-ca.key
I0809 21:03:58.248606     952 certs.go:256] generating profile certs ...
I0809 21:03:58.250900     952 certs.go:359] skipping valid signed profile cert regeneration for "minikube-user": C:\Users\admin\.minikube\profiles\minikube\client.key
I0809 21:03:58.317360     952 certs.go:359] skipping valid signed profile cert regeneration for "minikube": C:\Users\admin\.minikube\profiles\minikube\apiserver.key.7fb57e3c
I0809 21:03:58.382483     952 certs.go:359] skipping valid signed profile cert regeneration for "aggregator": C:\Users\admin\.minikube\profiles\minikube\proxy-client.key
I0809 21:03:58.396498     952 certs.go:484] found cert: C:\Users\admin\.minikube\certs\ca-key.pem (1675 bytes)
I0809 21:03:58.397670     952 certs.go:484] found cert: C:\Users\admin\.minikube\certs\ca.pem (1074 bytes)
I0809 21:03:58.398820     952 certs.go:484] found cert: C:\Users\admin\.minikube\certs\cert.pem (1119 bytes)
I0809 21:03:58.399856     952 certs.go:484] found cert: C:\Users\admin\.minikube\certs\key.pem (1675 bytes)
I0809 21:03:58.405316     952 ssh_runner.go:362] scp C:\Users\admin\.minikube\ca.crt --> /var/lib/minikube/certs/ca.crt (1111 bytes)
I0809 21:03:58.514330     952 ssh_runner.go:362] scp C:\Users\admin\.minikube\ca.key --> /var/lib/minikube/certs/ca.key (1675 bytes)
I0809 21:03:58.564610     952 ssh_runner.go:362] scp C:\Users\admin\.minikube\proxy-client-ca.crt --> /var/lib/minikube/certs/proxy-client-ca.crt (1119 bytes)
I0809 21:03:58.641233     952 ssh_runner.go:362] scp C:\Users\admin\.minikube\proxy-client-ca.key --> /var/lib/minikube/certs/proxy-client-ca.key (1679 bytes)
I0809 21:03:58.730369     952 ssh_runner.go:362] scp C:\Users\admin\.minikube\profiles\minikube\apiserver.crt --> /var/lib/minikube/certs/apiserver.crt (1411 bytes)
I0809 21:03:58.788102     952 ssh_runner.go:362] scp C:\Users\admin\.minikube\profiles\minikube\apiserver.key --> /var/lib/minikube/certs/apiserver.key (1675 bytes)
I0809 21:03:58.838043     952 ssh_runner.go:362] scp C:\Users\admin\.minikube\profiles\minikube\proxy-client.crt --> /var/lib/minikube/certs/proxy-client.crt (1147 bytes)
I0809 21:03:58.889382     952 ssh_runner.go:362] scp C:\Users\admin\.minikube\profiles\minikube\proxy-client.key --> /var/lib/minikube/certs/proxy-client.key (1675 bytes)
I0809 21:03:58.947578     952 ssh_runner.go:362] scp C:\Users\admin\.minikube\ca.crt --> /usr/share/ca-certificates/minikubeCA.pem (1111 bytes)
I0809 21:03:59.243686     952 ssh_runner.go:362] scp memory --> /var/lib/minikube/kubeconfig (738 bytes)
I0809 21:03:59.587030     952 ssh_runner.go:195] Run: openssl version
I0809 21:04:00.003345     952 ssh_runner.go:195] Run: sudo /bin/bash -c "test -s /usr/share/ca-certificates/minikubeCA.pem && ln -fs /usr/share/ca-certificates/minikubeCA.pem /etc/ssl/certs/minikubeCA.pem"
I0809 21:04:00.117160     952 ssh_runner.go:195] Run: ls -la /usr/share/ca-certificates/minikubeCA.pem
I0809 21:04:00.140688     952 certs.go:528] hashing: -rw-r--r-- 1 root root 1111 Aug  2 08:18 /usr/share/ca-certificates/minikubeCA.pem
I0809 21:04:00.187227     952 ssh_runner.go:195] Run: openssl x509 -hash -noout -in /usr/share/ca-certificates/minikubeCA.pem
I0809 21:04:00.436915     952 ssh_runner.go:195] Run: sudo /bin/bash -c "test -L /etc/ssl/certs/b5213941.0 || ln -fs /etc/ssl/certs/minikubeCA.pem /etc/ssl/certs/b5213941.0"
I0809 21:04:00.613493     952 ssh_runner.go:195] Run: stat /var/lib/minikube/certs/apiserver-kubelet-client.crt
I0809 21:04:00.705000     952 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/apiserver-etcd-client.crt -checkend 86400
I0809 21:04:00.920616     952 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/apiserver-kubelet-client.crt -checkend 86400
I0809 21:04:00.995639     952 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/etcd/server.crt -checkend 86400
I0809 21:04:01.341676     952 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/etcd/healthcheck-client.crt -checkend 86400
I0809 21:04:01.593352     952 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/etcd/peer.crt -checkend 86400
I0809 21:04:01.659533     952 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/front-proxy-client.crt -checkend 86400
I0809 21:04:01.738114     952 kubeadm.go:391] StartCluster: {Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.44@sha256:eb04641328b06c5c4a14f4348470e1046bbcf9c2cbc551486e343d3a49db557e Memory:3800 CPUs:2 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.30.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.30.0 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[dashboard:true default-storageclass:true storage-provisioner:true] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:C:\Users\admin:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s}
I0809 21:04:01.857321     952 ssh_runner.go:195] Run: docker ps --filter status=paused --filter=name=k8s_.*_(kube-system)_ --format={{.ID}}
I0809 21:04:02.941099     952 ssh_runner.go:195] Run: sudo ls /var/lib/kubelet/kubeadm-flags.env /var/lib/kubelet/config.yaml /var/lib/minikube/etcd
W0809 21:04:03.080657     952 kubeadm.go:404] apiserver tunnel failed: apiserver port not set
I0809 21:04:03.080657     952 kubeadm.go:407] found existing configuration files, will attempt cluster restart
I0809 21:04:03.080657     952 kubeadm.go:587] restartPrimaryControlPlane start ...
I0809 21:04:03.168022     952 ssh_runner.go:195] Run: sudo test -d /data/minikube
I0809 21:04:03.368071     952 kubeadm.go:129] /data/minikube skipping compat symlinks: sudo test -d /data/minikube: Process exited with status 1
stdout:

stderr:
I0809 21:04:03.416655     952 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "8443/tcp") 0).HostPort}}'" minikube
I0809 21:04:03.992704     952 kubeconfig.go:125] found "minikube" server: "https://127.0.0.1:60914"
I0809 21:04:03.993247     952 kubeconfig.go:47] verify endpoint returned: got: 127.0.0.1:60914, want: 127.0.0.1:50135
I0809 21:04:03.994156     952 kubeconfig.go:62] C:\Users\admin\.kube\config needs updating (will repair): [kubeconfig needs server address update]
I0809 21:04:03.994721     952 lock.go:35] WriteFile acquiring C:\Users\admin\.kube\config: {Name:mk0f6f650be7ab5b3e1ae83f14563cf1eb0bd30b Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0809 21:04:04.176722     952 ssh_runner.go:195] Run: sudo diff -u /var/tmp/minikube/kubeadm.yaml /var/tmp/minikube/kubeadm.yaml.new
I0809 21:04:04.256546     952 kubeadm.go:624] The running cluster does not require reconfiguration: 127.0.0.1
I0809 21:04:04.256546     952 kubeadm.go:591] duration metric: took 1.1758892s to restartPrimaryControlPlane
I0809 21:04:04.256546     952 kubeadm.go:393] duration metric: took 2.5184322s to StartCluster
I0809 21:04:04.256546     952 settings.go:142] acquiring lock: {Name:mk5c419c3fabff3b09a504acb297c33a24a93937 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0809 21:04:04.256546     952 settings.go:150] Updating kubeconfig:  C:\Users\admin\.kube\config
I0809 21:04:04.258443     952 lock.go:35] WriteFile acquiring C:\Users\admin\.kube\config: {Name:mk0f6f650be7ab5b3e1ae83f14563cf1eb0bd30b Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0809 21:04:04.260157     952 start.go:234] Will wait 6m0s for node &{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.30.0 ContainerRuntime:docker ControlPlane:true Worker:true}
I0809 21:04:04.262023     952 out.go:177] * Verifying Kubernetes components...
I0809 21:04:04.260157     952 addons.go:502] enable addons start: toEnable=map[ambassador:false auto-pause:false cloud-spanner:false csi-hostpath-driver:false dashboard:true default-storageclass:true efk:false freshpod:false gcp-auth:false gvisor:false headlamp:false helm-tiller:false inaccel:false ingress:false ingress-dns:false inspektor-gadget:false istio:false istio-provisioner:false kong:false kubeflow:false kubevirt:false logviewer:false metallb:false metrics-server:false nvidia-device-plugin:false nvidia-driver-installer:false nvidia-gpu-device-plugin:false olm:false pod-security-policy:false portainer:false registry:false registry-aliases:false registry-creds:false storage-provisioner:true storage-provisioner-gluster:false storage-provisioner-rancher:false volumesnapshots:false yakd:false]
I0809 21:04:04.260888     952 config.go:182] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.30.0
I0809 21:04:04.262023     952 addons.go:69] Setting storage-provisioner=true in profile "minikube"
I0809 21:04:04.262023     952 addons.go:234] Setting addon storage-provisioner=true in "minikube"
W0809 21:04:04.262023     952 addons.go:243] addon storage-provisioner should already be in state true
I0809 21:04:04.262023     952 addons.go:69] Setting dashboard=true in profile "minikube"
I0809 21:04:04.262023     952 addons.go:234] Setting addon dashboard=true in "minikube"
W0809 21:04:04.262023     952 addons.go:243] addon dashboard should already be in state true
I0809 21:04:04.262023     952 addons.go:69] Setting default-storageclass=true in profile "minikube"
I0809 21:04:04.262023     952 addons_storage_classes.go:33] enableOrDisableStorageClasses default-storageclass=true on "minikube"
I0809 21:04:04.262551     952 host.go:66] Checking if "minikube" exists ...
I0809 21:04:04.262551     952 host.go:66] Checking if "minikube" exists ...
I0809 21:04:04.386605     952 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0809 21:04:04.426089     952 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0809 21:04:04.485831     952 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0809 21:04:04.683333     952 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0809 21:04:05.308304     952 out.go:177]   - Using image docker.io/kubernetesui/dashboard:v2.7.0
I0809 21:04:05.309435     952 out.go:177]   - Using image docker.io/kubernetesui/metrics-scraper:v1.0.8
I0809 21:04:05.310756     952 addons.go:426] installing /etc/kubernetes/addons/dashboard-ns.yaml
I0809 21:04:05.310756     952 ssh_runner.go:362] scp dashboard/dashboard-ns.yaml --> /etc/kubernetes/addons/dashboard-ns.yaml (759 bytes)
I0809 21:04:05.354774     952 addons.go:234] Setting addon default-storageclass=true in "minikube"
W0809 21:04:05.354774     952 addons.go:243] addon default-storageclass should already be in state true
I0809 21:04:05.375374     952 host.go:66] Checking if "minikube" exists ...
I0809 21:04:05.401612     952 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0809 21:04:05.468034     952 out.go:177]   - Using image gcr.io/k8s-minikube/storage-provisioner:v5
I0809 21:04:05.486028     952 addons.go:426] installing /etc/kubernetes/addons/storage-provisioner.yaml
I0809 21:04:05.486028     952 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/storage-provisioner.yaml (2676 bytes)
I0809 21:04:05.581621     952 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0809 21:04:05.611297     952 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0809 21:04:05.642544     952 ssh_runner.go:235] Completed: sudo systemctl daemon-reload: (1.2164548s)
I0809 21:04:05.746732     952 ssh_runner.go:195] Run: sudo systemctl start kubelet
I0809 21:04:05.925351     952 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "8443/tcp") 0).HostPort}}'" minikube
I0809 21:04:06.257454     952 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:50136 SSHKeyPath:C:\Users\admin\.minikube\machines\minikube\id_rsa Username:docker}
I0809 21:04:06.277691     952 addons.go:426] installing /etc/kubernetes/addons/storageclass.yaml
I0809 21:04:06.277691     952 ssh_runner.go:362] scp storageclass/storageclass.yaml --> /etc/kubernetes/addons/storageclass.yaml (271 bytes)
I0809 21:04:06.308681     952 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:50136 SSHKeyPath:C:\Users\admin\.minikube\machines\minikube\id_rsa Username:docker}
I0809 21:04:06.314781     952 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0809 21:04:06.412695     952 api_server.go:52] waiting for apiserver process to appear ...
I0809 21:04:06.466815     952 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0809 21:04:06.519240     952 api_server.go:72] duration metric: took 2.2590832s to wait for apiserver process to appear ...
I0809 21:04:06.519240     952 api_server.go:88] waiting for apiserver healthz status ...
I0809 21:04:06.519240     952 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:50135/healthz ...
I0809 21:04:06.525204     952 api_server.go:269] stopped: https://127.0.0.1:50135/healthz: Get "https://127.0.0.1:50135/healthz": EOF
I0809 21:04:06.545409     952 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply -f /etc/kubernetes/addons/storage-provisioner.yaml
I0809 21:04:06.595837     952 addons.go:426] installing /etc/kubernetes/addons/dashboard-clusterrole.yaml
I0809 21:04:06.595837     952 ssh_runner.go:362] scp dashboard/dashboard-clusterrole.yaml --> /etc/kubernetes/addons/dashboard-clusterrole.yaml (1001 bytes)
I0809 21:04:06.669695     952 addons.go:426] installing /etc/kubernetes/addons/dashboard-clusterrolebinding.yaml
I0809 21:04:06.670266     952 ssh_runner.go:362] scp dashboard/dashboard-clusterrolebinding.yaml --> /etc/kubernetes/addons/dashboard-clusterrolebinding.yaml (1018 bytes)
I0809 21:04:06.717242     952 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:50136 SSHKeyPath:C:\Users\admin\.minikube\machines\minikube\id_rsa Username:docker}
I0809 21:04:06.753258     952 addons.go:426] installing /etc/kubernetes/addons/dashboard-configmap.yaml
I0809 21:04:06.753258     952 ssh_runner.go:362] scp dashboard/dashboard-configmap.yaml --> /etc/kubernetes/addons/dashboard-configmap.yaml (837 bytes)
I0809 21:04:06.897896     952 addons.go:426] installing /etc/kubernetes/addons/dashboard-dp.yaml
I0809 21:04:06.897896     952 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/dashboard-dp.yaml (4288 bytes)
I0809 21:04:07.028246     952 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:50135/healthz ...
I0809 21:04:07.034405     952 api_server.go:269] stopped: https://127.0.0.1:50135/healthz: Get "https://127.0.0.1:50135/healthz": EOF
I0809 21:04:07.055911     952 addons.go:426] installing /etc/kubernetes/addons/dashboard-role.yaml
I0809 21:04:07.055911     952 ssh_runner.go:362] scp dashboard/dashboard-role.yaml --> /etc/kubernetes/addons/dashboard-role.yaml (1724 bytes)
I0809 21:04:07.238018     952 addons.go:426] installing /etc/kubernetes/addons/dashboard-rolebinding.yaml
I0809 21:04:07.238018     952 ssh_runner.go:362] scp dashboard/dashboard-rolebinding.yaml --> /etc/kubernetes/addons/dashboard-rolebinding.yaml (1046 bytes)
I0809 21:04:07.291699     952 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply -f /etc/kubernetes/addons/storageclass.yaml
I0809 21:04:07.349006     952 addons.go:426] installing /etc/kubernetes/addons/dashboard-sa.yaml
I0809 21:04:07.349006     952 ssh_runner.go:362] scp dashboard/dashboard-sa.yaml --> /etc/kubernetes/addons/dashboard-sa.yaml (837 bytes)
I0809 21:04:07.452901     952 addons.go:426] installing /etc/kubernetes/addons/dashboard-secret.yaml
I0809 21:04:07.452901     952 ssh_runner.go:362] scp dashboard/dashboard-secret.yaml --> /etc/kubernetes/addons/dashboard-secret.yaml (1389 bytes)
I0809 21:04:07.525710     952 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:50135/healthz ...
I0809 21:04:07.529337     952 api_server.go:269] stopped: https://127.0.0.1:50135/healthz: Get "https://127.0.0.1:50135/healthz": EOF
I0809 21:04:07.549742     952 addons.go:426] installing /etc/kubernetes/addons/dashboard-svc.yaml
I0809 21:04:07.550375     952 ssh_runner.go:362] scp dashboard/dashboard-svc.yaml --> /etc/kubernetes/addons/dashboard-svc.yaml (1294 bytes)
I0809 21:04:07.756681     952 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply -f /etc/kubernetes/addons/dashboard-ns.yaml -f /etc/kubernetes/addons/dashboard-clusterrole.yaml -f /etc/kubernetes/addons/dashboard-clusterrolebinding.yaml -f /etc/kubernetes/addons/dashboard-configmap.yaml -f /etc/kubernetes/addons/dashboard-dp.yaml -f /etc/kubernetes/addons/dashboard-role.yaml -f /etc/kubernetes/addons/dashboard-rolebinding.yaml -f /etc/kubernetes/addons/dashboard-sa.yaml -f /etc/kubernetes/addons/dashboard-secret.yaml -f /etc/kubernetes/addons/dashboard-svc.yaml
I0809 21:04:08.028588     952 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:50135/healthz ...
I0809 21:04:08.040477     952 api_server.go:269] stopped: https://127.0.0.1:50135/healthz: Get "https://127.0.0.1:50135/healthz": EOF
I0809 21:04:08.526481     952 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:50135/healthz ...
I0809 21:04:08.544591     952 api_server.go:269] stopped: https://127.0.0.1:50135/healthz: Get "https://127.0.0.1:50135/healthz": EOF
I0809 21:04:08.696605     952 ssh_runner.go:235] Completed: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply -f /etc/kubernetes/addons/storage-provisioner.yaml: (2.1511956s)
W0809 21:04:08.696605     952 addons.go:452] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply -f /etc/kubernetes/addons/storage-provisioner.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storage-provisioner.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0809 21:04:08.696605     952 retry.go:31] will retry after 274.821911ms: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply -f /etc/kubernetes/addons/storage-provisioner.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storage-provisioner.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0809 21:04:08.739695     952 ssh_runner.go:235] Completed: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply -f /etc/kubernetes/addons/storageclass.yaml: (1.4479962s)
W0809 21:04:08.739695     952 addons.go:452] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply -f /etc/kubernetes/addons/storageclass.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storageclass.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0809 21:04:08.739695     952 retry.go:31] will retry after 159.635952ms: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply -f /etc/kubernetes/addons/storageclass.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storageclass.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
W0809 21:04:08.742255     952 addons.go:452] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply -f /etc/kubernetes/addons/dashboard-ns.yaml -f /etc/kubernetes/addons/dashboard-clusterrole.yaml -f /etc/kubernetes/addons/dashboard-clusterrolebinding.yaml -f /etc/kubernetes/addons/dashboard-configmap.yaml -f /etc/kubernetes/addons/dashboard-dp.yaml -f /etc/kubernetes/addons/dashboard-role.yaml -f /etc/kubernetes/addons/dashboard-rolebinding.yaml -f /etc/kubernetes/addons/dashboard-sa.yaml -f /etc/kubernetes/addons/dashboard-secret.yaml -f /etc/kubernetes/addons/dashboard-svc.yaml: Process exited with status 1
stdout:

stderr:
error validating "/etc/kubernetes/addons/dashboard-ns.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-clusterrole.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-clusterrolebinding.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-configmap.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-dp.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-role.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-rolebinding.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-sa.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-secret.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-svc.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0809 21:04:08.742368     952 retry.go:31] will retry after 261.581097ms: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply -f /etc/kubernetes/addons/dashboard-ns.yaml -f /etc/kubernetes/addons/dashboard-clusterrole.yaml -f /etc/kubernetes/addons/dashboard-clusterrolebinding.yaml -f /etc/kubernetes/addons/dashboard-configmap.yaml -f /etc/kubernetes/addons/dashboard-dp.yaml -f /etc/kubernetes/addons/dashboard-role.yaml -f /etc/kubernetes/addons/dashboard-rolebinding.yaml -f /etc/kubernetes/addons/dashboard-sa.yaml -f /etc/kubernetes/addons/dashboard-secret.yaml -f /etc/kubernetes/addons/dashboard-svc.yaml: Process exited with status 1
stdout:

stderr:
error validating "/etc/kubernetes/addons/dashboard-ns.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-clusterrole.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-clusterrolebinding.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-configmap.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-dp.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-role.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-rolebinding.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-sa.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-secret.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-svc.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0809 21:04:08.988028     952 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml
I0809 21:04:09.012248     952 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml
I0809 21:04:09.023107     952 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:50135/healthz ...
I0809 21:04:09.061961     952 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply --force -f /etc/kubernetes/addons/dashboard-ns.yaml -f /etc/kubernetes/addons/dashboard-clusterrole.yaml -f /etc/kubernetes/addons/dashboard-clusterrolebinding.yaml -f /etc/kubernetes/addons/dashboard-configmap.yaml -f /etc/kubernetes/addons/dashboard-dp.yaml -f /etc/kubernetes/addons/dashboard-role.yaml -f /etc/kubernetes/addons/dashboard-rolebinding.yaml -f /etc/kubernetes/addons/dashboard-sa.yaml -f /etc/kubernetes/addons/dashboard-secret.yaml -f /etc/kubernetes/addons/dashboard-svc.yaml
I0809 21:04:14.027994     952 api_server.go:269] stopped: https://127.0.0.1:50135/healthz: Get "https://127.0.0.1:50135/healthz": context deadline exceeded (Client.Timeout exceeded while awaiting headers)
I0809 21:04:14.027994     952 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:50135/healthz ...
I0809 21:04:17.592874     952 api_server.go:279] https://127.0.0.1:50135/healthz returned 403:
{"kind":"Status","apiVersion":"v1","metadata":{},"status":"Failure","message":"forbidden: User \"system:anonymous\" cannot get path \"/healthz\"","reason":"Forbidden","details":{},"code":403}
W0809 21:04:17.592874     952 api_server.go:103] status: https://127.0.0.1:50135/healthz returned error 403:
{"kind":"Status","apiVersion":"v1","metadata":{},"status":"Failure","message":"forbidden: User \"system:anonymous\" cannot get path \"/healthz\"","reason":"Forbidden","details":{},"code":403}
I0809 21:04:17.592874     952 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:50135/healthz ...
I0809 21:04:17.739971     952 api_server.go:279] https://127.0.0.1:50135/healthz returned 403:
{"kind":"Status","apiVersion":"v1","metadata":{},"status":"Failure","message":"forbidden: User \"system:anonymous\" cannot get path \"/healthz\"","reason":"Forbidden","details":{},"code":403}
W0809 21:04:17.739971     952 api_server.go:103] status: https://127.0.0.1:50135/healthz returned error 403:
{"kind":"Status","apiVersion":"v1","metadata":{},"status":"Failure","message":"forbidden: User \"system:anonymous\" cannot get path \"/healthz\"","reason":"Forbidden","details":{},"code":403}
I0809 21:04:18.022268     952 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:50135/healthz ...
I0809 21:04:18.239485     952 api_server.go:279] https://127.0.0.1:50135/healthz returned 500:
[+]ping ok
[+]log ok
[-]etcd failed: reason withheld
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[-]poststarthook/start-service-ip-repair-controllers failed: reason withheld
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: reason withheld
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[-]poststarthook/bootstrap-controller failed: reason withheld
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[-]poststarthook/apiservice-registration-controller failed: reason withheld
[+]poststarthook/apiservice-status-available-controller ok
[-]poststarthook/apiservice-discovery-controller failed: reason withheld
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
W0809 21:04:18.239504     952 api_server.go:103] status: https://127.0.0.1:50135/healthz returned error 500:
[+]ping ok
[+]log ok
[-]etcd failed: reason withheld
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[-]poststarthook/start-service-ip-repair-controllers failed: reason withheld
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: reason withheld
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[-]poststarthook/bootstrap-controller failed: reason withheld
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[-]poststarthook/apiservice-registration-controller failed: reason withheld
[+]poststarthook/apiservice-status-available-controller ok
[-]poststarthook/apiservice-discovery-controller failed: reason withheld
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
I0809 21:04:18.538522     952 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:50135/healthz ...
I0809 21:04:18.663559     952 api_server.go:279] https://127.0.0.1:50135/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: reason withheld
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
W0809 21:04:18.663559     952 api_server.go:103] status: https://127.0.0.1:50135/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: reason withheld
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
I0809 21:04:19.022130     952 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:50135/healthz ...
I0809 21:04:19.179102     952 api_server.go:279] https://127.0.0.1:50135/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
W0809 21:04:19.179102     952 api_server.go:103] status: https://127.0.0.1:50135/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
I0809 21:04:19.537900     952 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:50135/healthz ...
I0809 21:04:19.737824     952 api_server.go:279] https://127.0.0.1:50135/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
W0809 21:04:19.737824     952 api_server.go:103] status: https://127.0.0.1:50135/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
I0809 21:04:20.033981     952 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:50135/healthz ...
I0809 21:04:20.238206     952 api_server.go:279] https://127.0.0.1:50135/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
W0809 21:04:20.238206     952 api_server.go:103] status: https://127.0.0.1:50135/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
I0809 21:04:20.522577     952 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:50135/healthz ...
I0809 21:04:20.736247     952 api_server.go:279] https://127.0.0.1:50135/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
W0809 21:04:20.736247     952 api_server.go:103] status: https://127.0.0.1:50135/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
I0809 21:04:21.035324     952 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:50135/healthz ...
I0809 21:04:21.150916     952 api_server.go:279] https://127.0.0.1:50135/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
W0809 21:04:21.150916     952 api_server.go:103] status: https://127.0.0.1:50135/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
I0809 21:04:21.344329     952 ssh_runner.go:235] Completed: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml: (12.3563014s)
I0809 21:04:21.528088     952 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:50135/healthz ...
I0809 21:04:21.735766     952 api_server.go:279] https://127.0.0.1:50135/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
W0809 21:04:21.735766     952 api_server.go:103] status: https://127.0.0.1:50135/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
I0809 21:04:22.029451     952 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:50135/healthz ...
I0809 21:04:22.155147     952 api_server.go:279] https://127.0.0.1:50135/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
W0809 21:04:22.155295     952 api_server.go:103] status: https://127.0.0.1:50135/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
I0809 21:04:22.534441     952 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:50135/healthz ...
I0809 21:04:22.654161     952 api_server.go:279] https://127.0.0.1:50135/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
W0809 21:04:22.654386     952 api_server.go:103] status: https://127.0.0.1:50135/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
I0809 21:04:23.037102     952 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:50135/healthz ...
I0809 21:04:23.146815     952 api_server.go:279] https://127.0.0.1:50135/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
W0809 21:04:23.146815     952 api_server.go:103] status: https://127.0.0.1:50135/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
I0809 21:04:23.526210     952 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:50135/healthz ...
I0809 21:04:23.561673     952 api_server.go:279] https://127.0.0.1:50135/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
W0809 21:04:23.561673     952 api_server.go:103] status: https://127.0.0.1:50135/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
I0809 21:04:24.029289     952 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:50135/healthz ...
I0809 21:04:24.154441     952 api_server.go:279] https://127.0.0.1:50135/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
W0809 21:04:24.154441     952 api_server.go:103] status: https://127.0.0.1:50135/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
I0809 21:04:24.533963     952 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:50135/healthz ...
I0809 21:04:24.563278     952 api_server.go:279] https://127.0.0.1:50135/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
W0809 21:04:24.563278     952 api_server.go:103] status: https://127.0.0.1:50135/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
I0809 21:04:25.028767     952 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:50135/healthz ...
I0809 21:04:25.064036     952 api_server.go:279] https://127.0.0.1:50135/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
W0809 21:04:25.064036     952 api_server.go:103] status: https://127.0.0.1:50135/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
I0809 21:04:25.520618     952 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:50135/healthz ...
I0809 21:04:25.540552     952 api_server.go:279] https://127.0.0.1:50135/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
W0809 21:04:25.540552     952 api_server.go:103] status: https://127.0.0.1:50135/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
I0809 21:04:26.038906     952 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:50135/healthz ...
I0809 21:04:26.236674     952 api_server.go:279] https://127.0.0.1:50135/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
W0809 21:04:26.237823     952 api_server.go:103] status: https://127.0.0.1:50135/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
I0809 21:04:26.563878     952 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:50135/healthz ...
I0809 21:04:26.641643     952 api_server.go:279] https://127.0.0.1:50135/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
W0809 21:04:26.641643     952 api_server.go:103] status: https://127.0.0.1:50135/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
I0809 21:04:27.076161     952 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:50135/healthz ...
I0809 21:04:27.225706     952 api_server.go:279] https://127.0.0.1:50135/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
W0809 21:04:27.226583     952 api_server.go:103] status: https://127.0.0.1:50135/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
I0809 21:04:27.574026     952 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:50135/healthz ...
I0809 21:04:27.736736     952 api_server.go:279] https://127.0.0.1:50135/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
W0809 21:04:27.736736     952 api_server.go:103] status: https://127.0.0.1:50135/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
I0809 21:04:28.020965     952 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:50135/healthz ...
I0809 21:04:28.145527     952 api_server.go:279] https://127.0.0.1:50135/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
W0809 21:04:28.147284     952 api_server.go:103] status: https://127.0.0.1:50135/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
I0809 21:04:28.522101     952 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:50135/healthz ...
I0809 21:04:28.625917     952 api_server.go:279] https://127.0.0.1:50135/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
W0809 21:04:28.625917     952 api_server.go:103] status: https://127.0.0.1:50135/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
I0809 21:04:29.020512     952 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:50135/healthz ...
I0809 21:04:29.237334     952 api_server.go:279] https://127.0.0.1:50135/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
W0809 21:04:29.237649     952 api_server.go:103] status: https://127.0.0.1:50135/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
I0809 21:04:29.532073     952 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:50135/healthz ...
I0809 21:04:29.736145     952 api_server.go:279] https://127.0.0.1:50135/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
W0809 21:04:29.736357     952 api_server.go:103] status: https://127.0.0.1:50135/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
I0809 21:04:30.032192     952 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:50135/healthz ...
I0809 21:04:30.234360     952 api_server.go:279] https://127.0.0.1:50135/healthz returned 200:
ok
I0809 21:04:30.249134     952 api_server.go:141] control plane version: v1.30.0
I0809 21:04:30.249134     952 api_server.go:131] duration metric: took 23.7298946s to wait for apiserver health ...
I0809 21:04:30.252909     952 system_pods.go:43] waiting for kube-system pods to appear ...
I0809 21:04:30.474349     952 system_pods.go:59] 7 kube-system pods found
I0809 21:04:30.474349     952 system_pods.go:61] "coredns-7db6d8ff4d-rk9sx" [66a6c6e5-bc81-479d-a841-afd928a3b79b] Running / Ready:ContainersNotReady (containers with unready status: [coredns]) / ContainersReady:ContainersNotReady (containers with unready status: [coredns])
I0809 21:04:30.474349     952 system_pods.go:61] "etcd-minikube" [dd7ba1d8-8891-48d8-95be-68b4e1f64040] Running
I0809 21:04:30.474349     952 system_pods.go:61] "kube-apiserver-minikube" [df0e885e-0eda-4192-87d5-baf4ea900952] Running / Ready:ContainersNotReady (containers with unready status: [kube-apiserver]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-apiserver])
I0809 21:04:30.474349     952 system_pods.go:61] "kube-controller-manager-minikube" [4f37b72a-2885-49c7-bb42-6e30114265ed] Running
I0809 21:04:30.474349     952 system_pods.go:61] "kube-proxy-qs92z" [9841988a-a32e-4e1d-bd71-8cc2d22f46f7] Running / Ready:ContainersNotReady (containers with unready status: [kube-proxy]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-proxy])
I0809 21:04:30.474349     952 system_pods.go:61] "kube-scheduler-minikube" [1b8aad1e-b4e1-44f9-af7d-b7db22aa5802] Running
I0809 21:04:30.474349     952 system_pods.go:61] "storage-provisioner" [5e1dd67d-b837-4fa7-99bf-c4f77214459d] Running / Ready:ContainersNotReady (containers with unready status: [storage-provisioner]) / ContainersReady:ContainersNotReady (containers with unready status: [storage-provisioner])
I0809 21:04:30.474349     952 system_pods.go:74] duration metric: took 221.44ms to wait for pod list to return data ...
I0809 21:04:30.474349     952 kubeadm.go:576] duration metric: took 26.214192s to wait for: map[apiserver:true system_pods:true]
I0809 21:04:30.474349     952 node_conditions.go:102] verifying NodePressure condition ...
I0809 21:04:30.644671     952 node_conditions.go:122] node storage ephemeral capacity is 1055762868Ki
I0809 21:04:30.644671     952 node_conditions.go:123] node cpu capacity is 8
I0809 21:04:30.644671     952 node_conditions.go:105] duration metric: took 170.3221ms to run NodePressure ...
I0809 21:04:30.644671     952 start.go:240] waiting for startup goroutines ...
I0809 21:04:30.875484     952 ssh_runner.go:235] Completed: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml: (21.8632357s)


==> Docker <==
Aug 13 14:29:40 minikube cri-dockerd[1373]: time="2024-08-13T14:29:40Z" level=info msg="Stop pulling image docker130303/microservices-review-service:latest: Status: Image is up to date for docker130303/microservices-review-service:latest"
Aug 13 14:29:55 minikube dockerd[1084]: time="2024-08-13T14:29:55.798878646Z" level=info msg="ignoring event" container=1e4bb847d63e43f5863fda9af99bdcc8ff33dc2f18130d4f705dc955f7231bb1 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Aug 13 14:30:02 minikube cri-dockerd[1373]: time="2024-08-13T14:30:02Z" level=info msg="Stop pulling image docker130303/microservices-company-service:latest: Status: Image is up to date for docker130303/microservices-company-service:latest"
Aug 13 14:30:21 minikube dockerd[1084]: time="2024-08-13T14:30:21.989965001Z" level=info msg="ignoring event" container=25a42c974e31b1278e57b90a8135b37bda4f0b06df143309258a2a8cafaa6c41 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Aug 13 14:30:29 minikube cri-dockerd[1373]: time="2024-08-13T14:30:29Z" level=info msg="Stop pulling image docker130303/microservices-job-service:latest: Status: Image is up to date for docker130303/microservices-job-service:latest"
Aug 13 14:32:47 minikube dockerd[1084]: time="2024-08-13T14:32:47.865593842Z" level=info msg="ignoring event" container=3de911c0007e877e7a47a77b59e4ac2da22d6cc3b2a6d8ccd5e40236a6b2d129 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Aug 13 14:32:47 minikube dockerd[1084]: time="2024-08-13T14:32:47.865684249Z" level=info msg="ignoring event" container=50c53abcdca6190992533856ac85e8002fe7f77fcd8c1caac15ddcba81e772d4 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Aug 13 14:32:48 minikube dockerd[1084]: time="2024-08-13T14:32:48.060933917Z" level=info msg="ignoring event" container=5fdb123c65756379c6274a1415f0931627d8f38cf44354d8479b6a0da299e9a3 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Aug 13 14:32:50 minikube dockerd[1084]: time="2024-08-13T14:32:50.861619973Z" level=info msg="ignoring event" container=026edcf2f146a23f3a9776e669a8e095681fa8ea5909fb460b45de1691a81941 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Aug 13 14:32:52 minikube dockerd[1084]: time="2024-08-13T14:32:52.263024824Z" level=info msg="ignoring event" container=a2ec0042ecb3709659367a983ca7f4ad90bad91edb19f35bb5694e94404b749a module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Aug 13 14:32:54 minikube dockerd[1084]: time="2024-08-13T14:32:54.169557219Z" level=info msg="ignoring event" container=7d036053e8b77b327c3a8403991c9ce9f6ddba671d770416394b3a187267c922 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Aug 13 14:32:57 minikube dockerd[1084]: time="2024-08-13T14:32:57.461479689Z" level=info msg="ignoring event" container=f118cedda4c773c67508996041014294ece16ae15d0dfd6a18f0d48715904eb7 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Aug 13 14:32:57 minikube dockerd[1084]: time="2024-08-13T14:32:57.785638531Z" level=info msg="ignoring event" container=4e34845b29659a9948fbfbc8b0d9a1d136e6b22a66fc22fb74de723d3c18db9d module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Aug 13 14:33:00 minikube dockerd[1084]: time="2024-08-13T14:33:00.068416360Z" level=info msg="ignoring event" container=42dd128a7696cf6a627ad1d24bb19d2a476e1bbeb72fe63763ca0a34f1dbc571 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Aug 13 14:33:01 minikube cri-dockerd[1373]: time="2024-08-13T14:33:01Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/76de1efa54ea2af8fb8487c13e877c38e375cbf31ba09a9887a0ff228b32727c/resolv.conf as [nameserver 10.96.0.10 search job-app.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Aug 13 14:33:01 minikube cri-dockerd[1373]: time="2024-08-13T14:33:01Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/3e47be3ebc14287ce2e2af39d18bfb0ab074c2876843a16b2205743a81131559/resolv.conf as [nameserver 10.96.0.10 search job-app.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Aug 13 14:33:04 minikube cri-dockerd[1373]: time="2024-08-13T14:33:04Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/3abd0466640eb2c2f891c675fc7026439f84e84ca8ac5a4334d74ae4d81e83aa/resolv.conf as [nameserver 10.96.0.10 search job-app.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Aug 13 14:33:04 minikube cri-dockerd[1373]: time="2024-08-13T14:33:04Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/5f6144323fa69b3c2ad1b5c98ccf5fe5af6a3b8a8bcb00b17aacf2aa8e79421b/resolv.conf as [nameserver 10.96.0.10 search job-app.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Aug 13 14:33:05 minikube cri-dockerd[1373]: time="2024-08-13T14:33:05Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/4d0563114f3ded8d992e7910a78ee06c9dd966e5f9710f26961d25944843ac30/resolv.conf as [nameserver 10.96.0.10 search job-app.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Aug 13 14:33:07 minikube dockerd[1084]: time="2024-08-13T14:33:07.461704235Z" level=info msg="ignoring event" container=3034116f2cdbf3c4332b7b4497bfe6671685c986b735c8a7de57a39c79d6dccb module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Aug 13 14:33:08 minikube dockerd[1084]: time="2024-08-13T14:33:08.370803527Z" level=info msg="Container failed to exit within 30s of signal 15 - using the force" container=fe9e2e1dbd549808277f0019be5584537549eaefd35c6543802e730b348609b2 spanID=b1bafa135d4a61a9 traceID=02901b57e1ac0bfd0a436fc3bf17fc9d
Aug 13 14:33:09 minikube dockerd[1084]: time="2024-08-13T14:33:09.878522862Z" level=info msg="ignoring event" container=fe9e2e1dbd549808277f0019be5584537549eaefd35c6543802e730b348609b2 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Aug 13 14:33:10 minikube dockerd[1084]: time="2024-08-13T14:33:10.260802327Z" level=info msg="Container failed to exit within 30s of signal 15 - using the force" container=090c4b2c3969cdc8e6ffe18885b915b27bce57031d474322683b4adf863039ac spanID=f01396646973736f traceID=b07838935d5e7998ba8383fb88f42fa8
Aug 13 14:33:10 minikube cri-dockerd[1373]: time="2024-08-13T14:33:10Z" level=error msg="error getting RW layer size for container ID '5fdb123c65756379c6274a1415f0931627d8f38cf44354d8479b6a0da299e9a3': Error response from daemon: No such container: 5fdb123c65756379c6274a1415f0931627d8f38cf44354d8479b6a0da299e9a3"
Aug 13 14:33:10 minikube cri-dockerd[1373]: time="2024-08-13T14:33:10Z" level=error msg="Set backoffDuration to : 1m0s for container ID '5fdb123c65756379c6274a1415f0931627d8f38cf44354d8479b6a0da299e9a3'"
Aug 13 14:33:11 minikube dockerd[1084]: time="2024-08-13T14:33:11.184048288Z" level=info msg="ignoring event" container=090c4b2c3969cdc8e6ffe18885b915b27bce57031d474322683b4adf863039ac module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Aug 13 14:33:12 minikube dockerd[1084]: time="2024-08-13T14:33:12.174376526Z" level=info msg="Container failed to exit within 30s of signal 15 - using the force" container=bdf80d67a4d29c975d47ec7c9a90d20cc4edd5bffae419f895803622389cbe25 spanID=202697540f3f154d traceID=128acd1cadc00332ce67b5695220e3d7
Aug 13 14:33:12 minikube dockerd[1084]: time="2024-08-13T14:33:12.383947673Z" level=info msg="ignoring event" container=92a42cd40582c48dc5d0376c992ebdffc311451e3c171afea6d5c0413f9ae6ad module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Aug 13 14:33:12 minikube dockerd[1084]: time="2024-08-13T14:33:12.392406908Z" level=info msg="ignoring event" container=8c819e0b6412ccdf9b0d624226a006ac692425c41cf5a7c4d578a18037bd5784 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Aug 13 14:33:12 minikube dockerd[1084]: time="2024-08-13T14:33:12.592847078Z" level=info msg="ignoring event" container=bdf80d67a4d29c975d47ec7c9a90d20cc4edd5bffae419f895803622389cbe25 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Aug 13 14:33:13 minikube dockerd[1084]: time="2024-08-13T14:33:13.866136967Z" level=info msg="ignoring event" container=34aa0422508519267cde063fc2306dc62604f28bb9ae84d3f08699fc22caddc4 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Aug 13 14:33:14 minikube cri-dockerd[1373]: time="2024-08-13T14:33:14Z" level=info msg="Stop pulling image docker130303/microservices-company-service:latest: Status: Image is up to date for docker130303/microservices-company-service:latest"
Aug 13 14:33:17 minikube dockerd[1084]: time="2024-08-13T14:33:17.581749142Z" level=info msg="ignoring event" container=492d6db2ea64479d296c074aeb1fb25bb6a5d1fbf193698c98214c6264b257a6 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Aug 13 14:33:18 minikube dockerd[1084]: time="2024-08-13T14:33:18.389290722Z" level=info msg="ignoring event" container=3e47be3ebc14287ce2e2af39d18bfb0ab074c2876843a16b2205743a81131559 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Aug 13 14:33:20 minikube cri-dockerd[1373]: time="2024-08-13T14:33:20Z" level=info msg="Stop pulling image docker130303/microservices-job-service:latest: Status: Image is up to date for docker130303/microservices-job-service:latest"
Aug 13 14:33:22 minikube dockerd[1084]: time="2024-08-13T14:33:22.089281089Z" level=info msg="ignoring event" container=c2cb312021f7bd3671c966a609e03a9eb45702780d1e95dfb4a1bf7b3a7b3005 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Aug 13 14:33:22 minikube dockerd[1084]: time="2024-08-13T14:33:22.662088435Z" level=info msg="ignoring event" container=76de1efa54ea2af8fb8487c13e877c38e375cbf31ba09a9887a0ff228b32727c module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Aug 13 14:33:25 minikube cri-dockerd[1373]: time="2024-08-13T14:33:25Z" level=info msg="Stop pulling image docker130303/microservices-review-service:latest: Status: Image is up to date for docker130303/microservices-review-service:latest"
Aug 13 14:33:28 minikube dockerd[1084]: time="2024-08-13T14:33:28.697358482Z" level=info msg="ignoring event" container=343c5789f030af5a545c178534c8f95f9958de9571bd41e60260f83f49609e8f module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Aug 13 14:33:29 minikube cri-dockerd[1373]: time="2024-08-13T14:33:29Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"review-service-deployment-6bb45ffdd4-qw9k6_job-app\": unexpected command output nsenter: cannot open /proc/424063/ns/net: No such file or directory\n with error: exit status 1"
Aug 13 14:33:29 minikube dockerd[1084]: time="2024-08-13T14:33:29.781362995Z" level=info msg="ignoring event" container=5f6144323fa69b3c2ad1b5c98ccf5fe5af6a3b8a8bcb00b17aacf2aa8e79421b module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Aug 13 14:33:31 minikube cri-dockerd[1373]: time="2024-08-13T14:33:31Z" level=info msg="Stop pulling image openzipkin/zipkin:latest: Status: Image is up to date for openzipkin/zipkin:latest"
Aug 13 14:33:32 minikube dockerd[1084]: time="2024-08-13T14:33:32.275089210Z" level=info msg="ignoring event" container=0077bc4137180d370a3ea2ee6b5b8aa663a407a54347a760c810209010cf40c8 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Aug 13 14:33:32 minikube dockerd[1084]: time="2024-08-13T14:33:32.600260562Z" level=info msg="ignoring event" container=4d0563114f3ded8d992e7910a78ee06c9dd966e5f9710f26961d25944843ac30 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Aug 13 14:33:43 minikube dockerd[1084]: time="2024-08-13T14:33:43.778935677Z" level=info msg="Container failed to exit within 30s of signal 15 - using the force" container=a5e6c7f6e274f224b3e0fd79baa4f612018c3dfdd6e78eb1df5568b1f74b5264 spanID=9991f987d5f4708a traceID=7858ec44cd32a0fbd45b10902c8f7afe
Aug 13 14:33:44 minikube dockerd[1084]: time="2024-08-13T14:33:44.038871119Z" level=info msg="ignoring event" container=a5e6c7f6e274f224b3e0fd79baa4f612018c3dfdd6e78eb1df5568b1f74b5264 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Aug 13 14:33:44 minikube dockerd[1084]: time="2024-08-13T14:33:44.496508661Z" level=info msg="ignoring event" container=3abd0466640eb2c2f891c675fc7026439f84e84ca8ac5a4334d74ae4d81e83aa module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Aug 13 14:35:11 minikube cri-dockerd[1373]: time="2024-08-13T14:35:11Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/22e6da0ef166b0fb5fb1cf501956130c594da99e8f74996d2bf4979c5c2cc2cd/resolv.conf as [nameserver 10.96.0.10 search job-app.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Aug 13 14:35:17 minikube cri-dockerd[1373]: time="2024-08-13T14:35:17Z" level=info msg="Stop pulling image openzipkin/zipkin:latest: Status: Image is up to date for openzipkin/zipkin:latest"
Aug 13 14:35:20 minikube cri-dockerd[1373]: time="2024-08-13T14:35:20Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/5f107b1a944544eedc707bd97dcede363fa92d54b010ceb9ea46fbe6386b354c/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Aug 13 14:36:02 minikube cri-dockerd[1373]: time="2024-08-13T14:36:02Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/8f3b33c76df76a118a5ef02be1c3cb6c1ec3a10d9ac88c75a313868f70c15666/resolv.conf as [nameserver 10.96.0.10 search job-app.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Aug 13 14:36:09 minikube dockerd[1084]: time="2024-08-13T14:36:09.672530671Z" level=info msg="ignoring event" container=eda8a0816e1125bfe524101615eb63b28fa70716a9ffca13b02c80a95b6f952e module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Aug 13 14:36:09 minikube cri-dockerd[1373]: time="2024-08-13T14:36:09Z" level=info msg="Stop pulling image openzipkin/zipkin:latest: Status: Image is up to date for openzipkin/zipkin:latest"
Aug 13 14:36:10 minikube dockerd[1084]: time="2024-08-13T14:36:10.079900352Z" level=info msg="ignoring event" container=22e6da0ef166b0fb5fb1cf501956130c594da99e8f74996d2bf4979c5c2cc2cd module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Aug 13 14:36:11 minikube dockerd[1084]: time="2024-08-13T14:36:11.861416241Z" level=info msg="ignoring event" container=fc687a7b0016177616c93e00e5360bb5b0a7bad8a10d730855d2d34988bb2caf module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Aug 13 14:36:12 minikube dockerd[1084]: time="2024-08-13T14:36:12.481195169Z" level=info msg="ignoring event" container=8f3b33c76df76a118a5ef02be1c3cb6c1ec3a10d9ac88c75a313868f70c15666 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Aug 13 14:36:18 minikube cri-dockerd[1373]: time="2024-08-13T14:36:18Z" level=error msg="error getting RW layer size for container ID 'eda8a0816e1125bfe524101615eb63b28fa70716a9ffca13b02c80a95b6f952e': Error response from daemon: No such container: eda8a0816e1125bfe524101615eb63b28fa70716a9ffca13b02c80a95b6f952e"
Aug 13 14:36:18 minikube cri-dockerd[1373]: time="2024-08-13T14:36:18Z" level=error msg="Set backoffDuration to : 1m0s for container ID 'eda8a0816e1125bfe524101615eb63b28fa70716a9ffca13b02c80a95b6f952e'"
Aug 13 14:36:27 minikube cri-dockerd[1373]: time="2024-08-13T14:36:27Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/02cb8d327c4b449d0b7b62a126c5778d411c669ea937c9e08e39f8938ba5c3fd/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Aug 13 14:36:32 minikube cri-dockerd[1373]: time="2024-08-13T14:36:32Z" level=info msg="Stop pulling image openzipkin/zipkin:latest: Status: Image is up to date for openzipkin/zipkin:latest"


==> container status <==
CONTAINER           IMAGE                                                                                       CREATED              STATE               NAME                        ATTEMPT             POD ID              POD
1866f5ccb4b7b       openzipkin/zipkin@sha256:a7c69b5d23676a3cf7538ad8d9125f77fb2c8b72fc72bbca21357a3e984ac1bc   33 seconds ago       Running             zipkin                      0                   02cb8d327c4b4       zipkin-7d56c74569-rl2ft
e93644c1e6849       e42cf20fe44e3                                                                               About a minute ago   Running             rabbitmq                    0                   5f107b1a94454       rabbitmq-58f4956f6f-xxd5l
0acfb28afaebf       f2ad9f23df82a                                                                               3 hours ago          Running             mysql-client                0                   2fb6854d75d00       mysql-client
97238dbbaa29e       07655ddf2eebe                                                                               4 days ago           Running             kubernetes-dashboard        5                   fcd74e7843baa       kubernetes-dashboard-779776cb65-4dwms
ab46cd44752d1       6e38f40d628db                                                                               4 days ago           Running             storage-provisioner         7                   29b9654c31cd3       storage-provisioner
89948afcbaccf       c7aad43836fa5                                                                               4 days ago           Running             kube-controller-manager     4                   66192c145e5ba       kube-controller-manager-minikube
524f0f05158c8       115053965e86b                                                                               4 days ago           Running             dashboard-metrics-scraper   3                   a89b60322d4b0       dashboard-metrics-scraper-b5fc48f67-h92rw
0e1ea864067cd       cbb01a7bd410d                                                                               4 days ago           Running             coredns                     3                   a87e59bfa6f5a       coredns-7db6d8ff4d-rk9sx
2c82cff341ff3       07655ddf2eebe                                                                               4 days ago           Exited              kubernetes-dashboard        4                   fcd74e7843baa       kubernetes-dashboard-779776cb65-4dwms
3abdf9fc2c898       a0bf559e280cf                                                                               4 days ago           Running             kube-proxy                  3                   7ae3b832e2d6b       kube-proxy-qs92z
32e3e37bf1434       6e38f40d628db                                                                               4 days ago           Exited              storage-provisioner         6                   29b9654c31cd3       storage-provisioner
c1f692b7c8757       3861cfcd7c04c                                                                               4 days ago           Running             etcd                        3                   d45cfa40cecd6       etcd-minikube
749fd11cf8fdd       c42f13656d0b2                                                                               4 days ago           Running             kube-apiserver              3                   f61f5ffd70433       kube-apiserver-minikube
ec30e56023abe       259c8277fcbbc                                                                               4 days ago           Running             kube-scheduler              3                   dc8d08cc102b4       kube-scheduler-minikube
141df2ef222cc       c7aad43836fa5                                                                               4 days ago           Exited              kube-controller-manager     3                   66192c145e5ba       kube-controller-manager-minikube
249be7192274e       cbb01a7bd410d                                                                               9 days ago           Exited              coredns                     2                   5c0e30a0fa76c       coredns-7db6d8ff4d-rk9sx
d6944da4dbd06       115053965e86b                                                                               9 days ago           Exited              dashboard-metrics-scraper   2                   ff01d36e06435       dashboard-metrics-scraper-b5fc48f67-h92rw
409a2c61a8653       a0bf559e280cf                                                                               9 days ago           Exited              kube-proxy                  2                   f78a97fa7dd47       kube-proxy-qs92z
17d5d12f4c227       3861cfcd7c04c                                                                               9 days ago           Exited              etcd                        2                   0d0cc972c4c07       etcd-minikube
b17e003fd5b11       c42f13656d0b2                                                                               9 days ago           Exited              kube-apiserver              2                   f9dc9903bb8f6       kube-apiserver-minikube
5f09a99bb52e9       259c8277fcbbc                                                                               9 days ago           Exited              kube-scheduler              2                   16bcd8c97b92d       kube-scheduler-minikube


==> coredns [0e1ea864067c] <==
[INFO] 10.244.0.106:55478 - 34416 "AAAA IN company_db_container.cluster.local. udp 52 false 512" NXDOMAIN qr,aa,rd 145 0.000340824s
[INFO] 10.244.0.106:55478 - 62847 "A IN company_db_container.cluster.local. udp 52 false 512" NXDOMAIN qr,aa,rd 145 0.000457032s
[INFO] 10.244.0.106:41121 - 37546 "AAAA IN company_db_container. udp 38 false 512" NXDOMAIN qr,rd,ra 38 1.8902101070000001s
[INFO] 10.244.0.106:41121 - 45782 "A IN company_db_container. udp 38 false 512" NXDOMAIN qr,rd,ra 38 1.890327815s
[INFO] 10.244.0.106:43091 - 30164 "A IN company_db_container.job-app.svc.cluster.local. udp 64 false 512" NXDOMAIN qr,aa,rd 157 0.073509747s
[INFO] 10.244.0.106:43091 - 11484 "AAAA IN company_db_container.job-app.svc.cluster.local. udp 64 false 512" NXDOMAIN qr,aa,rd 157 0.0741481s
[INFO] 10.244.0.106:35747 - 41340 "AAAA IN company_db_container.svc.cluster.local. udp 56 false 512" NXDOMAIN qr,aa,rd 149 0.000427435s
[INFO] 10.244.0.106:35747 - 64119 "A IN company_db_container.svc.cluster.local. udp 56 false 512" NXDOMAIN qr,aa,rd 149 0.000617951s
[INFO] 10.244.0.106:42231 - 31309 "A IN company_db_container.cluster.local. udp 52 false 512" NXDOMAIN qr,aa,rd 145 0.000234619s
[INFO] 10.244.0.106:42231 - 46929 "AAAA IN company_db_container.cluster.local. udp 52 false 512" NXDOMAIN qr,aa,rd 145 0.006026295s
[INFO] 10.244.0.106:48005 - 42783 "A IN company_db_container. udp 38 false 512" NXDOMAIN qr,rd,ra 38 1.8919592349999999s
[INFO] 10.244.0.106:48005 - 10011 "AAAA IN company_db_container. udp 38 false 512" NXDOMAIN qr,rd,ra 38 1.890747936s
[INFO] 10.244.0.119:46628 - 19544 "AAAA IN review_db_container.job-app.svc.cluster.local. udp 63 false 512" NXDOMAIN qr,aa,rd 156 0.011735866s
[INFO] 10.244.0.119:46628 - 65347 "A IN review_db_container.job-app.svc.cluster.local. udp 63 false 512" NXDOMAIN qr,aa,rd 156 0.011842874s
[INFO] 10.244.0.119:44278 - 10977 "AAAA IN review_db_container.svc.cluster.local. udp 55 false 512" NXDOMAIN qr,aa,rd 148 0.078800515s
[INFO] 10.244.0.119:44278 - 38374 "A IN review_db_container.svc.cluster.local. udp 55 false 512" NXDOMAIN qr,aa,rd 148 0.00026592s
[INFO] 10.244.0.119:45918 - 10636 "AAAA IN review_db_container.cluster.local. udp 51 false 512" NXDOMAIN qr,aa,rd 144 0.00027522s
[INFO] 10.244.0.119:45918 - 5767 "A IN review_db_container.cluster.local. udp 51 false 512" NXDOMAIN qr,aa,rd 144 0.000198614s
[INFO] 10.244.0.119:39617 - 27707 "A IN review_db_container. udp 37 false 512" NXDOMAIN qr,rd,ra 37 1.899089346s
[INFO] 10.244.0.119:39617 - 28735 "AAAA IN review_db_container. udp 37 false 512" NXDOMAIN qr,rd,ra 37 1.900155524s
[INFO] 10.244.0.121:34477 - 15837 "A IN company_db_container.job-app.svc.cluster.local. udp 64 false 512" NXDOMAIN qr,aa,rd 157 0.002139121s
[INFO] 10.244.0.121:34477 - 48078 "AAAA IN company_db_container.job-app.svc.cluster.local. udp 64 false 512" NXDOMAIN qr,aa,rd 157 0.000778872s
[INFO] 10.244.0.121:60110 - 10142 "AAAA IN company_db_container.svc.cluster.local. udp 56 false 512" NXDOMAIN qr,aa,rd 149 0.000502681s
[INFO] 10.244.0.121:60110 - 37009 "A IN company_db_container.svc.cluster.local. udp 56 false 512" NXDOMAIN qr,aa,rd 149 0.000627977s
[INFO] 10.244.0.121:48181 - 32210 "AAAA IN company_db_container.cluster.local. udp 52 false 512" NXDOMAIN qr,aa,rd 145 0.000758672s
[INFO] 10.244.0.121:48181 - 57061 "A IN company_db_container.cluster.local. udp 52 false 512" NXDOMAIN qr,aa,rd 145 0.00243691s
[INFO] 10.244.0.121:47853 - 18909 "AAAA IN company_db_container. udp 38 false 512" NXDOMAIN qr,rd,ra 38 1.8851893020000001s
[INFO] 10.244.0.121:47853 - 11223 "A IN company_db_container. udp 38 false 512" NXDOMAIN qr,rd,ra 38 1.9020646129999998s
[INFO] 10.244.0.119:46172 - 53682 "A IN review_db_container.job-app.svc.cluster.local. udp 63 false 512" NXDOMAIN qr,aa,rd 156 0.000625443s
[INFO] 10.244.0.119:46172 - 58754 "AAAA IN review_db_container.job-app.svc.cluster.local. udp 63 false 512" NXDOMAIN qr,aa,rd 156 0.000311721s
[INFO] 10.244.0.119:48924 - 50692 "A IN review_db_container.svc.cluster.local. udp 55 false 512" NXDOMAIN qr,aa,rd 148 0.000537937s
[INFO] 10.244.0.119:48924 - 520 "AAAA IN review_db_container.svc.cluster.local. udp 55 false 512" NXDOMAIN qr,aa,rd 148 0.000428829s
[INFO] 10.244.0.119:47511 - 6056 "AAAA IN review_db_container.cluster.local. udp 51 false 512" NXDOMAIN qr,aa,rd 144 0.000394827s
[INFO] 10.244.0.119:47511 - 64408 "A IN review_db_container.cluster.local. udp 51 false 512" NXDOMAIN qr,aa,rd 144 0.000222515s
[INFO] 10.244.0.119:55475 - 33540 "AAAA IN review_db_container. udp 37 false 512" NXDOMAIN qr,rd,ra 37 1.915908763s
[INFO] 10.244.0.119:55475 - 48649 "A IN review_db_container. udp 37 false 512" NXDOMAIN qr,rd,ra 37 3.893944655s
[INFO] 10.244.0.120:36510 - 55685 "AAAA IN job_db_container.job-app.svc.cluster.local. udp 60 false 512" NXDOMAIN qr,aa,rd 153 0.001136677s
[INFO] 10.244.0.120:36510 - 24475 "A IN job_db_container.job-app.svc.cluster.local. udp 60 false 512" NXDOMAIN qr,aa,rd 153 0.001159579s
[INFO] 10.244.0.120:45870 - 25715 "AAAA IN job_db_container.svc.cluster.local. udp 52 false 512" NXDOMAIN qr,aa,rd 145 0.00029412s
[INFO] 10.244.0.120:45870 - 54646 "A IN job_db_container.svc.cluster.local. udp 52 false 512" NXDOMAIN qr,aa,rd 145 0.000414928s
[INFO] 10.244.0.120:41797 - 52082 "AAAA IN job_db_container.cluster.local. udp 48 false 512" NXDOMAIN qr,aa,rd 141 0.000394627s
[INFO] 10.244.0.120:41797 - 623 "A IN job_db_container.cluster.local. udp 48 false 512" NXDOMAIN qr,aa,rd 141 0.000265018s
[INFO] 10.244.0.120:41831 - 23058 "A IN job_db_container. udp 34 false 512" NXDOMAIN qr,rd,ra 34 1.898593584s
[INFO] 10.244.0.120:41831 - 33567 "AAAA IN job_db_container. udp 34 false 512" NXDOMAIN qr,rd,ra 34 1.899051915s
[INFO] 10.244.0.121:44665 - 42317 "AAAA IN company_db_container.job-app.svc.cluster.local. udp 64 false 512" NXDOMAIN qr,aa,rd 157 0.001711213s
[INFO] 10.244.0.121:44665 - 59006 "A IN company_db_container.job-app.svc.cluster.local. udp 64 false 512" NXDOMAIN qr,aa,rd 157 0.008754075s
[INFO] 10.244.0.121:41308 - 26427 "AAAA IN company_db_container.svc.cluster.local. udp 56 false 512" NXDOMAIN qr,aa,rd 149 0.000959963s
[INFO] 10.244.0.121:41308 - 32586 "A IN company_db_container.svc.cluster.local. udp 56 false 512" NXDOMAIN qr,aa,rd 149 0.000674844s
[INFO] 10.244.0.121:42122 - 62367 "AAAA IN company_db_container.cluster.local. udp 52 false 512" NXDOMAIN qr,aa,rd 145 0.000738849s
[INFO] 10.244.0.121:42122 - 40622 "A IN company_db_container.cluster.local. udp 52 false 512" NXDOMAIN qr,aa,rd 145 0.001048369s
[INFO] 10.244.0.121:44609 - 54269 "A IN company_db_container. udp 38 false 512" NXDOMAIN qr,rd,ra 38 1.88357709s
[INFO] 10.244.0.121:44609 - 2017 "AAAA IN company_db_container. udp 38 false 512" NXDOMAIN qr,rd,ra 38 1.9803168169999998s
[INFO] 10.244.0.120:35570 - 62774 "AAAA IN job_db_container.job-app.svc.cluster.local. udp 60 false 512" NXDOMAIN qr,aa,rd 153 0.000458832s
[INFO] 10.244.0.120:35570 - 28233 "A IN job_db_container.job-app.svc.cluster.local. udp 60 false 512" NXDOMAIN qr,aa,rd 153 0.102403333s
[INFO] 10.244.0.120:48267 - 46945 "A IN job_db_container.svc.cluster.local. udp 52 false 512" NXDOMAIN qr,aa,rd 145 0.001324191s
[INFO] 10.244.0.120:48267 - 19290 "AAAA IN job_db_container.svc.cluster.local. udp 52 false 512" NXDOMAIN qr,aa,rd 145 0.000756352s
[INFO] 10.244.0.120:59354 - 16878 "A IN job_db_container.cluster.local. udp 48 false 512" NXDOMAIN qr,aa,rd 141 0.000536237s
[INFO] 10.244.0.120:59354 - 33509 "AAAA IN job_db_container.cluster.local. udp 48 false 512" NXDOMAIN qr,aa,rd 141 0.000861359s
[INFO] 10.244.0.120:54290 - 48627 "AAAA IN job_db_container. udp 34 false 512" NXDOMAIN qr,rd,ra 34 1.895109334s
[INFO] 10.244.0.120:54290 - 45306 "A IN job_db_container. udp 34 false 512" NXDOMAIN qr,rd,ra 34 1.8953339489999999s


==> coredns [249be7192274] <==
.:53
[INFO] plugin/reload: Running configuration SHA512 = f869070685748660180df1b7a47d58cdafcf2f368266578c062d1151dc2c900964aecc5975e8882e6de6fdfb6460463e30ebfaad2ec8f0c3c6436f80225b3b5b
CoreDNS-1.11.1
linux/amd64, go1.20.7, ae2bbc2
[INFO] 127.0.0.1:39672 - 8229 "HINFO IN 6115913878834716355.7628130389254712937. udp 57 false 512" NXDOMAIN qr,rd,ra 57 0.088568788s


==> describe nodes <==
Name:               minikube
Roles:              control-plane
Labels:             beta.kubernetes.io/arch=amd64
                    beta.kubernetes.io/os=linux
                    kubernetes.io/arch=amd64
                    kubernetes.io/hostname=minikube
                    kubernetes.io/os=linux
                    minikube.k8s.io/commit=5883c09216182566a63dff4c326a6fc9ed2982ff
                    minikube.k8s.io/name=minikube
                    minikube.k8s.io/primary=true
                    minikube.k8s.io/updated_at=2024_08_02T15_19_47_0700
                    minikube.k8s.io/version=v1.33.1
                    node-role.kubernetes.io/control-plane=
                    node.kubernetes.io/exclude-from-external-load-balancers=
Annotations:        kubeadm.alpha.kubernetes.io/cri-socket: unix:///var/run/cri-dockerd.sock
                    node.alpha.kubernetes.io/ttl: 0
                    volumes.kubernetes.io/controller-managed-attach-detach: true
CreationTimestamp:  Fri, 02 Aug 2024 08:19:38 +0000
Taints:             <none>
Unschedulable:      false
Lease:
  HolderIdentity:  minikube
  AcquireTime:     <unset>
  RenewTime:       Tue, 13 Aug 2024 14:37:00 +0000
Conditions:
  Type             Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message
  ----             ------  -----------------                 ------------------                ------                       -------
  MemoryPressure   False   Tue, 13 Aug 2024 14:35:00 +0000   Fri, 02 Aug 2024 08:19:32 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available
  DiskPressure     False   Tue, 13 Aug 2024 14:35:00 +0000   Fri, 02 Aug 2024 08:19:32 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure
  PIDPressure      False   Tue, 13 Aug 2024 14:35:00 +0000   Fri, 02 Aug 2024 08:19:32 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available
  Ready            True    Tue, 13 Aug 2024 14:35:00 +0000   Fri, 02 Aug 2024 08:19:48 +0000   KubeletReady                 kubelet is posting ready status
Addresses:
  InternalIP:  192.168.49.2
  Hostname:    minikube
Capacity:
  cpu:                8
  ephemeral-storage:  1055762868Ki
  hugepages-1Gi:      0
  hugepages-2Mi:      0
  memory:             7694408Ki
  pods:               110
Allocatable:
  cpu:                8
  ephemeral-storage:  1055762868Ki
  hugepages-1Gi:      0
  hugepages-2Mi:      0
  memory:             7694408Ki
  pods:               110
System Info:
  Machine ID:                 795a33a1eb73447a985733ef38850dc3
  System UUID:                795a33a1eb73447a985733ef38850dc3
  Boot ID:                    ad7fcbe3-cecd-427f-a365-68260f5f4303
  Kernel Version:             5.15.146.1-microsoft-standard-WSL2
  OS Image:                   Ubuntu 22.04.4 LTS
  Operating System:           linux
  Architecture:               amd64
  Container Runtime Version:  docker://26.1.1
  Kubelet Version:            v1.30.0
  Kube-Proxy Version:         v1.30.0
PodCIDR:                      10.244.0.0/24
PodCIDRs:                     10.244.0.0/24
Non-terminated Pods:          (12 in total)
  Namespace                   Name                                         CPU Requests  CPU Limits  Memory Requests  Memory Limits  Age
  ---------                   ----                                         ------------  ----------  ---------------  -------------  ---
  default                     mysql-client                                 0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         3h2m
  default                     rabbitmq-58f4956f6f-xxd5l                    0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         111s
  default                     zipkin-7d56c74569-rl2ft                      0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         43s
  kube-system                 coredns-7db6d8ff4d-rk9sx                     100m (1%!)(MISSING)     0 (0%!)(MISSING)      70Mi (0%!)(MISSING)        170Mi (2%!)(MISSING)     11d
  kube-system                 etcd-minikube                                100m (1%!)(MISSING)     0 (0%!)(MISSING)      100Mi (1%!)(MISSING)       0 (0%!)(MISSING)         11d
  kube-system                 kube-apiserver-minikube                      250m (3%!)(MISSING)     0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         11d
  kube-system                 kube-controller-manager-minikube             200m (2%!)(MISSING)     0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         11d
  kube-system                 kube-proxy-qs92z                             0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         11d
  kube-system                 kube-scheduler-minikube                      100m (1%!)(MISSING)     0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         11d
  kube-system                 storage-provisioner                          0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         11d
  kubernetes-dashboard        dashboard-metrics-scraper-b5fc48f67-h92rw    0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         11d
  kubernetes-dashboard        kubernetes-dashboard-779776cb65-4dwms        0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         11d
Allocated resources:
  (Total limits may be over 100 percent, i.e., overcommitted.)
  Resource           Requests    Limits
  --------           --------    ------
  cpu                750m (9%!)(MISSING)   0 (0%!)(MISSING)
  memory             170Mi (2%!)(MISSING)  170Mi (2%!)(MISSING)
  ephemeral-storage  0 (0%!)(MISSING)      0 (0%!)(MISSING)
  hugepages-1Gi      0 (0%!)(MISSING)      0 (0%!)(MISSING)
  hugepages-2Mi      0 (0%!)(MISSING)      0 (0%!)(MISSING)
Events:              <none>


==> dmesg <==
[Aug12 11:45] hrtimer: interrupt took 184521 ns


==> etcd [17d5d12f4c22] <==
{"level":"info","ts":"2024-08-05T09:35:01.416844Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":4255293745,"revision":56223,"compact-revision":55979}
{"level":"info","ts":"2024-08-05T09:40:01.458254Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":56469}
{"level":"info","ts":"2024-08-05T09:40:01.47016Z","caller":"mvcc/kvstore_compaction.go:68","msg":"finished scheduled compaction","compact-revision":56469,"took":"11.419082ms","hash":3094732873,"current-db-size-bytes":3149824,"current-db-size":"3.1 MB","current-db-size-in-use-bytes":1601536,"current-db-size-in-use":"1.6 MB"}
{"level":"info","ts":"2024-08-05T09:40:01.470288Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":3094732873,"revision":56469,"compact-revision":56223}
{"level":"info","ts":"2024-08-05T09:44:40.330482Z","caller":"traceutil/trace.go:171","msg":"trace[173287108] transaction","detail":"{read_only:false; response_revision:56943; number_of_response:1; }","duration":"109.565037ms","start":"2024-08-05T09:44:40.220893Z","end":"2024-08-05T09:44:40.330458Z","steps":["trace[173287108] 'process raft request'  (duration: 109.389584ms)"],"step_count":1}
{"level":"info","ts":"2024-08-05T09:45:01.452393Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":56712}
{"level":"info","ts":"2024-08-05T09:45:01.462848Z","caller":"mvcc/kvstore_compaction.go:68","msg":"finished scheduled compaction","compact-revision":56712,"took":"9.74979ms","hash":1083847091,"current-db-size-bytes":3149824,"current-db-size":"3.1 MB","current-db-size-in-use-bytes":1568768,"current-db-size-in-use":"1.6 MB"}
{"level":"info","ts":"2024-08-05T09:45:01.463316Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":1083847091,"revision":56712,"compact-revision":56469}
{"level":"info","ts":"2024-08-05T09:46:22.046347Z","caller":"wal/wal.go:785","msg":"created a new WAL segment","path":"/var/lib/minikube/etcd/member/wal/0000000000000001-0000000000011696.wal"}
{"level":"info","ts":"2024-08-05T09:50:01.441691Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":56960}
{"level":"info","ts":"2024-08-05T09:50:01.452284Z","caller":"mvcc/kvstore_compaction.go:68","msg":"finished scheduled compaction","compact-revision":56960,"took":"9.854476ms","hash":2647021374,"current-db-size-bytes":3149824,"current-db-size":"3.1 MB","current-db-size-in-use-bytes":1585152,"current-db-size-in-use":"1.6 MB"}
{"level":"info","ts":"2024-08-05T09:50:01.452513Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":2647021374,"revision":56960,"compact-revision":56712}
{"level":"info","ts":"2024-08-05T09:52:08.401536Z","caller":"traceutil/trace.go:171","msg":"trace[820215807] transaction","detail":"{read_only:false; response_revision:57314; number_of_response:1; }","duration":"158.834372ms","start":"2024-08-05T09:52:08.242646Z","end":"2024-08-05T09:52:08.40148Z","steps":["trace[820215807] 'process raft request'  (duration: 86.463804ms)"],"step_count":1}
{"level":"info","ts":"2024-08-05T09:54:43.128914Z","caller":"traceutil/trace.go:171","msg":"trace[1916894636] transaction","detail":"{read_only:false; response_revision:57436; number_of_response:1; }","duration":"119.965215ms","start":"2024-08-05T09:54:43.008895Z","end":"2024-08-05T09:54:43.12886Z","steps":["trace[1916894636] 'process raft request'  (duration: 119.676781ms)"],"step_count":1}
{"level":"info","ts":"2024-08-05T09:55:01.464217Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":57205}
{"level":"info","ts":"2024-08-05T09:55:01.477353Z","caller":"mvcc/kvstore_compaction.go:68","msg":"finished scheduled compaction","compact-revision":57205,"took":"12.469299ms","hash":2216808652,"current-db-size-bytes":3149824,"current-db-size":"3.1 MB","current-db-size-in-use-bytes":1581056,"current-db-size-in-use":"1.6 MB"}
{"level":"info","ts":"2024-08-05T09:55:01.477576Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":2216808652,"revision":57205,"compact-revision":56960}
{"level":"info","ts":"2024-08-05T10:00:01.444028Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":57449}
{"level":"info","ts":"2024-08-05T10:00:01.450532Z","caller":"mvcc/kvstore_compaction.go:68","msg":"finished scheduled compaction","compact-revision":57449,"took":"5.529892ms","hash":2735711249,"current-db-size-bytes":3149824,"current-db-size":"3.1 MB","current-db-size-in-use-bytes":1552384,"current-db-size-in-use":"1.6 MB"}
{"level":"info","ts":"2024-08-05T10:00:01.450658Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":2735711249,"revision":57449,"compact-revision":57205}
{"level":"info","ts":"2024-08-05T10:01:44.162556Z","caller":"traceutil/trace.go:171","msg":"trace[1485034838] transaction","detail":"{read_only:false; response_revision:57784; number_of_response:1; }","duration":"102.39564ms","start":"2024-08-05T10:01:44.06013Z","end":"2024-08-05T10:01:44.162525Z","steps":["trace[1485034838] 'process raft request'  (duration: 102.21392ms)"],"step_count":1}
{"level":"info","ts":"2024-08-05T10:02:15.137571Z","caller":"traceutil/trace.go:171","msg":"trace[1227617266] transaction","detail":"{read_only:false; response_revision:57811; number_of_response:1; }","duration":"109.500802ms","start":"2024-08-05T10:02:15.028008Z","end":"2024-08-05T10:02:15.137509Z","steps":["trace[1227617266] 'process raft request'  (duration: 109.083153ms)"],"step_count":1}
{"level":"info","ts":"2024-08-05T10:05:01.469503Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":57697}
{"level":"info","ts":"2024-08-05T10:05:01.47974Z","caller":"mvcc/kvstore_compaction.go:68","msg":"finished scheduled compaction","compact-revision":57697,"took":"9.516325ms","hash":2892130979,"current-db-size-bytes":3149824,"current-db-size":"3.1 MB","current-db-size-in-use-bytes":1556480,"current-db-size-in-use":"1.6 MB"}
{"level":"info","ts":"2024-08-05T10:05:01.479944Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":2892130979,"revision":57697,"compact-revision":57449}
{"level":"info","ts":"2024-08-05T10:05:57.13583Z","caller":"traceutil/trace.go:171","msg":"trace[769712988] transaction","detail":"{read_only:false; response_revision:57988; number_of_response:1; }","duration":"107.170494ms","start":"2024-08-05T10:05:57.02863Z","end":"2024-08-05T10:05:57.135801Z","steps":["trace[769712988] 'process raft request'  (duration: 106.947471ms)"],"step_count":1}
{"level":"info","ts":"2024-08-05T10:10:01.493954Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":57941}
{"level":"info","ts":"2024-08-05T10:10:01.503543Z","caller":"mvcc/kvstore_compaction.go:68","msg":"finished scheduled compaction","compact-revision":57941,"took":"8.846244ms","hash":1965579048,"current-db-size-bytes":3149824,"current-db-size":"3.1 MB","current-db-size-in-use-bytes":1589248,"current-db-size-in-use":"1.6 MB"}
{"level":"info","ts":"2024-08-05T10:10:01.503771Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":1965579048,"revision":57941,"compact-revision":57697}
{"level":"info","ts":"2024-08-05T10:15:01.475462Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":58187}
{"level":"info","ts":"2024-08-05T10:15:01.479124Z","caller":"mvcc/kvstore_compaction.go:68","msg":"finished scheduled compaction","compact-revision":58187,"took":"3.450809ms","hash":3901172253,"current-db-size-bytes":3149824,"current-db-size":"3.1 MB","current-db-size-in-use-bytes":1597440,"current-db-size-in-use":"1.6 MB"}
{"level":"info","ts":"2024-08-05T10:15:01.4792Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":3901172253,"revision":58187,"compact-revision":57941}
{"level":"info","ts":"2024-08-05T10:17:42.199166Z","caller":"traceutil/trace.go:171","msg":"trace[827192561] linearizableReadLoop","detail":"{readStateIndex:73249; appliedIndex:73248; }","duration":"126.34699ms","start":"2024-08-05T10:17:42.07271Z","end":"2024-08-05T10:17:42.199057Z","steps":["trace[827192561] 'read index received'  (duration: 122.568663ms)","trace[827192561] 'applied index is now lower than readState.Index'  (duration: 3.775827ms)"],"step_count":2}
{"level":"warn","ts":"2024-08-05T10:17:42.199944Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"126.580617ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/health\" ","response":"range_response_count:0 size:6"}
{"level":"info","ts":"2024-08-05T10:17:42.200195Z","caller":"traceutil/trace.go:171","msg":"trace[577119258] range","detail":"{range_begin:/registry/health; range_end:; response_count:0; response_revision:58570; }","duration":"127.49832ms","start":"2024-08-05T10:17:42.072637Z","end":"2024-08-05T10:17:42.200136Z","steps":["trace[577119258] 'agreement among raft nodes before linearized reading'  (duration: 126.573416ms)"],"step_count":1}
{"level":"info","ts":"2024-08-05T10:20:01.473031Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":58435}
{"level":"info","ts":"2024-08-05T10:20:01.477587Z","caller":"mvcc/kvstore_compaction.go:68","msg":"finished scheduled compaction","compact-revision":58435,"took":"4.224884ms","hash":865138588,"current-db-size-bytes":3149824,"current-db-size":"3.1 MB","current-db-size-in-use-bytes":1585152,"current-db-size-in-use":"1.6 MB"}
{"level":"info","ts":"2024-08-05T10:20:01.477631Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":865138588,"revision":58435,"compact-revision":58187}
{"level":"info","ts":"2024-08-05T10:22:35.384863Z","caller":"traceutil/trace.go:171","msg":"trace[1193585200] transaction","detail":"{read_only:false; response_revision:58812; number_of_response:1; }","duration":"118.813102ms","start":"2024-08-05T10:22:35.266022Z","end":"2024-08-05T10:22:35.384835Z","steps":["trace[1193585200] 'process raft request'  (duration: 118.633691ms)"],"step_count":1}
{"level":"warn","ts":"2024-08-05T10:23:48.30302Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"258.585701ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/health\" ","response":"range_response_count:0 size:6"}
{"level":"warn","ts":"2024-08-05T10:23:48.303465Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"201.603421ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/limitranges/\" range_end:\"/registry/limitranges0\" count_only:true ","response":"range_response_count:0 size:6"}
{"level":"info","ts":"2024-08-05T10:23:48.303602Z","caller":"traceutil/trace.go:171","msg":"trace[668474982] range","detail":"{range_begin:/registry/health; range_end:; response_count:0; response_revision:58872; }","duration":"258.798715ms","start":"2024-08-05T10:23:48.044362Z","end":"2024-08-05T10:23:48.303161Z","steps":["trace[668474982] 'range keys from in-memory index tree'  (duration: 258.451692ms)"],"step_count":1}
{"level":"info","ts":"2024-08-05T10:23:48.303767Z","caller":"traceutil/trace.go:171","msg":"trace[1109464798] range","detail":"{range_begin:/registry/limitranges/; range_end:/registry/limitranges0; response_count:0; response_revision:58872; }","duration":"202.175558ms","start":"2024-08-05T10:23:48.101409Z","end":"2024-08-05T10:23:48.303585Z","steps":["trace[1109464798] 'count revisions from in-memory index tree'  (duration: 201.522915ms)"],"step_count":1}
{"level":"info","ts":"2024-08-05T10:25:01.470938Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":58682}
{"level":"info","ts":"2024-08-05T10:25:01.474145Z","caller":"mvcc/kvstore_compaction.go:68","msg":"finished scheduled compaction","compact-revision":58682,"took":"2.946368ms","hash":3024166076,"current-db-size-bytes":3149824,"current-db-size":"3.1 MB","current-db-size-in-use-bytes":1605632,"current-db-size-in-use":"1.6 MB"}
{"level":"info","ts":"2024-08-05T10:25:01.474212Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":3024166076,"revision":58682,"compact-revision":58435}
{"level":"info","ts":"2024-08-05T10:30:01.472854Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":58930}
{"level":"info","ts":"2024-08-05T10:30:01.476515Z","caller":"mvcc/kvstore_compaction.go:68","msg":"finished scheduled compaction","compact-revision":58930,"took":"3.433998ms","hash":3609223314,"current-db-size-bytes":3149824,"current-db-size":"3.1 MB","current-db-size-in-use-bytes":1613824,"current-db-size-in-use":"1.6 MB"}
{"level":"info","ts":"2024-08-05T10:30:01.476644Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":3609223314,"revision":58930,"compact-revision":58682}
{"level":"info","ts":"2024-08-05T10:35:01.465022Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":59180}
{"level":"info","ts":"2024-08-05T10:35:01.468312Z","caller":"mvcc/kvstore_compaction.go:68","msg":"finished scheduled compaction","compact-revision":59180,"took":"3.095439ms","hash":559807664,"current-db-size-bytes":3149824,"current-db-size":"3.1 MB","current-db-size-in-use-bytes":1622016,"current-db-size-in-use":"1.6 MB"}
{"level":"info","ts":"2024-08-05T10:35:01.468368Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":559807664,"revision":59180,"compact-revision":58930}
{"level":"warn","ts":"2024-08-05T10:38:22.337496Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"177.523218ms","expected-duration":"100ms","prefix":"","request":"header:<ID:8128030973982496645 > lease_revoke:<id:70cc911b4f99573d>","response":"size:30"}
{"level":"info","ts":"2024-08-05T10:40:01.474289Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":59429}
{"level":"info","ts":"2024-08-05T10:40:01.478267Z","caller":"mvcc/kvstore_compaction.go:68","msg":"finished scheduled compaction","compact-revision":59429,"took":"3.391789ms","hash":4122330536,"current-db-size-bytes":3149824,"current-db-size":"3.1 MB","current-db-size-in-use-bytes":1626112,"current-db-size-in-use":"1.6 MB"}
{"level":"info","ts":"2024-08-05T10:40:01.478335Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":4122330536,"revision":59429,"compact-revision":59180}
{"level":"info","ts":"2024-08-05T10:40:14.278418Z","caller":"traceutil/trace.go:171","msg":"trace[1066535665] transaction","detail":"{read_only:false; response_revision:59690; number_of_response:1; }","duration":"219.867411ms","start":"2024-08-05T10:40:14.058521Z","end":"2024-08-05T10:40:14.278388Z","steps":["trace[1066535665] 'process raft request'  (duration: 219.629505ms)"],"step_count":1}
{"level":"info","ts":"2024-08-05T10:45:01.487849Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":59678}
{"level":"info","ts":"2024-08-05T10:45:01.494871Z","caller":"mvcc/kvstore_compaction.go:68","msg":"finished scheduled compaction","compact-revision":59678,"took":"6.605666ms","hash":3907288032,"current-db-size-bytes":3149824,"current-db-size":"3.1 MB","current-db-size-in-use-bytes":1585152,"current-db-size-in-use":"1.6 MB"}
{"level":"info","ts":"2024-08-05T10:45:01.494995Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":3907288032,"revision":59678,"compact-revision":59429}


==> etcd [c1f692b7c875] <==
{"level":"warn","ts":"2024-08-13T14:33:07.561267Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2024-08-13T14:33:07.169702Z","time spent":"391.472394ms","remote":"127.0.0.1:51062","response type":"/etcdserverpb.KV/Txn","request count":1,"request size":811,"response count":0,"response size":42,"request content":"compare:<target:MOD key:\"/registry/events/job-app/company-service-deployment-855b66f9cc-rklsv.17eb505008ae404e\" mod_revision:0 > success:<request_put:<key:\"/registry/events/job-app/company-service-deployment-855b66f9cc-rklsv.17eb505008ae404e\" value_size:708 lease:8128031094857264636 >> failure:<>"}
{"level":"warn","ts":"2024-08-13T14:33:07.774657Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"108.938728ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/health\" ","response":"range_response_count:0 size:6"}
{"level":"info","ts":"2024-08-13T14:33:07.774815Z","caller":"traceutil/trace.go:171","msg":"trace[1270086300] range","detail":"{range_begin:/registry/health; range_end:; response_count:0; response_revision:151849; }","duration":"109.353457ms","start":"2024-08-13T14:33:07.665423Z","end":"2024-08-13T14:33:07.774776Z","steps":["trace[1270086300] 'range keys from in-memory index tree'  (duration: 108.746716ms)"],"step_count":1}
{"level":"info","ts":"2024-08-13T14:33:08.061224Z","caller":"traceutil/trace.go:171","msg":"trace[1047602607] transaction","detail":"{read_only:false; response_revision:151851; number_of_response:1; }","duration":"185.51465ms","start":"2024-08-13T14:33:07.875637Z","end":"2024-08-13T14:33:08.061152Z","steps":["trace[1047602607] 'process raft request'  (duration: 88.44093ms)","trace[1047602607] 'compare'  (duration: 96.394674ms)"],"step_count":2}
{"level":"info","ts":"2024-08-13T14:33:08.760009Z","caller":"traceutil/trace.go:171","msg":"trace[199253480] transaction","detail":"{read_only:false; response_revision:151852; number_of_response:1; }","duration":"194.956594ms","start":"2024-08-13T14:33:08.565024Z","end":"2024-08-13T14:33:08.75998Z","steps":["trace[199253480] 'process raft request'  (duration: 194.776982ms)"],"step_count":1}
{"level":"info","ts":"2024-08-13T14:33:08.882279Z","caller":"traceutil/trace.go:171","msg":"trace[608801212] transaction","detail":"{read_only:false; number_of_response:1; response_revision:151852; }","duration":"118.097553ms","start":"2024-08-13T14:33:08.764145Z","end":"2024-08-13T14:33:08.882243Z","steps":["trace[608801212] 'process raft request'  (duration: 95.582118ms)","trace[608801212] 'compare'  (duration: 22.420329ms)"],"step_count":2}
{"level":"warn","ts":"2024-08-13T14:33:09.373105Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"302.518728ms","expected-duration":"100ms","prefix":"","request":"header:<ID:8128031094857265048 > lease_revoke:<id:70cc9137744bef01>","response":"size:30"}
{"level":"info","ts":"2024-08-13T14:34:57.87259Z","caller":"traceutil/trace.go:171","msg":"trace[405936897] transaction","detail":"{read_only:false; response_revision:152010; number_of_response:1; }","duration":"109.849129ms","start":"2024-08-13T14:34:57.762679Z","end":"2024-08-13T14:34:57.872528Z","steps":["trace[405936897] 'process raft request'  (duration: 109.398501ms)"],"step_count":1}
{"level":"warn","ts":"2024-08-13T14:35:18.865351Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"100.842993ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/serviceaccounts/default/default\" ","response":"range_response_count:1 size:172"}
{"level":"info","ts":"2024-08-13T14:35:18.865631Z","caller":"traceutil/trace.go:171","msg":"trace[1872019104] range","detail":"{range_begin:/registry/serviceaccounts/default/default; range_end:; response_count:1; response_revision:152067; }","duration":"101.237018ms","start":"2024-08-13T14:35:18.764348Z","end":"2024-08-13T14:35:18.865585Z","steps":["trace[1872019104] 'range keys from in-memory index tree'  (duration: 92.10824ms)"],"step_count":1}
{"level":"info","ts":"2024-08-13T14:35:35.751358Z","caller":"traceutil/trace.go:171","msg":"trace[843163826] transaction","detail":"{read_only:false; response_revision:152095; number_of_response:1; }","duration":"197.364496ms","start":"2024-08-13T14:35:35.553934Z","end":"2024-08-13T14:35:35.751299Z","steps":["trace[843163826] 'process raft request'  (duration: 196.892869ms)"],"step_count":1}
{"level":"info","ts":"2024-08-13T14:35:37.966044Z","caller":"traceutil/trace.go:171","msg":"trace[457280608] transaction","detail":"{read_only:false; response_revision:152096; number_of_response:1; }","duration":"108.085386ms","start":"2024-08-13T14:35:37.857895Z","end":"2024-08-13T14:35:37.965981Z","steps":["trace[457280608] 'process raft request'  (duration: 107.492352ms)"],"step_count":1}
{"level":"info","ts":"2024-08-13T14:35:42.348975Z","caller":"traceutil/trace.go:171","msg":"trace[154052645] transaction","detail":"{read_only:false; response_revision:152099; number_of_response:1; }","duration":"269.153928ms","start":"2024-08-13T14:35:42.079748Z","end":"2024-08-13T14:35:42.348902Z","steps":["trace[154052645] 'process raft request'  (duration: 268.693742ms)"],"step_count":1}
{"level":"info","ts":"2024-08-13T14:35:45.14858Z","caller":"traceutil/trace.go:171","msg":"trace[1557381885] transaction","detail":"{read_only:false; response_revision:152102; number_of_response:1; }","duration":"192.935586ms","start":"2024-08-13T14:35:44.955583Z","end":"2024-08-13T14:35:45.148519Z","steps":["trace[1557381885] 'process raft request'  (duration: 192.633695ms)"],"step_count":1}
{"level":"info","ts":"2024-08-13T14:35:46.749326Z","caller":"traceutil/trace.go:171","msg":"trace[936460551] transaction","detail":"{read_only:false; response_revision:152103; number_of_response:1; }","duration":"100.20948ms","start":"2024-08-13T14:35:46.649058Z","end":"2024-08-13T14:35:46.749267Z","steps":["trace[936460551] 'process raft request'  (duration: 99.803592ms)"],"step_count":1}
{"level":"info","ts":"2024-08-13T14:35:59.268847Z","caller":"traceutil/trace.go:171","msg":"trace[1995479693] linearizableReadLoop","detail":"{readStateIndex:189584; appliedIndex:189581; }","duration":"105.185909ms","start":"2024-08-13T14:35:59.163605Z","end":"2024-08-13T14:35:59.268791Z","steps":["trace[1995479693] 'read index received'  (duration: 2.954622ms)","trace[1995479693] 'applied index is now lower than readState.Index'  (duration: 102.228487ms)"],"step_count":2}
{"level":"info","ts":"2024-08-13T14:35:59.269326Z","caller":"traceutil/trace.go:171","msg":"trace[526159861] transaction","detail":"{read_only:false; response_revision:152120; number_of_response:1; }","duration":"110.589415ms","start":"2024-08-13T14:35:59.158684Z","end":"2024-08-13T14:35:59.269273Z","steps":["trace[526159861] 'process raft request'  (duration: 90.04247ms)","trace[526159861] 'compare'  (duration: 19.363756ms)"],"step_count":2}
{"level":"info","ts":"2024-08-13T14:35:59.270082Z","caller":"traceutil/trace.go:171","msg":"trace[533271534] transaction","detail":"{read_only:false; number_of_response:1; response_revision:152122; }","duration":"106.692122ms","start":"2024-08-13T14:35:59.163348Z","end":"2024-08-13T14:35:59.27004Z","steps":["trace[533271534] 'process raft request'  (duration: 105.276815ms)"],"step_count":1}
{"level":"info","ts":"2024-08-13T14:35:59.270273Z","caller":"traceutil/trace.go:171","msg":"trace[222744172] transaction","detail":"{read_only:false; response_revision:152121; number_of_response:1; }","duration":"110.192985ms","start":"2024-08-13T14:35:59.15983Z","end":"2024-08-13T14:35:59.270023Z","steps":["trace[222744172] 'process raft request'  (duration: 108.574463ms)"],"step_count":1}
{"level":"warn","ts":"2024-08-13T14:35:59.365261Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"201.590156ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/services/endpoints/job-app/zipkin\" ","response":"range_response_count:0 size:6"}
{"level":"info","ts":"2024-08-13T14:35:59.365858Z","caller":"traceutil/trace.go:171","msg":"trace[1539024098] range","detail":"{range_begin:/registry/services/endpoints/job-app/zipkin; range_end:; response_count:0; response_revision:152122; }","duration":"202.245406ms","start":"2024-08-13T14:35:59.163529Z","end":"2024-08-13T14:35:59.365774Z","steps":["trace[1539024098] 'agreement among raft nodes before linearized reading'  (duration: 107.263964ms)","trace[1539024098] 'range keys from in-memory index tree'  (duration: 94.325992ms)"],"step_count":2}
{"level":"warn","ts":"2024-08-13T14:35:59.366902Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"111.45288ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" ","response":"range_response_count:1 size:1112"}
{"level":"info","ts":"2024-08-13T14:35:59.367145Z","caller":"traceutil/trace.go:171","msg":"trace[1268481645] range","detail":"{range_begin:/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath; range_end:; response_count:1; response_revision:152122; }","duration":"111.806606ms","start":"2024-08-13T14:35:59.255264Z","end":"2024-08-13T14:35:59.367071Z","steps":["trace[1268481645] 'agreement among raft nodes before linearized reading'  (duration: 111.448979ms)"],"step_count":1}
{"level":"warn","ts":"2024-08-13T14:35:59.370169Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"119.638896ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/namespaces/\" range_end:\"/registry/namespaces0\" count_only:true ","response":"range_response_count:0 size:8"}
{"level":"info","ts":"2024-08-13T14:35:59.451528Z","caller":"traceutil/trace.go:171","msg":"trace[663754789] range","detail":"{range_begin:/registry/namespaces/; range_end:/registry/namespaces0; response_count:0; response_revision:152122; }","duration":"201.070818ms","start":"2024-08-13T14:35:59.250322Z","end":"2024-08-13T14:35:59.451393Z","steps":["trace[663754789] 'agreement among raft nodes before linearized reading'  (duration: 119.532788ms)"],"step_count":1}
{"level":"warn","ts":"2024-08-13T14:35:59.450116Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"285.838292ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/pods/job-app/zipkin-7d56c74569-2lnxr\" ","response":"range_response_count:1 size:1654"}
{"level":"info","ts":"2024-08-13T14:35:59.452013Z","caller":"traceutil/trace.go:171","msg":"trace[955972061] range","detail":"{range_begin:/registry/pods/job-app/zipkin-7d56c74569-2lnxr; range_end:; response_count:1; response_revision:152122; }","duration":"287.828441ms","start":"2024-08-13T14:35:59.164147Z","end":"2024-08-13T14:35:59.451976Z","steps":["trace[955972061] 'agreement among raft nodes before linearized reading'  (duration: 285.529268ms)"],"step_count":1}
{"level":"info","ts":"2024-08-13T14:35:59.550942Z","caller":"traceutil/trace.go:171","msg":"trace[1139719457] transaction","detail":"{read_only:false; number_of_response:1; response_revision:152122; }","duration":"100.588063ms","start":"2024-08-13T14:35:59.450272Z","end":"2024-08-13T14:35:59.55086Z","steps":["trace[1139719457] 'process raft request'  (duration: 100.325243ms)"],"step_count":1}
{"level":"info","ts":"2024-08-13T14:35:59.552814Z","caller":"traceutil/trace.go:171","msg":"trace[1738549744] linearizableReadLoop","detail":"{readStateIndex:189586; appliedIndex:189586; }","duration":"101.838357ms","start":"2024-08-13T14:35:59.450925Z","end":"2024-08-13T14:35:59.552763Z","steps":["trace[1738549744] 'read index received'  (duration: 101.816455ms)","trace[1738549744] 'applied index is now lower than readState.Index'  (duration: 17.902µs)"],"step_count":2}
{"level":"warn","ts":"2024-08-13T14:35:59.553479Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"102.497407ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/serviceaccounts/job-app/default\" ","response":"range_response_count:1 size:174"}
{"level":"info","ts":"2024-08-13T14:35:59.553644Z","caller":"traceutil/trace.go:171","msg":"trace[824327922] range","detail":"{range_begin:/registry/serviceaccounts/job-app/default; range_end:; response_count:1; response_revision:152122; }","duration":"102.756326ms","start":"2024-08-13T14:35:59.450846Z","end":"2024-08-13T14:35:59.553602Z","steps":["trace[824327922] 'agreement among raft nodes before linearized reading'  (duration: 102.179583ms)"],"step_count":1}
{"level":"info","ts":"2024-08-13T14:35:59.570983Z","caller":"traceutil/trace.go:171","msg":"trace[727002371] transaction","detail":"{read_only:false; response_revision:152123; number_of_response:1; }","duration":"104.037422ms","start":"2024-08-13T14:35:59.466788Z","end":"2024-08-13T14:35:59.570825Z","steps":["trace[727002371] 'process raft request'  (duration: 101.983067ms)"],"step_count":1}
{"level":"info","ts":"2024-08-13T14:35:59.571417Z","caller":"traceutil/trace.go:171","msg":"trace[372716019] transaction","detail":"{read_only:false; response_revision:152125; number_of_response:1; }","duration":"101.866058ms","start":"2024-08-13T14:35:59.469485Z","end":"2024-08-13T14:35:59.571351Z","steps":["trace[372716019] 'process raft request'  (duration: 100.047822ms)"],"step_count":1}
{"level":"warn","ts":"2024-08-13T14:35:59.571646Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"106.041573ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/endpointslices/job-app/zipkin-dt7tz\" ","response":"range_response_count:1 size:924"}
{"level":"warn","ts":"2024-08-13T14:35:59.573249Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"107.933915ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/services/endpoints/job-app/zipkin\" ","response":"range_response_count:0 size:6"}
{"level":"info","ts":"2024-08-13T14:35:59.652033Z","caller":"traceutil/trace.go:171","msg":"trace[194501807] range","detail":"{range_begin:/registry/services/endpoints/job-app/zipkin; range_end:; response_count:0; response_revision:152127; }","duration":"186.822047ms","start":"2024-08-13T14:35:59.46516Z","end":"2024-08-13T14:35:59.651982Z","steps":["trace[194501807] 'agreement among raft nodes before linearized reading'  (duration: 107.917614ms)"],"step_count":1}
{"level":"info","ts":"2024-08-13T14:35:59.651313Z","caller":"traceutil/trace.go:171","msg":"trace[936784057] range","detail":"{range_begin:/registry/endpointslices/job-app/zipkin-dt7tz; range_end:; response_count:1; response_revision:152127; }","duration":"185.773568ms","start":"2024-08-13T14:35:59.465454Z","end":"2024-08-13T14:35:59.651228Z","steps":["trace[936784057] 'agreement among raft nodes before linearized reading'  (duration: 106.006471ms)"],"step_count":1}
{"level":"warn","ts":"2024-08-13T14:35:59.786255Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"135.299673ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/replicasets/job-app/zipkin-7d56c74569\" ","response":"range_response_count:1 size:1830"}
{"level":"info","ts":"2024-08-13T14:35:59.786444Z","caller":"traceutil/trace.go:171","msg":"trace[562376981] range","detail":"{range_begin:/registry/replicasets/job-app/zipkin-7d56c74569; range_end:; response_count:1; response_revision:152127; }","duration":"135.563392ms","start":"2024-08-13T14:35:59.650849Z","end":"2024-08-13T14:35:59.786412Z","steps":["trace[562376981] 'range keys from in-memory index tree'  (duration: 134.923344ms)"],"step_count":1}
{"level":"warn","ts":"2024-08-13T14:35:59.786924Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"135.933921ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/health\" ","response":"range_response_count:0 size:6"}
{"level":"info","ts":"2024-08-13T14:35:59.786997Z","caller":"traceutil/trace.go:171","msg":"trace[1735330295] range","detail":"{range_begin:/registry/health; range_end:; response_count:0; response_revision:152127; }","duration":"136.085432ms","start":"2024-08-13T14:35:59.65089Z","end":"2024-08-13T14:35:59.786976Z","steps":["trace[1735330295] 'range keys from in-memory index tree'  (duration: 135.724805ms)"],"step_count":1}
{"level":"info","ts":"2024-08-13T14:35:59.848161Z","caller":"traceutil/trace.go:171","msg":"trace[186602683] transaction","detail":"{read_only:false; number_of_response:1; response_revision:152129; }","duration":"181.635357ms","start":"2024-08-13T14:35:59.666498Z","end":"2024-08-13T14:35:59.848134Z","steps":["trace[186602683] 'process raft request'  (duration: 181.256128ms)"],"step_count":1}
{"level":"info","ts":"2024-08-13T14:35:59.848194Z","caller":"traceutil/trace.go:171","msg":"trace[1792545814] transaction","detail":"{read_only:false; response_revision:152128; number_of_response:1; }","duration":"194.750942ms","start":"2024-08-13T14:35:59.653417Z","end":"2024-08-13T14:35:59.848168Z","steps":["trace[1792545814] 'process raft request'  (duration: 116.64457ms)","trace[1792545814] 'compare'  (duration: 15.532968ms)"],"step_count":2}
{"level":"info","ts":"2024-08-13T14:36:03.760621Z","caller":"traceutil/trace.go:171","msg":"trace[1798665122] transaction","detail":"{read_only:false; response_revision:152136; number_of_response:1; }","duration":"109.72625ms","start":"2024-08-13T14:36:03.650835Z","end":"2024-08-13T14:36:03.760562Z","steps":["trace[1798665122] 'process raft request'  (duration: 109.273916ms)"],"step_count":1}
{"level":"info","ts":"2024-08-13T14:36:04.150049Z","caller":"traceutil/trace.go:171","msg":"trace[1426165051] transaction","detail":"{read_only:false; response_revision:152137; number_of_response:1; }","duration":"180.100641ms","start":"2024-08-13T14:36:03.969883Z","end":"2024-08-13T14:36:04.149984Z","steps":["trace[1426165051] 'process raft request'  (duration: 91.988616ms)","trace[1426165051] 'compare'  (duration: 86.57731ms)"],"step_count":2}
{"level":"info","ts":"2024-08-13T14:36:06.360178Z","caller":"traceutil/trace.go:171","msg":"trace[618434589] transaction","detail":"{read_only:false; response_revision:152140; number_of_response:1; }","duration":"106.400886ms","start":"2024-08-13T14:36:06.253694Z","end":"2024-08-13T14:36:06.360095Z","steps":["trace[618434589] 'process raft request'  (duration: 105.778046ms)"],"step_count":1}
{"level":"info","ts":"2024-08-13T14:36:08.850305Z","caller":"traceutil/trace.go:171","msg":"trace[135476548] transaction","detail":"{read_only:false; response_revision:152141; number_of_response:1; }","duration":"191.971343ms","start":"2024-08-13T14:36:08.658311Z","end":"2024-08-13T14:36:08.850282Z","steps":["trace[135476548] 'process raft request'  (duration: 191.821834ms)"],"step_count":1}
{"level":"info","ts":"2024-08-13T14:36:10.77347Z","caller":"traceutil/trace.go:171","msg":"trace[14307980] transaction","detail":"{read_only:false; response_revision:152145; number_of_response:1; }","duration":"106.6203ms","start":"2024-08-13T14:36:10.666788Z","end":"2024-08-13T14:36:10.773409Z","steps":["trace[14307980] 'process raft request'  (duration: 106.098166ms)"],"step_count":1}
{"level":"info","ts":"2024-08-13T14:36:35.147073Z","caller":"traceutil/trace.go:171","msg":"trace[1690396167] transaction","detail":"{read_only:false; response_revision:152199; number_of_response:1; }","duration":"177.589926ms","start":"2024-08-13T14:36:34.969375Z","end":"2024-08-13T14:36:35.146965Z","steps":["trace[1690396167] 'process raft request'  (duration: 177.193ms)"],"step_count":1}
{"level":"info","ts":"2024-08-13T14:36:35.561147Z","caller":"traceutil/trace.go:171","msg":"trace[871489043] linearizableReadLoop","detail":"{readStateIndex:189674; appliedIndex:189673; }","duration":"109.620514ms","start":"2024-08-13T14:36:35.451471Z","end":"2024-08-13T14:36:35.561092Z","steps":["trace[871489043] 'read index received'  (duration: 108.614249ms)","trace[871489043] 'applied index is now lower than readState.Index'  (duration: 1.002965ms)"],"step_count":2}
{"level":"info","ts":"2024-08-13T14:36:35.561536Z","caller":"traceutil/trace.go:171","msg":"trace[1559555156] transaction","detail":"{read_only:false; response_revision:152200; number_of_response:1; }","duration":"114.013799ms","start":"2024-08-13T14:36:35.447464Z","end":"2024-08-13T14:36:35.561478Z","steps":["trace[1559555156] 'process raft request'  (duration: 112.815221ms)"],"step_count":1}
{"level":"warn","ts":"2024-08-13T14:36:35.561672Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"110.249056ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/health\" ","response":"range_response_count:0 size:6"}
{"level":"info","ts":"2024-08-13T14:36:35.561788Z","caller":"traceutil/trace.go:171","msg":"trace[1319926036] range","detail":"{range_begin:/registry/health; range_end:; response_count:0; response_revision:152200; }","duration":"110.47647ms","start":"2024-08-13T14:36:35.451278Z","end":"2024-08-13T14:36:35.561755Z","steps":["trace[1319926036] 'agreement among raft nodes before linearized reading'  (duration: 110.263756ms)"],"step_count":1}
{"level":"info","ts":"2024-08-13T14:36:36.546549Z","caller":"traceutil/trace.go:171","msg":"trace[1016796401] transaction","detail":"{read_only:false; response_revision:152201; number_of_response:1; }","duration":"191.174407ms","start":"2024-08-13T14:36:36.355317Z","end":"2024-08-13T14:36:36.546492Z","steps":["trace[1016796401] 'process raft request'  (duration: 190.783082ms)"],"step_count":1}
{"level":"warn","ts":"2024-08-13T14:36:36.66602Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"108.185921ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/health\" ","response":"range_response_count:0 size:6"}
{"level":"info","ts":"2024-08-13T14:36:36.746024Z","caller":"traceutil/trace.go:171","msg":"trace[1937523237] range","detail":"{range_begin:/registry/health; range_end:; response_count:0; response_revision:152201; }","duration":"188.183513ms","start":"2024-08-13T14:36:36.55777Z","end":"2024-08-13T14:36:36.745953Z","steps":["trace[1937523237] 'range keys from in-memory index tree'  (duration: 107.836498ms)"],"step_count":1}
{"level":"info","ts":"2024-08-13T14:36:50.347856Z","caller":"traceutil/trace.go:171","msg":"trace[782235188] transaction","detail":"{read_only:false; response_revision:152211; number_of_response:1; }","duration":"176.419212ms","start":"2024-08-13T14:36:50.171374Z","end":"2024-08-13T14:36:50.347793Z","steps":["trace[782235188] 'process raft request'  (duration: 175.918779ms)"],"step_count":1}
{"level":"info","ts":"2024-08-13T14:37:08.375458Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":151651}
{"level":"info","ts":"2024-08-13T14:37:08.392055Z","caller":"mvcc/kvstore_compaction.go:68","msg":"finished scheduled compaction","compact-revision":151651,"took":"12.639532ms","hash":150149959,"current-db-size-bytes":6430720,"current-db-size":"6.4 MB","current-db-size-in-use-bytes":4419584,"current-db-size-in-use":"4.4 MB"}
{"level":"info","ts":"2024-08-13T14:37:08.392162Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":150149959,"revision":151651,"compact-revision":151384}


==> kernel <==
 14:37:11 up 1 day,  5:58,  0 users,  load average: 3.39, 3.56, 3.62
Linux minikube 5.15.146.1-microsoft-standard-WSL2 #1 SMP Thu Jan 11 04:09:03 UTC 2024 x86_64 x86_64 x86_64 GNU/Linux
PRETTY_NAME="Ubuntu 22.04.4 LTS"


==> kube-apiserver [749fd11cf8fd] <==
Trace[1909963792]: [673.331201ms] [673.331201ms] END
I0813 14:32:50.661398       1 trace.go:236] Trace[765717549]: "Delete" accept:application/vnd.kubernetes.protobuf,application/json,audit-id:f98e837f-25eb-4509-bfc5-91915a769ad3,client:192.168.49.2,api-group:discovery.k8s.io,api-version:v1,name:rabbitmq-j4s7t,subresource:,namespace:job-app,protocol:HTTP/2.0,resource:endpointslices,scope:resource,url:/apis/discovery.k8s.io/v1/namespaces/job-app/endpointslices/rabbitmq-j4s7t,user-agent:kube-controller-manager/v1.30.0 (linux/amd64) kubernetes/7c48c2b/system:serviceaccount:kube-system:generic-garbage-collector,verb:DELETE (13-Aug-2024 14:32:50.082) (total time: 578ms):
Trace[765717549]: ---"Object deleted from database" 578ms (14:32:50.661)
Trace[765717549]: [578.591115ms] [578.591115ms] END
I0813 14:32:51.285007       1 trace.go:236] Trace[1045559800]: "Delete" accept:application/json,audit-id:8d168ce8-2148-404c-8a4d-5c845048f684,client:192.168.49.1,api-group:,api-version:v1,name:rabbitmq,subresource:,namespace:job-app,protocol:HTTP/2.0,resource:services,scope:resource,url:/api/v1/namespaces/job-app/services/rabbitmq,user-agent:kubectl.exe/v1.29.2 (windows/amd64) kubernetes/4b8e819,verb:DELETE (13-Aug-2024 14:32:49.263) (total time: 2021ms):
Trace[1045559800]: ---"Object deleted from database" 2020ms (14:32:51.284)
Trace[1045559800]: [2.021366898s] [2.021366898s] END
I0813 14:32:52.278609       1 trace.go:236] Trace[936879557]: "Delete" accept:application/json,audit-id:1173147c-d775-49ae-a010-d08eecd42ff8,client:192.168.49.1,api-group:,api-version:v1,name:review-service,subresource:,namespace:job-app,protocol:HTTP/2.0,resource:services,scope:resource,url:/api/v1/namespaces/job-app/services/review-service,user-agent:kubectl.exe/v1.29.2 (windows/amd64) kubernetes/4b8e819,verb:DELETE (13-Aug-2024 14:32:51.683) (total time: 594ms):
Trace[936879557]: ---"Object deleted from database" 594ms (14:32:52.278)
Trace[936879557]: [594.781523ms] [594.781523ms] END
I0813 14:32:53.761146       1 trace.go:236] Trace[1480474422]: "Delete" accept:application/json,audit-id:f67e4e19-25f3-466a-907a-06317c7d020e,client:192.168.49.1,api-group:,api-version:v1,name:zipkin,subresource:,namespace:job-app,protocol:HTTP/2.0,resource:services,scope:resource,url:/api/v1/namespaces/job-app/services/zipkin,user-agent:kubectl.exe/v1.29.2 (windows/amd64) kubernetes/4b8e819,verb:DELETE (13-Aug-2024 14:32:52.368) (total time: 1392ms):
Trace[1480474422]: ---"Object deleted from database" 1391ms (14:32:53.760)
Trace[1480474422]: [1.392225523s] [1.392225523s] END
I0813 14:32:56.165252       1 trace.go:236] Trace[678701992]: "Delete" accept:application/vnd.kubernetes.protobuf,application/json,audit-id:2399c4b2-1d74-4359-a707-ce3606c86295,client:192.168.49.2,api-group:apps,api-version:v1,name:review-service-deployment-6bb45ffdd4,subresource:,namespace:job-app,protocol:HTTP/2.0,resource:replicasets,scope:resource,url:/apis/apps/v1/namespaces/job-app/replicasets/review-service-deployment-6bb45ffdd4,user-agent:kube-controller-manager/v1.30.0 (linux/amd64) kubernetes/7c48c2b/system:serviceaccount:kube-system:generic-garbage-collector,verb:DELETE (13-Aug-2024 14:32:55.619) (total time: 546ms):
Trace[678701992]: ---"Object deleted from database" 505ms (14:32:56.164)
Trace[678701992]: [546.077268ms] [546.077268ms] END
I0813 14:32:57.161079       1 trace.go:236] Trace[859142822]: "Delete" accept:application/vnd.kubernetes.protobuf,application/json,audit-id:9e3333e9-c163-432c-90ea-5a3655d860fc,client:192.168.49.2,api-group:,api-version:v1,name:review-service-deployment-6bb45ffdd4-qw9k6,subresource:,namespace:job-app,protocol:HTTP/2.0,resource:pods,scope:resource,url:/api/v1/namespaces/job-app/pods/review-service-deployment-6bb45ffdd4-qw9k6,user-agent:kube-controller-manager/v1.30.0 (linux/amd64) kubernetes/7c48c2b/system:serviceaccount:kube-system:generic-garbage-collector,verb:DELETE (13-Aug-2024 14:32:56.286) (total time: 690ms):
Trace[859142822]: ---"Writing http response done" 100ms (14:32:56.976)
Trace[859142822]: [690.076448ms] [690.076448ms] END
I0813 14:32:57.568301       1 trace.go:236] Trace[1852404378]: "Delete" accept:application/vnd.kubernetes.protobuf,application/json,audit-id:5a1f5e9e-33f5-4593-9403-f3d65debf561,client:192.168.49.2,api-group:,api-version:v1,name:zipkin-7d56c74569-frc6b,subresource:,namespace:job-app,protocol:HTTP/2.0,resource:pods,scope:resource,url:/api/v1/namespaces/job-app/pods/zipkin-7d56c74569-frc6b,user-agent:kube-controller-manager/v1.30.0 (linux/amd64) kubernetes/7c48c2b/system:serviceaccount:kube-system:generic-garbage-collector,verb:DELETE (13-Aug-2024 14:32:56.772) (total time: 795ms):
Trace[1852404378]: [795.784202ms] [795.784202ms] END
I0813 14:32:57.767668       1 trace.go:236] Trace[1105172037]: "Delete" accept:application/json,audit-id:bb103854-de6b-48ac-afa1-6d346612a94e,client:192.168.49.1,api-group:apps,api-version:v1,name:job-db,subresource:,namespace:job-app,protocol:HTTP/2.0,resource:statefulsets,scope:resource,url:/apis/apps/v1/namespaces/job-app/statefulsets/job-db,user-agent:kubectl.exe/v1.29.2 (windows/amd64) kubernetes/4b8e819,verb:DELETE (13-Aug-2024 14:32:57.261) (total time: 506ms):
Trace[1105172037]: ---"Object deleted from database" 505ms (14:32:57.767)
Trace[1105172037]: [506.017519ms] [506.017519ms] END
I0813 14:32:58.378612       1 trace.go:236] Trace[1460963251]: "Delete" accept:application/json,audit-id:56e9d02d-20d5-4889-ab46-4cd8f7efb09c,client:192.168.49.1,api-group:apps,api-version:v1,name:review-db,subresource:,namespace:job-app,protocol:HTTP/2.0,resource:statefulsets,scope:resource,url:/apis/apps/v1/namespaces/job-app/statefulsets/review-db,user-agent:kubectl.exe/v1.29.2 (windows/amd64) kubernetes/4b8e819,verb:DELETE (13-Aug-2024 14:32:57.863) (total time: 514ms):
Trace[1460963251]: ---"Object deleted from database" 514ms (14:32:58.377)
Trace[1460963251]: [514.813323ms] [514.813323ms] END
I0813 14:32:58.761652       1 trace.go:236] Trace[593449376]: "Delete" accept:application/vnd.kubernetes.protobuf, */*,audit-id:1860d06c-a1d3-4f42-b20c-e5b585fb6cee,client:192.168.49.2,api-group:,api-version:v1,name:review-db-0,subresource:,namespace:job-app,protocol:HTTP/2.0,resource:pods,scope:resource,url:/api/v1/namespaces/job-app/pods/review-db-0,user-agent:kube-controller-manager/v1.30.0 (linux/amd64) kubernetes/7c48c2b/system:serviceaccount:kube-system:statefulset-controller,verb:DELETE (13-Aug-2024 14:32:58.162) (total time: 599ms):
Trace[593449376]: ---"Object deleted from database" 599ms (14:32:58.761)
Trace[593449376]: [599.282919ms] [599.282919ms] END
I0813 14:32:58.865422       1 trace.go:236] Trace[573172267]: "Patch" accept:application/vnd.kubernetes.protobuf, */*,audit-id:4ae13032-f5b1-4f3a-80af-8a6110cae7a2,client:192.168.49.2,api-group:,api-version:v1,name:review-db.17eb504da22fd8e6,subresource:,namespace:job-app,protocol:HTTP/2.0,resource:events,scope:resource,url:/api/v1/namespaces/job-app/events/review-db.17eb504da22fd8e6,user-agent:kube-controller-manager/v1.30.0 (linux/amd64) kubernetes/7c48c2b/system:serviceaccount:kube-system:statefulset-controller,verb:PATCH (13-Aug-2024 14:32:58.073) (total time: 791ms):
Trace[573172267]: ["GuaranteedUpdate etcd3" audit-id:4ae13032-f5b1-4f3a-80af-8a6110cae7a2,key:/events/job-app/review-db.17eb504da22fd8e6,type:*core.Event,resource:events 791ms (14:32:58.073)
Trace[573172267]:  ---"initial value restored" 688ms (14:32:58.762)
Trace[573172267]:  ---"Txn call completed" 101ms (14:32:58.864)]
Trace[573172267]: ---"Object stored in database" 101ms (14:32:58.864)
Trace[573172267]: [791.799128ms] [791.799128ms] END
I0813 14:32:59.285732       1 trace.go:236] Trace[152027591]: "Delete" accept:application/vnd.kubernetes.protobuf,application/json,audit-id:112fc5c2-e57b-415a-80ff-250b6024b4b9,client:192.168.49.2,api-group:apps,api-version:v1,name:review-db-6468b749fb,subresource:,namespace:job-app,protocol:HTTP/2.0,resource:controllerrevisions,scope:resource,url:/apis/apps/v1/namespaces/job-app/controllerrevisions/review-db-6468b749fb,user-agent:kube-controller-manager/v1.30.0 (linux/amd64) kubernetes/7c48c2b/system:serviceaccount:kube-system:generic-garbage-collector,verb:DELETE (13-Aug-2024 14:32:58.768) (total time: 517ms):
Trace[152027591]: ---"Object deleted from database" 516ms (14:32:59.285)
Trace[152027591]: [517.043576ms] [517.043576ms] END
I0813 14:32:59.860603       1 trace.go:236] Trace[411083677]: "Patch" accept:application/vnd.kubernetes.protobuf, */*,audit-id:194f3727-6793-41d4-897e-6f42c6d31d30,client:192.168.49.2,api-group:,api-version:v1,name:review-db.17eb504da22fd8e6,subresource:,namespace:job-app,protocol:HTTP/2.0,resource:events,scope:resource,url:/api/v1/namespaces/job-app/events/review-db.17eb504da22fd8e6,user-agent:kube-controller-manager/v1.30.0 (linux/amd64) kubernetes/7c48c2b/system:serviceaccount:kube-system:statefulset-controller,verb:PATCH (13-Aug-2024 14:32:59.286) (total time: 574ms):
Trace[411083677]: ["GuaranteedUpdate etcd3" audit-id:194f3727-6793-41d4-897e-6f42c6d31d30,key:/events/job-app/review-db.17eb504da22fd8e6,type:*core.Event,resource:events 573ms (14:32:59.286)
Trace[411083677]:  ---"initial value restored" 391ms (14:32:59.677)
Trace[411083677]:  ---"Txn call completed" 181ms (14:32:59.860)]
Trace[411083677]: ---"Object stored in database" 181ms (14:32:59.860)
Trace[411083677]: [574.106091ms] [574.106091ms] END
I0813 14:33:04.482407       1 trace.go:236] Trace[524830928]: "GuaranteedUpdate etcd3" audit-id:,key:/masterleases/192.168.49.2,type:*v1.Endpoints,resource:apiServerIPInfo (13-Aug-2024 14:33:03.872) (total time: 610ms):
Trace[524830928]: ---"initial value restored" 193ms (14:33:04.065)
Trace[524830928]: ---"Transaction prepared" 295ms (14:33:04.360)
Trace[524830928]: ---"Txn call completed" 121ms (14:33:04.482)
Trace[524830928]: [610.0647ms] [610.0647ms] END
I0813 14:33:06.461854       1 trace.go:236] Trace[74423222]: "Delete" accept:application/vnd.kubernetes.protobuf,application/json,audit-id:6823259c-9e8e-46b7-9e83-ab972b6f4b7c,client:192.168.49.2,api-group:,api-version:v1,name:company-db-0,subresource:,namespace:job-app,protocol:HTTP/2.0,resource:pods,scope:resource,url:/api/v1/namespaces/job-app/pods/company-db-0,user-agent:kubelet/v1.30.0 (linux/amd64) kubernetes/7c48c2b,verb:DELETE (13-Aug-2024 14:33:05.773) (total time: 687ms):
Trace[74423222]: ---"limitedReadBody succeeded" len:73 86ms (14:33:05.860)
Trace[74423222]: ---"Object deleted from database" 300ms (14:33:06.461)
Trace[74423222]: [687.845504ms] [687.845504ms] END
I0813 14:35:10.099252       1 alloc.go:330] "allocated clusterIPs" service="job-app/zipkin" clusterIPs={"IPv4":"10.111.79.60"}
I0813 14:35:18.552961       1 alloc.go:330] "allocated clusterIPs" service="default/rabbitmq" clusterIPs={"IPv4":"10.105.186.241"}
I0813 14:35:59.849050       1 trace.go:236] Trace[1110754808]: "Delete" accept:application/json,audit-id:66c4c443-8f5e-45d8-b77e-3385588b07c7,client:192.168.49.1,api-group:,api-version:v1,name:zipkin,subresource:,namespace:job-app,protocol:HTTP/2.0,resource:services,scope:resource,url:/api/v1/namespaces/job-app/services/zipkin,user-agent:kubectl.exe/v1.29.2 (windows/amd64) kubernetes/4b8e819,verb:DELETE (13-Aug-2024 14:35:59.070) (total time: 778ms):
Trace[1110754808]: ---"Object deleted from database" 778ms (14:35:59.848)
Trace[1110754808]: [778.85796ms] [778.85796ms] END
I0813 14:36:26.172120       1 alloc.go:330] "allocated clusterIPs" service="default/zipkin" clusterIPs={"IPv4":"10.101.221.204"}


==> kube-apiserver [b17e003fd5b1] <==
Trace[1046323629]: [690.744588ms] [690.744588ms] END
E0804 08:37:55.306817       1 authentication.go:73] "Unable to authenticate the request" err="[invalid bearer token, service account token has expired]"
E0804 08:37:55.370666       1 authentication.go:73] "Unable to authenticate the request" err="[invalid bearer token, service account token has expired]"
I0804 13:17:31.188158       1 trace.go:236] Trace[2062864202]: "Get" accept:application/vnd.kubernetes.protobuf,application/json,audit-id:07a709f2-fecc-4353-a69a-e659be0d3b4c,client:192.168.49.2,api-group:,api-version:v1,name:my-deployment-d4f99b466-kvq9r,subresource:,namespace:default,protocol:HTTP/2.0,resource:pods,scope:resource,url:/api/v1/namespaces/default/pods/my-deployment-d4f99b466-kvq9r,user-agent:kubelet/v1.30.0 (linux/amd64) kubernetes/7c48c2b,verb:GET (04-Aug-2024 13:17:30.622) (total time: 563ms):
Trace[2062864202]: ---"About to write a response" 563ms (13:17:31.185)
Trace[2062864202]: [563.338793ms] [563.338793ms] END
I0804 13:17:31.465901       1 trace.go:236] Trace[1251302768]: "GuaranteedUpdate etcd3" audit-id:,key:/masterleases/192.168.49.2,type:*v1.Endpoints,resource:apiServerIPInfo (04-Aug-2024 13:17:30.605) (total time: 860ms):
Trace[1251302768]: ---"Transaction prepared" 579ms (13:17:31.186)
Trace[1251302768]: ---"Txn call completed" 279ms (13:17:31.465)
Trace[1251302768]: [860.513861ms] [860.513861ms] END
I0804 14:05:01.679591       1 trace.go:236] Trace[1050694592]: "Update" accept:application/vnd.kubernetes.protobuf, */*,audit-id:f3e02e2b-0e0f-4d21-8223-31cca96d0070,client:::1,api-group:coordination.k8s.io,api-version:v1,name:apiserver-eqt674mfxb4j56mrjjkoe7b7ii,subresource:,namespace:kube-system,protocol:HTTP/2.0,resource:leases,scope:resource,url:/apis/coordination.k8s.io/v1/namespaces/kube-system/leases/apiserver-eqt674mfxb4j56mrjjkoe7b7ii,user-agent:kube-apiserver/v1.30.0 (linux/amd64) kubernetes/7c48c2b,verb:PUT (04-Aug-2024 13:17:43.650) (total time: 504ms):
Trace[1050694592]: ["GuaranteedUpdate etcd3" audit-id:f3e02e2b-0e0f-4d21-8223-31cca96d0070,key:/leases/kube-system/apiserver-eqt674mfxb4j56mrjjkoe7b7ii,type:*coordination.Lease,resource:leases.coordination.k8s.io 504ms (13:17:43.650)
Trace[1050694592]:  ---"Txn call completed" 502ms (14:05:01.679)]
Trace[1050694592]: [504.763394ms] [504.763394ms] END
I0804 14:05:02.028858       1 trace.go:236] Trace[306515954]: "GuaranteedUpdate etcd3" audit-id:,key:/masterleases/192.168.49.2,type:*v1.Endpoints,resource:apiServerIPInfo (04-Aug-2024 13:17:43.648) (total time: 855ms):
Trace[306515954]: ---"initial value restored" 505ms (14:05:01.678)
Trace[306515954]: ---"Transaction prepared" 166ms (14:05:01.845)
Trace[306515954]: ---"Txn call completed" 183ms (14:05:02.028)
Trace[306515954]: [855.745925ms] [855.745925ms] END
E0804 14:05:14.168768       1 authentication.go:73] "Unable to authenticate the request" err="[invalid bearer token, service account token has expired]"
E0804 14:05:14.557231       1 authentication.go:73] "Unable to authenticate the request" err="[invalid bearer token, service account token has expired]"
I0804 18:46:41.989740       1 trace.go:236] Trace[2027533307]: "Update" accept:application/json, */*,audit-id:86d04f28-87c4-46df-9658-86d20ad67977,client:192.168.49.2,api-group:,api-version:v1,name:k8s.io-minikube-hostpath,subresource:,namespace:kube-system,protocol:HTTP/2.0,resource:endpoints,scope:resource,url:/api/v1/namespaces/kube-system/endpoints/k8s.io-minikube-hostpath,user-agent:storage-provisioner/v0.0.0 (linux/amd64) kubernetes/$Format,verb:PUT (04-Aug-2024 15:46:42.421) (total time: 1152ms):
Trace[2027533307]: ["GuaranteedUpdate etcd3" audit-id:86d04f28-87c4-46df-9658-86d20ad67977,key:/services/endpoints/kube-system/k8s.io-minikube-hostpath,type:*core.Endpoints,resource:endpoints 1151ms (15:46:42.422)
Trace[2027533307]:  ---"Txn call completed" 1150ms (18:46:41.989)]
Trace[2027533307]: [1.152091465s] [1.152091465s] END
I0804 18:46:41.989779       1 trace.go:236] Trace[336651199]: "Update" accept:application/vnd.kubernetes.protobuf, */*,audit-id:fa42d704-625a-4ce0-9cdb-e815a0b45acd,client:::1,api-group:coordination.k8s.io,api-version:v1,name:apiserver-eqt674mfxb4j56mrjjkoe7b7ii,subresource:,namespace:kube-system,protocol:HTTP/2.0,resource:leases,scope:resource,url:/apis/coordination.k8s.io/v1/namespaces/kube-system/leases/apiserver-eqt674mfxb4j56mrjjkoe7b7ii,user-agent:kube-apiserver/v1.30.0 (linux/amd64) kubernetes/7c48c2b,verb:PUT (04-Aug-2024 15:46:42.421) (total time: 1152ms):
Trace[336651199]: ["GuaranteedUpdate etcd3" audit-id:fa42d704-625a-4ce0-9cdb-e815a0b45acd,key:/leases/kube-system/apiserver-eqt674mfxb4j56mrjjkoe7b7ii,type:*coordination.Lease,resource:leases.coordination.k8s.io 1152ms (15:46:42.421)
Trace[336651199]:  ---"About to Encode" 99ms (15:46:42.520)
Trace[336651199]:  ---"Txn call completed" 1053ms (18:46:41.989)]
Trace[336651199]: [1.152864587s] [1.152864587s] END
I0804 18:46:41.992016       1 trace.go:236] Trace[125425849]: "GuaranteedUpdate etcd3" audit-id:,key:/masterleases/192.168.49.2,type:*v1.Endpoints,resource:apiServerIPInfo (04-Aug-2024 15:46:42.333) (total time: 1242ms):
Trace[125425849]: ---"initial value restored" 86ms (15:46:42.419)
Trace[125425849]: ---"Transaction prepared" 1115ms (18:46:41.951)
Trace[125425849]: [1.242864655s] [1.242864655s] END
E0804 18:46:43.847541       1 authentication.go:73] "Unable to authenticate the request" err="[invalid bearer token, service account token has expired]"
E0805 01:26:18.357953       1 authentication.go:73] "Unable to authenticate the request" err="[invalid bearer token, service account token has expired]"
I0805 01:26:19.275883       1 trace.go:236] Trace[131906564]: "Get" accept:application/json, */*,audit-id:88bd94e2-86c3-401e-9295-4f13a58f0aa0,client:192.168.49.2,api-group:,api-version:v1,name:k8s.io-minikube-hostpath,subresource:,namespace:kube-system,protocol:HTTP/2.0,resource:endpoints,scope:resource,url:/api/v1/namespaces/kube-system/endpoints/k8s.io-minikube-hostpath,user-agent:storage-provisioner/v0.0.0 (linux/amd64) kubernetes/$Format,verb:GET (05-Aug-2024 01:26:17.732) (total time: 1543ms):
Trace[131906564]: ---"About to write a response" 1543ms (01:26:19.275)
Trace[131906564]: [1.543196944s] [1.543196944s] END
I0805 01:26:19.276143       1 trace.go:236] Trace[1758256418]: "Update" accept:application/vnd.kubernetes.protobuf, */*,audit-id:07af825e-ba3f-4fa4-94c7-2374fcb53442,client:::1,api-group:coordination.k8s.io,api-version:v1,name:apiserver-eqt674mfxb4j56mrjjkoe7b7ii,subresource:,namespace:kube-system,protocol:HTTP/2.0,resource:leases,scope:resource,url:/apis/coordination.k8s.io/v1/namespaces/kube-system/leases/apiserver-eqt674mfxb4j56mrjjkoe7b7ii,user-agent:kube-apiserver/v1.30.0 (linux/amd64) kubernetes/7c48c2b,verb:PUT (05-Aug-2024 01:26:17.734) (total time: 1541ms):
Trace[1758256418]: ["GuaranteedUpdate etcd3" audit-id:07af825e-ba3f-4fa4-94c7-2374fcb53442,key:/leases/kube-system/apiserver-eqt674mfxb4j56mrjjkoe7b7ii,type:*coordination.Lease,resource:leases.coordination.k8s.io 1540ms (01:26:17.735)
Trace[1758256418]:  ---"Txn call completed" 1540ms (01:26:19.275)]
Trace[1758256418]: [1.54113338s] [1.54113338s] END
I0805 01:26:19.354532       1 trace.go:236] Trace[2061558364]: "GuaranteedUpdate etcd3" audit-id:,key:/masterleases/192.168.49.2,type:*v1.Endpoints,resource:apiServerIPInfo (05-Aug-2024 01:26:17.727) (total time: 1626ms):
Trace[2061558364]: ---"initial value restored" 1543ms (01:26:19.271)
Trace[2061558364]: ---"Txn call completed" 73ms (01:26:19.354)
Trace[2061558364]: [1.62665831s] [1.62665831s] END
I0805 01:32:00.354179       1 trace.go:236] Trace[1841149343]: "Update" accept:application/json, */*,audit-id:24049b93-d40b-4643-b95b-1a99d6d97340,client:192.168.49.2,api-group:,api-version:v1,name:k8s.io-minikube-hostpath,subresource:,namespace:kube-system,protocol:HTTP/2.0,resource:endpoints,scope:resource,url:/api/v1/namespaces/kube-system/endpoints/k8s.io-minikube-hostpath,user-agent:storage-provisioner/v0.0.0 (linux/amd64) kubernetes/$Format,verb:PUT (05-Aug-2024 01:31:59.808) (total time: 545ms):
Trace[1841149343]: ["GuaranteedUpdate etcd3" audit-id:24049b93-d40b-4643-b95b-1a99d6d97340,key:/services/endpoints/kube-system/k8s.io-minikube-hostpath,type:*core.Endpoints,resource:endpoints 545ms (01:31:59.808)
Trace[1841149343]:  ---"Txn call completed" 544ms (01:32:00.353)]
Trace[1841149343]: [545.989598ms] [545.989598ms] END
I0805 08:25:57.396224       1 trace.go:236] Trace[153606619]: "Get" accept:application/json, */*,audit-id:a50ba21d-4493-4078-adcc-4f8284210195,client:192.168.49.2,api-group:,api-version:v1,name:k8s.io-minikube-hostpath,subresource:,namespace:kube-system,protocol:HTTP/2.0,resource:endpoints,scope:resource,url:/api/v1/namespaces/kube-system/endpoints/k8s.io-minikube-hostpath,user-agent:storage-provisioner/v0.0.0 (linux/amd64) kubernetes/$Format,verb:GET (05-Aug-2024 07:41:39.338) (total time: 585ms):
Trace[153606619]: ---"About to write a response" 585ms (08:25:57.395)
Trace[153606619]: [585.170165ms] [585.170165ms] END
I0805 08:25:57.418041       1 trace.go:236] Trace[761165855]: "GuaranteedUpdate etcd3" audit-id:,key:/masterleases/192.168.49.2,type:*v1.Endpoints,resource:apiServerIPInfo (05-Aug-2024 07:41:39.203) (total time: 742ms):
Trace[761165855]: ---"initial value restored" 535ms (08:25:57.211)
Trace[761165855]: ---"Transaction prepared" 183ms (08:25:57.395)
Trace[761165855]: [742.292167ms] [742.292167ms] END
E0805 08:26:04.234406       1 authentication.go:73] "Unable to authenticate the request" err="[invalid bearer token, service account token has expired]"
E0805 08:26:14.152780       1 authentication.go:73] "Unable to authenticate the request" err="[invalid bearer token, service account token has expired]"


==> kube-controller-manager [141df2ef222c] <==
I0809 14:04:13.495375       1 serving.go:380] Generated self-signed cert in-memory
I0809 14:04:15.628637       1 controllermanager.go:189] "Starting" version="v1.30.0"
I0809 14:04:15.628728       1 controllermanager.go:191] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I0809 14:04:15.633025       1 secure_serving.go:213] Serving securely on 127.0.0.1:10257
I0809 14:04:15.635093       1 tlsconfig.go:240] "Starting DynamicServingCertificateController"
I0809 14:04:15.635235       1 dynamic_cafile_content.go:157] "Starting controller" name="request-header::/var/lib/minikube/certs/front-proxy-ca.crt"
I0809 14:04:15.635342       1 dynamic_cafile_content.go:157] "Starting controller" name="client-ca-bundle::/var/lib/minikube/certs/ca.crt"
E0809 14:04:28.255269       1 controllermanager.go:234] "Error building controller context" err="failed to wait for apiserver being healthy: timed out waiting for the condition: failed to get apiserver /healthz status: an error on the server (\"[+]ping ok\\n[+]log ok\\n[+]etcd ok\\n[+]poststarthook/start-apiserver-admission-initializer ok\\n[+]poststarthook/generic-apiserver-start-informers ok\\n[+]poststarthook/priority-and-fairness-config-consumer ok\\n[+]poststarthook/priority-and-fairness-filter ok\\n[+]poststarthook/storage-object-count-tracker-hook ok\\n[+]poststarthook/start-apiextensions-informers ok\\n[+]poststarthook/start-apiextensions-controllers ok\\n[+]poststarthook/crd-informer-synced ok\\n[+]poststarthook/start-service-ip-repair-controllers ok\\n[-]poststarthook/rbac/bootstrap-roles failed: reason withheld\\n[+]poststarthook/scheduling/bootstrap-system-priority-classes ok\\n[+]poststarthook/priority-and-fairness-config-producer ok\\n[+]poststarthook/start-system-namespaces-controller ok\\n[+]poststarthook/bootstrap-controller ok\\n[+]poststarthook/start-cluster-authentication-info-controller ok\\n[+]poststarthook/start-kube-apiserver-identity-lease-controller ok\\n[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok\\n[+]poststarthook/start-legacy-token-tracking-controller ok\\n[+]poststarthook/aggregator-reload-proxy-client-cert ok\\n[+]poststarthook/start-kube-aggregator-informers ok\\n[+]poststarthook/apiservice-registration-controller ok\\n[+]poststarthook/apiservice-status-available-controller ok\\n[+]poststarthook/apiservice-discovery-controller ok\\n[+]poststarthook/kube-apiserver-autoregistration ok\\n[+]autoregister-completion ok\\n[+]poststarthook/apiservice-openapi-controller ok\\n[+]poststarthook/apiservice-openapiv3-controller ok\\nhealthz check failed\") has prevented the request from succeeding"


==> kube-controller-manager [89948afcbacc] <==
E0813 14:33:57.981731       1 pv_controller.go:1568] "Error finding provisioning plugin for claim" err="storageclass.storage.k8s.io \"manual\" not found" logger="persistentvolume-binder-controller" PVC="job-app/company-db-pvc"
E0813 14:34:12.979186       1 pv_controller.go:1568] "Error finding provisioning plugin for claim" err="storageclass.storage.k8s.io \"manual\" not found" logger="persistentvolume-binder-controller" PVC="job-app/review-db-pvc"
E0813 14:34:12.979333       1 pv_controller.go:1568] "Error finding provisioning plugin for claim" err="storageclass.storage.k8s.io \"manual\" not found" logger="persistentvolume-binder-controller" PVC="job-app/company-db-pvc"
E0813 14:34:12.979557       1 pv_controller.go:1568] "Error finding provisioning plugin for claim" err="storageclass.storage.k8s.io \"manual\" not found" logger="persistentvolume-binder-controller" PVC="job-app/job-db-pvc"
E0813 14:34:27.980669       1 pv_controller.go:1568] "Error finding provisioning plugin for claim" err="storageclass.storage.k8s.io \"manual\" not found" logger="persistentvolume-binder-controller" PVC="job-app/company-db-pvc"
E0813 14:34:27.981507       1 pv_controller.go:1568] "Error finding provisioning plugin for claim" err="storageclass.storage.k8s.io \"manual\" not found" logger="persistentvolume-binder-controller" PVC="job-app/job-db-pvc"
E0813 14:34:27.981874       1 pv_controller.go:1568] "Error finding provisioning plugin for claim" err="storageclass.storage.k8s.io \"manual\" not found" logger="persistentvolume-binder-controller" PVC="job-app/review-db-pvc"
E0813 14:34:42.979228       1 pv_controller.go:1568] "Error finding provisioning plugin for claim" err="storageclass.storage.k8s.io \"manual\" not found" logger="persistentvolume-binder-controller" PVC="job-app/job-db-pvc"
E0813 14:34:42.979604       1 pv_controller.go:1568] "Error finding provisioning plugin for claim" err="storageclass.storage.k8s.io \"manual\" not found" logger="persistentvolume-binder-controller" PVC="job-app/review-db-pvc"
E0813 14:34:42.979712       1 pv_controller.go:1568] "Error finding provisioning plugin for claim" err="storageclass.storage.k8s.io \"manual\" not found" logger="persistentvolume-binder-controller" PVC="job-app/company-db-pvc"
E0813 14:34:58.052792       1 pv_controller.go:1568] "Error finding provisioning plugin for claim" err="storageclass.storage.k8s.io \"manual\" not found" logger="persistentvolume-binder-controller" PVC="job-app/company-db-pvc"
E0813 14:34:58.053384       1 pv_controller.go:1568] "Error finding provisioning plugin for claim" err="storageclass.storage.k8s.io \"manual\" not found" logger="persistentvolume-binder-controller" PVC="job-app/job-db-pvc"
E0813 14:34:58.054185       1 pv_controller.go:1568] "Error finding provisioning plugin for claim" err="storageclass.storage.k8s.io \"manual\" not found" logger="persistentvolume-binder-controller" PVC="job-app/review-db-pvc"
I0813 14:35:10.060864       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="job-app/zipkin-7d56c74569" duration="76.36419ms"
I0813 14:35:10.101486       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="job-app/zipkin-7d56c74569" duration="40.557871ms"
I0813 14:35:10.101618       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="job-app/zipkin-7d56c74569" duration="77.305µs"
I0813 14:35:10.102865       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="job-app/zipkin-7d56c74569" duration="57.703µs"
E0813 14:35:13.051718       1 pv_controller.go:1568] "Error finding provisioning plugin for claim" err="storageclass.storage.k8s.io \"manual\" not found" logger="persistentvolume-binder-controller" PVC="job-app/company-db-pvc"
E0813 14:35:13.052249       1 pv_controller.go:1568] "Error finding provisioning plugin for claim" err="storageclass.storage.k8s.io \"manual\" not found" logger="persistentvolume-binder-controller" PVC="job-app/job-db-pvc"
E0813 14:35:13.052607       1 pv_controller.go:1568] "Error finding provisioning plugin for claim" err="storageclass.storage.k8s.io \"manual\" not found" logger="persistentvolume-binder-controller" PVC="job-app/review-db-pvc"
I0813 14:35:18.560896       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/rabbitmq-58f4956f6f" duration="244.388394ms"
I0813 14:35:18.762947       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/rabbitmq-58f4956f6f" duration="201.495575ms"
I0813 14:35:18.763311       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/rabbitmq-58f4956f6f" duration="223.614µs"
I0813 14:35:18.872249       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/rabbitmq-58f4956f6f" duration="122.607µs"
I0813 14:35:19.379171       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="job-app/zipkin-7d56c74569" duration="93.442125ms"
I0813 14:35:19.379442       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="job-app/zipkin-7d56c74569" duration="145.31µs"
I0813 14:35:22.794464       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/rabbitmq-58f4956f6f" duration="15.29587ms"
I0813 14:35:22.794699       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/rabbitmq-58f4956f6f" duration="132.009µs"
E0813 14:35:28.051801       1 pv_controller.go:1568] "Error finding provisioning plugin for claim" err="storageclass.storage.k8s.io \"manual\" not found" logger="persistentvolume-binder-controller" PVC="job-app/job-db-pvc"
E0813 14:35:28.052072       1 pv_controller.go:1568] "Error finding provisioning plugin for claim" err="storageclass.storage.k8s.io \"manual\" not found" logger="persistentvolume-binder-controller" PVC="job-app/review-db-pvc"
E0813 14:35:28.052399       1 pv_controller.go:1568] "Error finding provisioning plugin for claim" err="storageclass.storage.k8s.io \"manual\" not found" logger="persistentvolume-binder-controller" PVC="job-app/company-db-pvc"
E0813 14:35:43.051023       1 pv_controller.go:1568] "Error finding provisioning plugin for claim" err="storageclass.storage.k8s.io \"manual\" not found" logger="persistentvolume-binder-controller" PVC="job-app/company-db-pvc"
E0813 14:35:43.051667       1 pv_controller.go:1568] "Error finding provisioning plugin for claim" err="storageclass.storage.k8s.io \"manual\" not found" logger="persistentvolume-binder-controller" PVC="job-app/job-db-pvc"
E0813 14:35:43.052095       1 pv_controller.go:1568] "Error finding provisioning plugin for claim" err="storageclass.storage.k8s.io \"manual\" not found" logger="persistentvolume-binder-controller" PVC="job-app/review-db-pvc"
E0813 14:35:58.052695       1 pv_controller.go:1568] "Error finding provisioning plugin for claim" err="storageclass.storage.k8s.io \"manual\" not found" logger="persistentvolume-binder-controller" PVC="job-app/job-db-pvc"
E0813 14:35:58.053143       1 pv_controller.go:1568] "Error finding provisioning plugin for claim" err="storageclass.storage.k8s.io \"manual\" not found" logger="persistentvolume-binder-controller" PVC="job-app/review-db-pvc"
E0813 14:35:58.053375       1 pv_controller.go:1568] "Error finding provisioning plugin for claim" err="storageclass.storage.k8s.io \"manual\" not found" logger="persistentvolume-binder-controller" PVC="job-app/company-db-pvc"
I0813 14:35:59.276157       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="job-app/zipkin-7d56c74569" duration="224.051846ms"
I0813 14:35:59.948937       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="job-app/zipkin-7d56c74569" duration="600.347739ms"
I0813 14:36:00.051119       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="job-app/zipkin-7d56c74569" duration="102.054373ms"
I0813 14:36:00.051408       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="job-app/zipkin-7d56c74569" duration="93.407µs"
I0813 14:36:00.152651       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="job-app/zipkin-7d56c74569" duration="22.302µs"
E0813 14:36:13.049879       1 pv_controller.go:1568] "Error finding provisioning plugin for claim" err="storageclass.storage.k8s.io \"manual\" not found" logger="persistentvolume-binder-controller" PVC="job-app/review-db-pvc"
E0813 14:36:13.050003       1 pv_controller.go:1568] "Error finding provisioning plugin for claim" err="storageclass.storage.k8s.io \"manual\" not found" logger="persistentvolume-binder-controller" PVC="job-app/company-db-pvc"
E0813 14:36:13.050217       1 pv_controller.go:1568] "Error finding provisioning plugin for claim" err="storageclass.storage.k8s.io \"manual\" not found" logger="persistentvolume-binder-controller" PVC="job-app/job-db-pvc"
I0813 14:36:26.170302       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/zipkin-7d56c74569" duration="44.221437ms"
I0813 14:36:26.269985       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/zipkin-7d56c74569" duration="99.470482ms"
I0813 14:36:26.270291       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/zipkin-7d56c74569" duration="68.905µs"
I0813 14:36:26.374398       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/zipkin-7d56c74569" duration="149.409µs"
E0813 14:36:28.051485       1 pv_controller.go:1568] "Error finding provisioning plugin for claim" err="storageclass.storage.k8s.io \"manual\" not found" logger="persistentvolume-binder-controller" PVC="job-app/company-db-pvc"
E0813 14:36:28.052270       1 pv_controller.go:1568] "Error finding provisioning plugin for claim" err="storageclass.storage.k8s.io \"manual\" not found" logger="persistentvolume-binder-controller" PVC="job-app/job-db-pvc"
E0813 14:36:28.052589       1 pv_controller.go:1568] "Error finding provisioning plugin for claim" err="storageclass.storage.k8s.io \"manual\" not found" logger="persistentvolume-binder-controller" PVC="job-app/review-db-pvc"
I0813 14:36:34.950159       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/zipkin-7d56c74569" duration="180.198495ms"
I0813 14:36:34.950565       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/zipkin-7d56c74569" duration="191.912µs"
E0813 14:36:43.050968       1 pv_controller.go:1568] "Error finding provisioning plugin for claim" err="storageclass.storage.k8s.io \"manual\" not found" logger="persistentvolume-binder-controller" PVC="job-app/job-db-pvc"
E0813 14:36:43.051486       1 pv_controller.go:1568] "Error finding provisioning plugin for claim" err="storageclass.storage.k8s.io \"manual\" not found" logger="persistentvolume-binder-controller" PVC="job-app/review-db-pvc"
E0813 14:36:43.051618       1 pv_controller.go:1568] "Error finding provisioning plugin for claim" err="storageclass.storage.k8s.io \"manual\" not found" logger="persistentvolume-binder-controller" PVC="job-app/company-db-pvc"
E0813 14:36:58.051890       1 pv_controller.go:1568] "Error finding provisioning plugin for claim" err="storageclass.storage.k8s.io \"manual\" not found" logger="persistentvolume-binder-controller" PVC="job-app/company-db-pvc"
E0813 14:36:58.052582       1 pv_controller.go:1568] "Error finding provisioning plugin for claim" err="storageclass.storage.k8s.io \"manual\" not found" logger="persistentvolume-binder-controller" PVC="job-app/job-db-pvc"
E0813 14:36:58.052865       1 pv_controller.go:1568] "Error finding provisioning plugin for claim" err="storageclass.storage.k8s.io \"manual\" not found" logger="persistentvolume-binder-controller" PVC="job-app/review-db-pvc"


==> kube-proxy [3abdf9fc2c89] <==
I0809 14:04:43.439627       1 server_linux.go:69] "Using iptables proxy"
I0809 14:04:44.533587       1 server.go:1062] "Successfully retrieved node IP(s)" IPs=["192.168.49.2"]
I0809 14:04:45.831534       1 server.go:659] "kube-proxy running in dual-stack mode" primary ipFamily="IPv4"
I0809 14:04:45.831760       1 server_linux.go:165] "Using iptables Proxier"
I0809 14:04:45.844200       1 server_linux.go:511] "Detect-local-mode set to ClusterCIDR, but no cluster CIDR for family" ipFamily="IPv6"
I0809 14:04:45.844289       1 server_linux.go:528] "Defaulting to no-op detect-local"
I0809 14:04:45.852108       1 proxier.go:243] "Setting route_localnet=1 to allow node-ports on localhost; to change this either disable iptables.localhostNodePorts (--iptables-localhost-nodeports) or set nodePortAddresses (--nodeport-addresses) to filter loopback addresses"
I0809 14:04:45.856547       1 server.go:872] "Version info" version="v1.30.0"
I0809 14:04:45.856702       1 server.go:874] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I0809 14:04:46.026388       1 config.go:192] "Starting service config controller"
I0809 14:04:46.026517       1 shared_informer.go:313] Waiting for caches to sync for service config
I0809 14:04:46.032966       1 config.go:101] "Starting endpoint slice config controller"
I0809 14:04:46.033022       1 shared_informer.go:313] Waiting for caches to sync for endpoint slice config
I0809 14:04:46.033028       1 config.go:319] "Starting node config controller"
I0809 14:04:46.033061       1 shared_informer.go:313] Waiting for caches to sync for node config
I0809 14:04:46.134939       1 shared_informer.go:320] Caches are synced for node config
I0809 14:04:46.135192       1 shared_informer.go:320] Caches are synced for endpoint slice config
I0809 14:04:46.226739       1 shared_informer.go:320] Caches are synced for service config


==> kube-proxy [409a2c61a865] <==
I0804 02:55:18.262344       1 server_linux.go:69] "Using iptables proxy"
I0804 02:55:18.462577       1 server.go:1062] "Successfully retrieved node IP(s)" IPs=["192.168.49.2"]
I0804 02:55:20.132541       1 server.go:659] "kube-proxy running in dual-stack mode" primary ipFamily="IPv4"
I0804 02:55:20.132638       1 server_linux.go:165] "Using iptables Proxier"
I0804 02:55:20.144090       1 server_linux.go:511] "Detect-local-mode set to ClusterCIDR, but no cluster CIDR for family" ipFamily="IPv6"
I0804 02:55:20.144150       1 server_linux.go:528] "Defaulting to no-op detect-local"
I0804 02:55:20.227094       1 proxier.go:243] "Setting route_localnet=1 to allow node-ports on localhost; to change this either disable iptables.localhostNodePorts (--iptables-localhost-nodeports) or set nodePortAddresses (--nodeport-addresses) to filter loopback addresses"
I0804 02:55:20.236564       1 server.go:872] "Version info" version="v1.30.0"
I0804 02:55:20.236842       1 server.go:874] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I0804 02:55:20.242397       1 config.go:192] "Starting service config controller"
I0804 02:55:20.244549       1 config.go:101] "Starting endpoint slice config controller"
I0804 02:55:20.246558       1 config.go:319] "Starting node config controller"
I0804 02:55:20.246690       1 shared_informer.go:313] Waiting for caches to sync for node config
I0804 02:55:20.246919       1 shared_informer.go:313] Waiting for caches to sync for endpoint slice config
I0804 02:55:20.247178       1 shared_informer.go:313] Waiting for caches to sync for service config
I0804 02:55:20.348289       1 shared_informer.go:320] Caches are synced for service config
I0804 02:55:20.348584       1 shared_informer.go:320] Caches are synced for node config
I0804 02:55:20.348663       1 shared_informer.go:320] Caches are synced for endpoint slice config
I0804 08:37:41.667620       1 trace.go:236] Trace[1898290047]: "iptables ChainExists" (04-Aug-2024 04:12:25.468) (total time: 4904ms):
Trace[1898290047]: [4.904110602s] [4.904110602s] END
I0804 08:37:41.669997       1 trace.go:236] Trace[1199277266]: "iptables ChainExists" (04-Aug-2024 04:12:25.468) (total time: 4908ms):
Trace[1199277266]: [4.908808384s] [4.908808384s] END


==> kube-scheduler [5f09a99bb52e] <==
I0804 02:54:48.086920       1 serving.go:380] Generated self-signed cert in-memory
I0804 02:54:54.850275       1 server.go:154] "Starting Kubernetes Scheduler" version="v1.30.0"
I0804 02:54:54.850481       1 server.go:156] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I0804 02:54:55.032143       1 secure_serving.go:213] Serving securely on 127.0.0.1:10259
I0804 02:54:55.032488       1 requestheader_controller.go:169] Starting RequestHeaderAuthRequestController
I0804 02:54:55.032554       1 shared_informer.go:313] Waiting for caches to sync for RequestHeaderAuthRequestController
I0804 02:54:55.032640       1 tlsconfig.go:240] "Starting DynamicServingCertificateController"
I0804 02:54:55.035262       1 configmap_cafile_content.go:202] "Starting controller" name="client-ca::kube-system::extension-apiserver-authentication::requestheader-client-ca-file"
I0804 02:54:55.041077       1 shared_informer.go:313] Waiting for caches to sync for client-ca::kube-system::extension-apiserver-authentication::requestheader-client-ca-file
I0804 02:54:55.133962       1 configmap_cafile_content.go:202] "Starting controller" name="client-ca::kube-system::extension-apiserver-authentication::client-ca-file"
I0804 02:54:55.134070       1 shared_informer.go:313] Waiting for caches to sync for client-ca::kube-system::extension-apiserver-authentication::client-ca-file
I0804 02:54:55.141719       1 shared_informer.go:320] Caches are synced for client-ca::kube-system::extension-apiserver-authentication::requestheader-client-ca-file
I0804 02:54:55.232836       1 shared_informer.go:320] Caches are synced for RequestHeaderAuthRequestController
I0804 02:54:55.235914       1 shared_informer.go:320] Caches are synced for client-ca::kube-system::extension-apiserver-authentication::client-ca-file


==> kube-scheduler [ec30e56023ab] <==
I0809 14:04:13.171087       1 serving.go:380] Generated self-signed cert in-memory
W0809 14:04:18.152616       1 requestheader_controller.go:193] Unable to get configmap/extension-apiserver-authentication in kube-system.  Usually fixed by 'kubectl create rolebinding -n kube-system ROLEBINDING_NAME --role=extension-apiserver-authentication-reader --serviceaccount=YOUR_NS:YOUR_SA'
W0809 14:04:18.224670       1 authentication.go:368] Error looking up in-cluster authentication configuration: configmaps "extension-apiserver-authentication" is forbidden: User "system:kube-scheduler" cannot get resource "configmaps" in API group "" in the namespace "kube-system"
W0809 14:04:18.224821       1 authentication.go:369] Continuing without authentication configuration. This may treat all requests as anonymous.
W0809 14:04:18.224862       1 authentication.go:370] To require authentication configuration lookup to succeed, set --authentication-tolerate-lookup-failure=false
I0809 14:04:18.667133       1 server.go:154] "Starting Kubernetes Scheduler" version="v1.30.0"
I0809 14:04:18.667259       1 server.go:156] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I0809 14:04:18.733368       1 configmap_cafile_content.go:202] "Starting controller" name="client-ca::kube-system::extension-apiserver-authentication::client-ca-file"
I0809 14:04:18.733456       1 shared_informer.go:313] Waiting for caches to sync for client-ca::kube-system::extension-apiserver-authentication::client-ca-file
I0809 14:04:18.741116       1 secure_serving.go:213] Serving securely on 127.0.0.1:10259
I0809 14:04:18.741429       1 tlsconfig.go:240] "Starting DynamicServingCertificateController"
I0809 14:04:19.135900       1 shared_informer.go:320] Caches are synced for client-ca::kube-system::extension-apiserver-authentication::client-ca-file
I0810 09:43:03.752438       1 trace.go:236] Trace[1049583894]: "Scheduling" namespace:default,name:my-replicaset-248xm (10-Aug-2024 09:43:03.551) (total time: 112ms):
Trace[1049583894]: ---"Computing predicates done" 105ms (09:43:03.663)
Trace[1049583894]: [112.076801ms] [112.076801ms] END


==> kubelet <==
Aug 13 14:35:10 minikube kubelet[1654]: I0813 14:35:10.045124    1654 topology_manager.go:215] "Topology Admit Handler" podUID="4b6cfa66-384a-4447-bce1-38d1ac18ac71" podNamespace="job-app" podName="zipkin-7d56c74569-5ms7j"
Aug 13 14:35:10 minikube kubelet[1654]: E0813 14:35:10.045304    1654 cpu_manager.go:395] "RemoveStaleState: removing container" podUID="26c8e140-7b29-4730-899f-e91e414689eb" containerName="review-db-container"
Aug 13 14:35:10 minikube kubelet[1654]: E0813 14:35:10.045333    1654 cpu_manager.go:395] "RemoveStaleState: removing container" podUID="c896a4fa-7f63-445c-9406-c4507e0bb583" containerName="job"
Aug 13 14:35:10 minikube kubelet[1654]: E0813 14:35:10.045348    1654 cpu_manager.go:395] "RemoveStaleState: removing container" podUID="0c81c729-d091-4d76-9cad-a1edf2cedb2c" containerName="zipkin"
Aug 13 14:35:10 minikube kubelet[1654]: E0813 14:35:10.045360    1654 cpu_manager.go:395] "RemoveStaleState: removing container" podUID="5e154f45-3291-4818-bc2b-d9f47023f898" containerName="review"
Aug 13 14:35:10 minikube kubelet[1654]: E0813 14:35:10.045373    1654 cpu_manager.go:395] "RemoveStaleState: removing container" podUID="ab507253-8400-483a-b563-eba601ea0af5" containerName="job-db-container"
Aug 13 14:35:10 minikube kubelet[1654]: E0813 14:35:10.045396    1654 cpu_manager.go:395] "RemoveStaleState: removing container" podUID="97764d19-0ca6-4d0d-b3c7-0066db7a71c3" containerName="zipkin"
Aug 13 14:35:10 minikube kubelet[1654]: E0813 14:35:10.045416    1654 cpu_manager.go:395] "RemoveStaleState: removing container" podUID="9e2ccd10-abd0-4a8d-9da5-6cdfca784daa" containerName="company-db-container"
Aug 13 14:35:10 minikube kubelet[1654]: E0813 14:35:10.045433    1654 cpu_manager.go:395] "RemoveStaleState: removing container" podUID="3336a70f-aaf2-4dd9-a8df-25b3f37e6be0" containerName="review"
Aug 13 14:35:10 minikube kubelet[1654]: E0813 14:35:10.045480    1654 cpu_manager.go:395] "RemoveStaleState: removing container" podUID="c896a4fa-7f63-445c-9406-c4507e0bb583" containerName="job"
Aug 13 14:35:10 minikube kubelet[1654]: E0813 14:35:10.045500    1654 cpu_manager.go:395] "RemoveStaleState: removing container" podUID="0ce6ef63-1e86-4f65-bbc6-b7c6d86ccef4" containerName="company"
Aug 13 14:35:10 minikube kubelet[1654]: E0813 14:35:10.045517    1654 cpu_manager.go:395] "RemoveStaleState: removing container" podUID="5e154f45-3291-4818-bc2b-d9f47023f898" containerName="review"
Aug 13 14:35:10 minikube kubelet[1654]: E0813 14:35:10.045531    1654 cpu_manager.go:395] "RemoveStaleState: removing container" podUID="11058cd3-2310-4152-b317-0a685600fb94" containerName="rabbitmq"
Aug 13 14:35:10 minikube kubelet[1654]: E0813 14:35:10.045544    1654 cpu_manager.go:395] "RemoveStaleState: removing container" podUID="ecb8e07a-dd97-4a9a-b7ac-4d13bd884993" containerName="job"
Aug 13 14:35:10 minikube kubelet[1654]: E0813 14:35:10.045556    1654 cpu_manager.go:395] "RemoveStaleState: removing container" podUID="b1ecd7b2-56b6-48a0-b928-ef3c0362e713" containerName="company"
Aug 13 14:35:10 minikube kubelet[1654]: E0813 14:35:10.045570    1654 cpu_manager.go:395] "RemoveStaleState: removing container" podUID="fec58581-11a9-407f-a0ce-4b798f4b03f7" containerName="rabbitmq"
Aug 13 14:35:10 minikube kubelet[1654]: E0813 14:35:10.045584    1654 cpu_manager.go:395] "RemoveStaleState: removing container" podUID="0ce6ef63-1e86-4f65-bbc6-b7c6d86ccef4" containerName="company"
Aug 13 14:35:10 minikube kubelet[1654]: I0813 14:35:10.045646    1654 memory_manager.go:354] "RemoveStaleState removing state" podUID="c896a4fa-7f63-445c-9406-c4507e0bb583" containerName="job"
Aug 13 14:35:10 minikube kubelet[1654]: I0813 14:35:10.045664    1654 memory_manager.go:354] "RemoveStaleState removing state" podUID="0c81c729-d091-4d76-9cad-a1edf2cedb2c" containerName="zipkin"
Aug 13 14:35:10 minikube kubelet[1654]: I0813 14:35:10.045680    1654 memory_manager.go:354] "RemoveStaleState removing state" podUID="3336a70f-aaf2-4dd9-a8df-25b3f37e6be0" containerName="review"
Aug 13 14:35:10 minikube kubelet[1654]: I0813 14:35:10.045693    1654 memory_manager.go:354] "RemoveStaleState removing state" podUID="0ce6ef63-1e86-4f65-bbc6-b7c6d86ccef4" containerName="company"
Aug 13 14:35:10 minikube kubelet[1654]: I0813 14:35:10.045705    1654 memory_manager.go:354] "RemoveStaleState removing state" podUID="c896a4fa-7f63-445c-9406-c4507e0bb583" containerName="job"
Aug 13 14:35:10 minikube kubelet[1654]: I0813 14:35:10.045720    1654 memory_manager.go:354] "RemoveStaleState removing state" podUID="fec58581-11a9-407f-a0ce-4b798f4b03f7" containerName="rabbitmq"
Aug 13 14:35:10 minikube kubelet[1654]: I0813 14:35:10.045735    1654 memory_manager.go:354] "RemoveStaleState removing state" podUID="97764d19-0ca6-4d0d-b3c7-0066db7a71c3" containerName="zipkin"
Aug 13 14:35:10 minikube kubelet[1654]: I0813 14:35:10.045747    1654 memory_manager.go:354] "RemoveStaleState removing state" podUID="b1ecd7b2-56b6-48a0-b928-ef3c0362e713" containerName="company"
Aug 13 14:35:10 minikube kubelet[1654]: I0813 14:35:10.045759    1654 memory_manager.go:354] "RemoveStaleState removing state" podUID="0ce6ef63-1e86-4f65-bbc6-b7c6d86ccef4" containerName="company"
Aug 13 14:35:10 minikube kubelet[1654]: I0813 14:35:10.045771    1654 memory_manager.go:354] "RemoveStaleState removing state" podUID="ecb8e07a-dd97-4a9a-b7ac-4d13bd884993" containerName="job"
Aug 13 14:35:10 minikube kubelet[1654]: I0813 14:35:10.045785    1654 memory_manager.go:354] "RemoveStaleState removing state" podUID="26c8e140-7b29-4730-899f-e91e414689eb" containerName="review-db-container"
Aug 13 14:35:10 minikube kubelet[1654]: I0813 14:35:10.045798    1654 memory_manager.go:354] "RemoveStaleState removing state" podUID="5e154f45-3291-4818-bc2b-d9f47023f898" containerName="review"
Aug 13 14:35:10 minikube kubelet[1654]: I0813 14:35:10.045810    1654 memory_manager.go:354] "RemoveStaleState removing state" podUID="5e154f45-3291-4818-bc2b-d9f47023f898" containerName="review"
Aug 13 14:35:10 minikube kubelet[1654]: I0813 14:35:10.045823    1654 memory_manager.go:354] "RemoveStaleState removing state" podUID="9e2ccd10-abd0-4a8d-9da5-6cdfca784daa" containerName="company-db-container"
Aug 13 14:35:10 minikube kubelet[1654]: I0813 14:35:10.045833    1654 memory_manager.go:354] "RemoveStaleState removing state" podUID="11058cd3-2310-4152-b317-0a685600fb94" containerName="rabbitmq"
Aug 13 14:35:10 minikube kubelet[1654]: I0813 14:35:10.045850    1654 memory_manager.go:354] "RemoveStaleState removing state" podUID="ab507253-8400-483a-b563-eba601ea0af5" containerName="job-db-container"
Aug 13 14:35:10 minikube kubelet[1654]: I0813 14:35:10.212917    1654 reconciler_common.go:247] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-9hcpw\" (UniqueName: \"kubernetes.io/projected/4b6cfa66-384a-4447-bce1-38d1ac18ac71-kube-api-access-9hcpw\") pod \"zipkin-7d56c74569-5ms7j\" (UID: \"4b6cfa66-384a-4447-bce1-38d1ac18ac71\") " pod="job-app/zipkin-7d56c74569-5ms7j"
Aug 13 14:35:11 minikube kubelet[1654]: I0813 14:35:11.394757    1654 pod_container_deletor.go:80] "Container not found in pod's containers" containerID="22e6da0ef166b0fb5fb1cf501956130c594da99e8f74996d2bf4979c5c2cc2cd"
Aug 13 14:35:18 minikube kubelet[1654]: I0813 14:35:18.474290    1654 topology_manager.go:215] "Topology Admit Handler" podUID="a17b3d06-69e2-45bb-9972-156e755aac17" podNamespace="default" podName="rabbitmq-58f4956f6f-xxd5l"
Aug 13 14:35:18 minikube kubelet[1654]: I0813 14:35:18.654770    1654 reconciler_common.go:247] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-p7stz\" (UniqueName: \"kubernetes.io/projected/a17b3d06-69e2-45bb-9972-156e755aac17-kube-api-access-p7stz\") pod \"rabbitmq-58f4956f6f-xxd5l\" (UID: \"a17b3d06-69e2-45bb-9972-156e755aac17\") " pod="default/rabbitmq-58f4956f6f-xxd5l"
Aug 13 14:35:22 minikube kubelet[1654]: I0813 14:35:22.778849    1654 pod_startup_latency_tracker.go:104] "Observed pod startup duration" pod="job-app/zipkin-7d56c74569-5ms7j" podStartSLOduration=6.760396345 podStartE2EDuration="12.778805745s" podCreationTimestamp="2024-08-13 14:35:10 +0000 UTC" firstStartedPulling="2024-08-13 14:35:11.473018523 +0000 UTC m=+107624.144942864" lastFinishedPulling="2024-08-13 14:35:17.489716095 +0000 UTC m=+107630.163352264" observedRunningTime="2024-08-13 14:35:19.288040123 +0000 UTC m=+107631.961676392" watchObservedRunningTime="2024-08-13 14:35:22.778805745 +0000 UTC m=+107635.452441914"
Aug 13 14:35:22 minikube kubelet[1654]: I0813 14:35:22.780429    1654 pod_startup_latency_tracker.go:104] "Observed pod startup duration" pod="default/rabbitmq-58f4956f6f-xxd5l" podStartSLOduration=4.780395746 podStartE2EDuration="4.780395746s" podCreationTimestamp="2024-08-13 14:35:18 +0000 UTC" firstStartedPulling="0001-01-01 00:00:00 +0000 UTC" lastFinishedPulling="0001-01-01 00:00:00 +0000 UTC" observedRunningTime="2024-08-13 14:35:22.778166004 +0000 UTC m=+107635.451802173" watchObservedRunningTime="2024-08-13 14:35:22.780395746 +0000 UTC m=+107635.454031915"
Aug 13 14:35:59 minikube kubelet[1654]: I0813 14:35:59.150851    1654 topology_manager.go:215] "Topology Admit Handler" podUID="99c9860a-bbbe-4a89-b143-9160728e56dd" podNamespace="job-app" podName="zipkin-7d56c74569-2lnxr"
Aug 13 14:35:59 minikube kubelet[1654]: I0813 14:35:59.260957    1654 reconciler_common.go:247] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-dvvp4\" (UniqueName: \"kubernetes.io/projected/99c9860a-bbbe-4a89-b143-9160728e56dd-kube-api-access-dvvp4\") pod \"zipkin-7d56c74569-2lnxr\" (UID: \"99c9860a-bbbe-4a89-b143-9160728e56dd\") " pod="job-app/zipkin-7d56c74569-2lnxr"
Aug 13 14:36:02 minikube kubelet[1654]: I0813 14:36:02.470624    1654 pod_container_deletor.go:80] "Container not found in pod's containers" containerID="8f3b33c76df76a118a5ef02be1c3cb6c1ec3a10d9ac88c75a313868f70c15666"
Aug 13 14:36:10 minikube kubelet[1654]: I0813 14:36:10.686989    1654 reconciler_common.go:161] "operationExecutor.UnmountVolume started for volume \"kube-api-access-9hcpw\" (UniqueName: \"kubernetes.io/projected/4b6cfa66-384a-4447-bce1-38d1ac18ac71-kube-api-access-9hcpw\") pod \"4b6cfa66-384a-4447-bce1-38d1ac18ac71\" (UID: \"4b6cfa66-384a-4447-bce1-38d1ac18ac71\") "
Aug 13 14:36:10 minikube kubelet[1654]: I0813 14:36:10.760534    1654 operation_generator.go:887] UnmountVolume.TearDown succeeded for volume "kubernetes.io/projected/4b6cfa66-384a-4447-bce1-38d1ac18ac71-kube-api-access-9hcpw" (OuterVolumeSpecName: "kube-api-access-9hcpw") pod "4b6cfa66-384a-4447-bce1-38d1ac18ac71" (UID: "4b6cfa66-384a-4447-bce1-38d1ac18ac71"). InnerVolumeSpecName "kube-api-access-9hcpw". PluginName "kubernetes.io/projected", VolumeGidValue ""
Aug 13 14:36:10 minikube kubelet[1654]: I0813 14:36:10.848027    1654 reconciler_common.go:289] "Volume detached for volume \"kube-api-access-9hcpw\" (UniqueName: \"kubernetes.io/projected/4b6cfa66-384a-4447-bce1-38d1ac18ac71-kube-api-access-9hcpw\") on node \"minikube\" DevicePath \"\""
Aug 13 14:36:11 minikube kubelet[1654]: I0813 14:36:11.253862    1654 pod_startup_latency_tracker.go:104] "Observed pod startup duration" pod="job-app/zipkin-7d56c74569-2lnxr" podStartSLOduration=5.927631725 podStartE2EDuration="12.253820622s" podCreationTimestamp="2024-08-13 14:35:59 +0000 UTC" firstStartedPulling="2024-08-13 14:36:03.569631415 +0000 UTC m=+107676.245716300" lastFinishedPulling="2024-08-13 14:36:09.895820412 +0000 UTC m=+107682.571905197" observedRunningTime="2024-08-13 14:36:11.253210883 +0000 UTC m=+107683.929295768" watchObservedRunningTime="2024-08-13 14:36:11.253820622 +0000 UTC m=+107683.929905507"
Aug 13 14:36:11 minikube kubelet[1654]: I0813 14:36:11.260465    1654 scope.go:117] "RemoveContainer" containerID="eda8a0816e1125bfe524101615eb63b28fa70716a9ffca13b02c80a95b6f952e"
Aug 13 14:36:12 minikube kubelet[1654]: I0813 14:36:12.180396    1654 kubelet_volumes.go:163] "Cleaned up orphaned pod volumes dir" podUID="4b6cfa66-384a-4447-bce1-38d1ac18ac71" path="/var/lib/kubelet/pods/4b6cfa66-384a-4447-bce1-38d1ac18ac71/volumes"
Aug 13 14:36:12 minikube kubelet[1654]: I0813 14:36:12.598818    1654 scope.go:117] "RemoveContainer" containerID="fc687a7b0016177616c93e00e5360bb5b0a7bad8a10d730855d2d34988bb2caf"
Aug 13 14:36:12 minikube kubelet[1654]: I0813 14:36:12.755585    1654 reconciler_common.go:161] "operationExecutor.UnmountVolume started for volume \"kube-api-access-dvvp4\" (UniqueName: \"kubernetes.io/projected/99c9860a-bbbe-4a89-b143-9160728e56dd-kube-api-access-dvvp4\") pod \"99c9860a-bbbe-4a89-b143-9160728e56dd\" (UID: \"99c9860a-bbbe-4a89-b143-9160728e56dd\") "
Aug 13 14:36:12 minikube kubelet[1654]: I0813 14:36:12.758668    1654 operation_generator.go:887] UnmountVolume.TearDown succeeded for volume "kubernetes.io/projected/99c9860a-bbbe-4a89-b143-9160728e56dd-kube-api-access-dvvp4" (OuterVolumeSpecName: "kube-api-access-dvvp4") pod "99c9860a-bbbe-4a89-b143-9160728e56dd" (UID: "99c9860a-bbbe-4a89-b143-9160728e56dd"). InnerVolumeSpecName "kube-api-access-dvvp4". PluginName "kubernetes.io/projected", VolumeGidValue ""
Aug 13 14:36:12 minikube kubelet[1654]: I0813 14:36:12.856525    1654 reconciler_common.go:289] "Volume detached for volume \"kube-api-access-dvvp4\" (UniqueName: \"kubernetes.io/projected/99c9860a-bbbe-4a89-b143-9160728e56dd-kube-api-access-dvvp4\") on node \"minikube\" DevicePath \"\""
Aug 13 14:36:14 minikube kubelet[1654]: I0813 14:36:14.181430    1654 kubelet_volumes.go:163] "Cleaned up orphaned pod volumes dir" podUID="99c9860a-bbbe-4a89-b143-9160728e56dd" path="/var/lib/kubelet/pods/99c9860a-bbbe-4a89-b143-9160728e56dd/volumes"
Aug 13 14:36:26 minikube kubelet[1654]: I0813 14:36:26.260131    1654 topology_manager.go:215] "Topology Admit Handler" podUID="7280abbd-e319-4fcf-b4b4-a3539ab990b9" podNamespace="default" podName="zipkin-7d56c74569-rl2ft"
Aug 13 14:36:26 minikube kubelet[1654]: E0813 14:36:26.260446    1654 cpu_manager.go:395] "RemoveStaleState: removing container" podUID="99c9860a-bbbe-4a89-b143-9160728e56dd" containerName="zipkin"
Aug 13 14:36:26 minikube kubelet[1654]: E0813 14:36:26.260499    1654 cpu_manager.go:395] "RemoveStaleState: removing container" podUID="4b6cfa66-384a-4447-bce1-38d1ac18ac71" containerName="zipkin"
Aug 13 14:36:26 minikube kubelet[1654]: I0813 14:36:26.260596    1654 memory_manager.go:354] "RemoveStaleState removing state" podUID="4b6cfa66-384a-4447-bce1-38d1ac18ac71" containerName="zipkin"
Aug 13 14:36:26 minikube kubelet[1654]: I0813 14:36:26.260620    1654 memory_manager.go:354] "RemoveStaleState removing state" podUID="99c9860a-bbbe-4a89-b143-9160728e56dd" containerName="zipkin"
Aug 13 14:36:26 minikube kubelet[1654]: I0813 14:36:26.388176    1654 reconciler_common.go:247] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-74gpq\" (UniqueName: \"kubernetes.io/projected/7280abbd-e319-4fcf-b4b4-a3539ab990b9-kube-api-access-74gpq\") pod \"zipkin-7d56c74569-rl2ft\" (UID: \"7280abbd-e319-4fcf-b4b4-a3539ab990b9\") " pod="default/zipkin-7d56c74569-rl2ft"
Aug 13 14:36:27 minikube kubelet[1654]: I0813 14:36:27.907065    1654 pod_container_deletor.go:80] "Container not found in pod's containers" containerID="02cb8d327c4b449d0b7b62a126c5778d411c669ea937c9e08e39f8938ba5c3fd"


==> kubernetes-dashboard [2c82cff341ff] <==
2024/08/09 14:04:44 Starting overwatch
panic: Get "https://10.96.0.1:443/api/v1/namespaces/kubernetes-dashboard/secrets/kubernetes-dashboard-csrf": dial tcp 10.96.0.1:443: connect: connection refused

goroutine 1 [running]:
github.com/kubernetes/dashboard/src/app/backend/client/csrf.(*csrfTokenManager).init(0xc00069fae8)
	/home/runner/work/dashboard/dashboard/src/app/backend/client/csrf/manager.go:41 +0x30e
github.com/kubernetes/dashboard/src/app/backend/client/csrf.NewCsrfTokenManager(...)
	/home/runner/work/dashboard/dashboard/src/app/backend/client/csrf/manager.go:66
github.com/kubernetes/dashboard/src/app/backend/client.(*clientManager).initCSRFKey(0xc000484900)
	/home/runner/work/dashboard/dashboard/src/app/backend/client/manager.go:527 +0x94
github.com/kubernetes/dashboard/src/app/backend/client.(*clientManager).init(0x19aba3a?)
	/home/runner/work/dashboard/dashboard/src/app/backend/client/manager.go:495 +0x32
github.com/kubernetes/dashboard/src/app/backend/client.NewClientManager(...)
	/home/runner/work/dashboard/dashboard/src/app/backend/client/manager.go:594
main.main()
	/home/runner/work/dashboard/dashboard/src/app/backend/dashboard.go:96 +0x1cf
2024/08/09 14:04:44 Using namespace: kubernetes-dashboard
2024/08/09 14:04:44 Using in-cluster config to connect to apiserver
2024/08/09 14:04:44 Using secret token for csrf signing
2024/08/09 14:04:44 Initializing csrf token from kubernetes-dashboard-csrf secret


==> kubernetes-dashboard [97238dbbaa29] <==
2024/08/09 14:05:27 Starting overwatch
2024/08/09 14:05:27 Using namespace: kubernetes-dashboard
2024/08/09 14:05:27 Using in-cluster config to connect to apiserver
2024/08/09 14:05:27 Using secret token for csrf signing
2024/08/09 14:05:27 Initializing csrf token from kubernetes-dashboard-csrf secret
2024/08/09 14:05:27 Empty token. Generating and storing in a secret kubernetes-dashboard-csrf
2024/08/09 14:05:27 Successful initial request to the apiserver, version: v1.30.0
2024/08/09 14:05:27 Generating JWE encryption key
2024/08/09 14:05:27 New synchronizer has been registered: kubernetes-dashboard-key-holder-kubernetes-dashboard. Starting
2024/08/09 14:05:27 Starting secret synchronizer for kubernetes-dashboard-key-holder in namespace kubernetes-dashboard
2024/08/09 14:05:27 Initializing JWE encryption key from synchronized object
2024/08/09 14:05:27 Creating in-cluster Sidecar client
2024/08/09 14:05:27 Successful request to sidecar
2024/08/09 14:05:27 Serving insecurely on HTTP port: 9090


==> storage-provisioner [32e3e37bf143] <==
I0809 14:04:39.324734       1 storage_provisioner.go:116] Initializing the minikube storage provisioner...
F0809 14:05:00.806056       1 main.go:39] error getting server version: Get "https://10.96.0.1:443/version?timeout=32s": dial tcp 10.96.0.1:443: connect: connection refused


==> storage-provisioner [ab46cd44752d] <==
I0809 14:05:11.816013       1 storage_provisioner.go:116] Initializing the minikube storage provisioner...
I0809 14:05:11.847065       1 storage_provisioner.go:141] Storage provisioner initialized, now starting service!
I0809 14:05:11.847610       1 leaderelection.go:243] attempting to acquire leader lease kube-system/k8s.io-minikube-hostpath...
I0809 14:05:29.541390       1 leaderelection.go:253] successfully acquired lease kube-system/k8s.io-minikube-hostpath
I0809 14:05:29.542146       1 controller.go:835] Starting provisioner controller k8s.io/minikube-hostpath_minikube_6e3ff30c-dc68-4145-aa9e-9be0d1719ccb!
I0809 14:05:29.545421       1 event.go:282] Event(v1.ObjectReference{Kind:"Endpoints", Namespace:"kube-system", Name:"k8s.io-minikube-hostpath", UID:"330ad99f-8fbe-4630-8cdb-4221d20756b8", APIVersion:"v1", ResourceVersion:"60231", FieldPath:""}): type: 'Normal' reason: 'LeaderElection' minikube_6e3ff30c-dc68-4145-aa9e-9be0d1719ccb became leader
I0809 14:05:29.717646       1 controller.go:884] Started provisioner controller k8s.io/minikube-hostpath_minikube_6e3ff30c-dc68-4145-aa9e-9be0d1719ccb!
I0812 15:07:37.199088       1 controller.go:1332] provision "job-app/company-db-storage-company-db-0" class "standard": started
I0812 15:07:37.208656       1 event.go:282] Event(v1.ObjectReference{Kind:"PersistentVolumeClaim", Namespace:"job-app", Name:"company-db-storage-company-db-0", UID:"ecbd4595-49e8-4241-a41a-2ccd507b8595", APIVersion:"v1", ResourceVersion:"120143", FieldPath:""}): type: 'Normal' reason: 'Provisioning' External provisioner is provisioning volume for claim "job-app/company-db-storage-company-db-0"
I0812 15:07:37.206728       1 storage_provisioner.go:61] Provisioning volume {&StorageClass{ObjectMeta:{standard    c52c0fbe-06ea-4410-a70c-77e37dc3cb1c 336 0 2024-08-02 08:19:54 +0000 UTC <nil> <nil> map[addonmanager.kubernetes.io/mode:EnsureExists] map[kubectl.kubernetes.io/last-applied-configuration:{"apiVersion":"storage.k8s.io/v1","kind":"StorageClass","metadata":{"annotations":{"storageclass.kubernetes.io/is-default-class":"true"},"labels":{"addonmanager.kubernetes.io/mode":"EnsureExists"},"name":"standard"},"provisioner":"k8s.io/minikube-hostpath"}
 storageclass.kubernetes.io/is-default-class:true] [] []  [{kubectl-client-side-apply Update storage.k8s.io/v1 2024-08-02 08:19:54 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:kubectl.kubernetes.io/last-applied-configuration":{},"f:storageclass.kubernetes.io/is-default-class":{}},"f:labels":{".":{},"f:addonmanager.kubernetes.io/mode":{}}},"f:provisioner":{},"f:reclaimPolicy":{},"f:volumeBindingMode":{}}}]},Provisioner:k8s.io/minikube-hostpath,Parameters:map[string]string{},ReclaimPolicy:*Delete,MountOptions:[],AllowVolumeExpansion:nil,VolumeBindingMode:*Immediate,AllowedTopologies:[]TopologySelectorTerm{},} pvc-ecbd4595-49e8-4241-a41a-2ccd507b8595 &PersistentVolumeClaim{ObjectMeta:{company-db-storage-company-db-0  job-app  ecbd4595-49e8-4241-a41a-2ccd507b8595 120143 0 2024-08-12 15:07:37 +0000 UTC <nil> <nil> map[app:company-db] map[volume.beta.kubernetes.io/storage-provisioner:k8s.io/minikube-hostpath volume.kubernetes.io/storage-provisioner:k8s.io/minikube-hostpath] [] [kubernetes.io/pvc-protection]  [{kube-controller-manager Update v1 2024-08-12 15:07:37 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:volume.beta.kubernetes.io/storage-provisioner":{},"f:volume.kubernetes.io/storage-provisioner":{}},"f:labels":{".":{},"f:app":{}}},"f:spec":{"f:accessModes":{},"f:resources":{"f:requests":{".":{},"f:storage":{}}},"f:volumeMode":{}}}}]},Spec:PersistentVolumeClaimSpec{AccessModes:[ReadWriteOnce],Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{storage: {{2147483648 0} {<nil>} 2Gi BinarySI},},},VolumeName:,Selector:nil,StorageClassName:*standard,VolumeMode:*Filesystem,DataSource:nil,},Status:PersistentVolumeClaimStatus{Phase:Pending,AccessModes:[],Capacity:ResourceList{},Conditions:[]PersistentVolumeClaimCondition{},},} nil} to /tmp/hostpath-provisioner/job-app/company-db-storage-company-db-0
I0812 15:07:37.227344       1 controller.go:1439] provision "job-app/company-db-storage-company-db-0" class "standard": volume "pvc-ecbd4595-49e8-4241-a41a-2ccd507b8595" provisioned
I0812 15:07:37.227451       1 controller.go:1456] provision "job-app/company-db-storage-company-db-0" class "standard": succeeded
I0812 15:07:37.228288       1 volume_store.go:212] Trying to save persistentvolume "pvc-ecbd4595-49e8-4241-a41a-2ccd507b8595"
I0812 15:07:37.291543       1 volume_store.go:219] persistentvolume "pvc-ecbd4595-49e8-4241-a41a-2ccd507b8595" saved
I0812 15:07:37.292238       1 event.go:282] Event(v1.ObjectReference{Kind:"PersistentVolumeClaim", Namespace:"job-app", Name:"company-db-storage-company-db-0", UID:"ecbd4595-49e8-4241-a41a-2ccd507b8595", APIVersion:"v1", ResourceVersion:"120143", FieldPath:""}): type: 'Normal' reason: 'ProvisioningSucceeded' Successfully provisioned volume pvc-ecbd4595-49e8-4241-a41a-2ccd507b8595
I0812 15:08:48.614218       1 controller.go:1332] provision "job-app/job-db-storage-job-db-0" class "standard": started
I0812 15:08:48.615038       1 event.go:282] Event(v1.ObjectReference{Kind:"PersistentVolumeClaim", Namespace:"job-app", Name:"job-db-storage-job-db-0", UID:"1ec8893a-933c-4f47-b0ac-6286906e2905", APIVersion:"v1", ResourceVersion:"120238", FieldPath:""}): type: 'Normal' reason: 'Provisioning' External provisioner is provisioning volume for claim "job-app/job-db-storage-job-db-0"
I0812 15:08:48.614943       1 storage_provisioner.go:61] Provisioning volume {&StorageClass{ObjectMeta:{standard    c52c0fbe-06ea-4410-a70c-77e37dc3cb1c 336 0 2024-08-02 08:19:54 +0000 UTC <nil> <nil> map[addonmanager.kubernetes.io/mode:EnsureExists] map[kubectl.kubernetes.io/last-applied-configuration:{"apiVersion":"storage.k8s.io/v1","kind":"StorageClass","metadata":{"annotations":{"storageclass.kubernetes.io/is-default-class":"true"},"labels":{"addonmanager.kubernetes.io/mode":"EnsureExists"},"name":"standard"},"provisioner":"k8s.io/minikube-hostpath"}
 storageclass.kubernetes.io/is-default-class:true] [] []  [{kubectl-client-side-apply Update storage.k8s.io/v1 2024-08-02 08:19:54 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:kubectl.kubernetes.io/last-applied-configuration":{},"f:storageclass.kubernetes.io/is-default-class":{}},"f:labels":{".":{},"f:addonmanager.kubernetes.io/mode":{}}},"f:provisioner":{},"f:reclaimPolicy":{},"f:volumeBindingMode":{}}}]},Provisioner:k8s.io/minikube-hostpath,Parameters:map[string]string{},ReclaimPolicy:*Delete,MountOptions:[],AllowVolumeExpansion:nil,VolumeBindingMode:*Immediate,AllowedTopologies:[]TopologySelectorTerm{},} pvc-1ec8893a-933c-4f47-b0ac-6286906e2905 &PersistentVolumeClaim{ObjectMeta:{job-db-storage-job-db-0  job-app  1ec8893a-933c-4f47-b0ac-6286906e2905 120238 0 2024-08-12 15:08:48 +0000 UTC <nil> <nil> map[app:job-db] map[volume.beta.kubernetes.io/storage-provisioner:k8s.io/minikube-hostpath volume.kubernetes.io/storage-provisioner:k8s.io/minikube-hostpath] [] [kubernetes.io/pvc-protection]  [{kube-controller-manager Update v1 2024-08-12 15:08:48 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:volume.beta.kubernetes.io/storage-provisioner":{},"f:volume.kubernetes.io/storage-provisioner":{}},"f:labels":{".":{},"f:app":{}}},"f:spec":{"f:accessModes":{},"f:resources":{"f:requests":{".":{},"f:storage":{}}},"f:volumeMode":{}}}}]},Spec:PersistentVolumeClaimSpec{AccessModes:[ReadWriteOnce],Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{storage: {{2147483648 0} {<nil>} 2Gi BinarySI},},},VolumeName:,Selector:nil,StorageClassName:*standard,VolumeMode:*Filesystem,DataSource:nil,},Status:PersistentVolumeClaimStatus{Phase:Pending,AccessModes:[],Capacity:ResourceList{},Conditions:[]PersistentVolumeClaimCondition{},},} nil} to /tmp/hostpath-provisioner/job-app/job-db-storage-job-db-0
I0812 15:08:48.619832       1 controller.go:1439] provision "job-app/job-db-storage-job-db-0" class "standard": volume "pvc-1ec8893a-933c-4f47-b0ac-6286906e2905" provisioned
I0812 15:08:48.619996       1 controller.go:1456] provision "job-app/job-db-storage-job-db-0" class "standard": succeeded
I0812 15:08:48.620172       1 volume_store.go:212] Trying to save persistentvolume "pvc-1ec8893a-933c-4f47-b0ac-6286906e2905"
I0812 15:08:48.696656       1 volume_store.go:219] persistentvolume "pvc-1ec8893a-933c-4f47-b0ac-6286906e2905" saved
I0812 15:08:48.697011       1 event.go:282] Event(v1.ObjectReference{Kind:"PersistentVolumeClaim", Namespace:"job-app", Name:"job-db-storage-job-db-0", UID:"1ec8893a-933c-4f47-b0ac-6286906e2905", APIVersion:"v1", ResourceVersion:"120238", FieldPath:""}): type: 'Normal' reason: 'ProvisioningSucceeded' Successfully provisioned volume pvc-1ec8893a-933c-4f47-b0ac-6286906e2905
I0812 15:09:06.382153       1 controller.go:1332] provision "job-app/review-db-storage-review-db-0" class "standard": started
I0812 15:09:06.382458       1 event.go:282] Event(v1.ObjectReference{Kind:"PersistentVolumeClaim", Namespace:"job-app", Name:"review-db-storage-review-db-0", UID:"5582c238-d335-4bb3-8090-cdd6a4fbe745", APIVersion:"v1", ResourceVersion:"120290", FieldPath:""}): type: 'Normal' reason: 'Provisioning' External provisioner is provisioning volume for claim "job-app/review-db-storage-review-db-0"
I0812 15:09:06.382291       1 storage_provisioner.go:61] Provisioning volume {&StorageClass{ObjectMeta:{standard    c52c0fbe-06ea-4410-a70c-77e37dc3cb1c 336 0 2024-08-02 08:19:54 +0000 UTC <nil> <nil> map[addonmanager.kubernetes.io/mode:EnsureExists] map[kubectl.kubernetes.io/last-applied-configuration:{"apiVersion":"storage.k8s.io/v1","kind":"StorageClass","metadata":{"annotations":{"storageclass.kubernetes.io/is-default-class":"true"},"labels":{"addonmanager.kubernetes.io/mode":"EnsureExists"},"name":"standard"},"provisioner":"k8s.io/minikube-hostpath"}
 storageclass.kubernetes.io/is-default-class:true] [] []  [{kubectl-client-side-apply Update storage.k8s.io/v1 2024-08-02 08:19:54 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:kubectl.kubernetes.io/last-applied-configuration":{},"f:storageclass.kubernetes.io/is-default-class":{}},"f:labels":{".":{},"f:addonmanager.kubernetes.io/mode":{}}},"f:provisioner":{},"f:reclaimPolicy":{},"f:volumeBindingMode":{}}}]},Provisioner:k8s.io/minikube-hostpath,Parameters:map[string]string{},ReclaimPolicy:*Delete,MountOptions:[],AllowVolumeExpansion:nil,VolumeBindingMode:*Immediate,AllowedTopologies:[]TopologySelectorTerm{},} pvc-5582c238-d335-4bb3-8090-cdd6a4fbe745 &PersistentVolumeClaim{ObjectMeta:{review-db-storage-review-db-0  job-app  5582c238-d335-4bb3-8090-cdd6a4fbe745 120290 0 2024-08-12 15:09:06 +0000 UTC <nil> <nil> map[app:review-db] map[volume.beta.kubernetes.io/storage-provisioner:k8s.io/minikube-hostpath volume.kubernetes.io/storage-provisioner:k8s.io/minikube-hostpath] [] [kubernetes.io/pvc-protection]  [{kube-controller-manager Update v1 2024-08-12 15:09:06 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:volume.beta.kubernetes.io/storage-provisioner":{},"f:volume.kubernetes.io/storage-provisioner":{}},"f:labels":{".":{},"f:app":{}}},"f:spec":{"f:accessModes":{},"f:resources":{"f:requests":{".":{},"f:storage":{}}},"f:volumeMode":{}}}}]},Spec:PersistentVolumeClaimSpec{AccessModes:[ReadWriteOnce],Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{storage: {{2147483648 0} {<nil>} 2Gi BinarySI},},},VolumeName:,Selector:nil,StorageClassName:*standard,VolumeMode:*Filesystem,DataSource:nil,},Status:PersistentVolumeClaimStatus{Phase:Pending,AccessModes:[],Capacity:ResourceList{},Conditions:[]PersistentVolumeClaimCondition{},},} nil} to /tmp/hostpath-provisioner/job-app/review-db-storage-review-db-0
I0812 15:09:06.383037       1 controller.go:1439] provision "job-app/review-db-storage-review-db-0" class "standard": volume "pvc-5582c238-d335-4bb3-8090-cdd6a4fbe745" provisioned
I0812 15:09:06.383100       1 controller.go:1456] provision "job-app/review-db-storage-review-db-0" class "standard": succeeded
I0812 15:09:06.383108       1 volume_store.go:212] Trying to save persistentvolume "pvc-5582c238-d335-4bb3-8090-cdd6a4fbe745"
I0812 15:09:06.392705       1 volume_store.go:219] persistentvolume "pvc-5582c238-d335-4bb3-8090-cdd6a4fbe745" saved
I0812 15:09:06.393050       1 event.go:282] Event(v1.ObjectReference{Kind:"PersistentVolumeClaim", Namespace:"job-app", Name:"review-db-storage-review-db-0", UID:"5582c238-d335-4bb3-8090-cdd6a4fbe745", APIVersion:"v1", ResourceVersion:"120290", FieldPath:""}): type: 'Normal' reason: 'ProvisioningSucceeded' Successfully provisioned volume pvc-5582c238-d335-4bb3-8090-cdd6a4fbe745

